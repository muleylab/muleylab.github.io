[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Our approach goes beyond the mere identification of crucial genes. We intricately explore the interplay between these genes and their specific anatomical locations, aiming for a comprehensive understanding of their operations within the intricate tapestry of genetic components. Through the utilization of simulations, statistical modeling, artificial intelligence, and network theoretical analysis, we unravel the complex relationships and regulatory mechanisms underlying infectious, autoimmune, neurodegenerative, and neuropsychiatric diseases and disorders.\nBy unraveling these complexities, we gain invaluable insights into the dynamic nature of genes and their profound impact on the survival and adaptability of living organisms. This knowledge forms the foundation of our research, empowering us to develop novel therapeutic strategies and interventions that precisely target these genes and their associated pathways. Our ultimate goal is to make transformative discoveries that pave the way for personalized treatments, improved patient outcomes, and a deeper understanding of the intricate mechanisms governing health and disease. The frontiers areas that we have been working on are given below."
  },
  {
    "objectID": "research.html#gene-expression-and-regulation",
    "href": "research.html#gene-expression-and-regulation",
    "title": "Research",
    "section": "Gene expression and regulation",
    "text": "Gene expression and regulation\nOur group studies how the body makes proteins from the instructions in our DNA (Muley and Pathania, 2017). This process is called gene expression and it’s important for keeping our cells healthy. We look at how genes are turned on and off and how that can change how cells work (Muley and Koenig, 2022). We’re especially interested in ways that this process can go wrong and cause problems like aging and diseases of the brain.\n\n\n\n\n \n\n\nFigure 1: Upstream region of a gene bounded by regulatory factors dictating its expression (left panel). Gene expression profiles of tissues, cell types, and time series (toy example)"
  },
  {
    "objectID": "research.html#intercellular-protein-protein-interaction-networks",
    "href": "research.html#intercellular-protein-protein-interaction-networks",
    "title": "Research",
    "section": "Intercellular protein-protein interaction networks",
    "text": "Intercellular protein-protein interaction networks\nProtein-protein interaction (PPI) networks is a scientific field that looks at how proteins in a cell interact with each other. We use computational tools and methods to study and map out these interactions to understand how they contribute to the overall function of the cell (Muley and Acharya, 2012, Muley, Ph.D. thesis, 2012). This knowledge can be used to develop new drugs, identify new drug targets, and improve our understanding of diseases and the functioning of cells.\n\n\n\nFigure 2: Protein-protein interactions subnetworks associated with cell surface regulates bacterial cell division\n\n\nIn addition to studying interactions within a cell, we are also interested in understanding protein interactions between cells. These interactions play a critical role in various cellular processes, including cell communication, growth, and differentiation and are involved in both normal and disease processes. Understanding intercellular interactions has potential applications in medicine, such as developing new drugs and therapies that target these interactions."
  },
  {
    "objectID": "research.html#functional-and-developmental-neurobiology",
    "href": "research.html#functional-and-developmental-neurobiology",
    "title": "Research",
    "section": "Functional and Developmental neurobiology",
    "text": "Functional and Developmental neurobiology\nDevelopmental neurobiology is a field of study that focuses on the development and maturation of the nervous system. It encompasses the cellular and molecular mechanisms that underlie the formation and function of the brain and spinal cord.\nThe cerebrum is the largest part of the brain and is responsible for many functions such as consciousness, movement, sensation, perception, reasoning, and memory. The cerebrum develops from a structure called the telencephalon, which is a part of the embryonic brain. During embryonic development, the cerebrum is formed from a structure called the neural plate, which is a sheet of cells that folds and develops into the three primary brain vesicles: the prosencephalon, the mesencephalon, and the rhombencephalon. The prosencephalon develops into the telencephalon, which gives rise to the cerebrum.\n\n\n\n\n \n\n\nFigure 3: Brain and its extensions throughout body (left panel). A closuer look at the the hypothalamus and its connection with the pituitary gland (right panel), which together plays a critical role in regulating many of the body’s physiological processes.\n\n\nLineage specific divergence refers to the process by which different species or lineages of organisms evolve different characteristics or features. In the case of the cerebrum, different lineages of animals, such as birds and mammals, have developed different characteristics in their cerebrum despite having a similar developmental origin. The molecular pathways that control cerebrum development in birds and mammals diverge, leading to the formation of distinct structures and functions. This divergence is thought to have occurred as a result of different selective pressures and adaptations to different environments. We have identified molecular pathways that are relevant to these differences, and we have provided evidence for the possible origins of neuropsychiatric diseases due to impaired cerebrum development (Muley et.al., 2020). We are currently developing a computational framework to address some long-standing questions in biology, such as the spatiotemporal map of protein-protein interactions that connect brain-driven processes to various organs in the body during development and the formation of neural circuits."
  },
  {
    "objectID": "research.html#evolutionary-biology",
    "href": "research.html#evolutionary-biology",
    "title": "Research",
    "section": "Evolutionary biology",
    "text": "Evolutionary biology\nEvolutionary biology is the study of how living things have changed and developed over time. It uses information from different areas of science to understand how life on Earth began and how it has changed. The main idea is that all living things come from a common ancestor and have grown and changed through natural processes.\n\n\n\nFigure 4: The distribution of about PDZ domain containing proteins across 1,476 microbial genomes (left panel), their classification (middle panel), and the evolutionary origin (right)\n\n\nOur goal is to investigate the distribution and evolution of protein domains, such as the PDZ domain, across different genomes. The PDZ domain is particularly challenging to study due to its high sequence divergence and frequent combination with other domains in proteins (Figure 4). Our previous research on PDZ domain-containing proteins from over 1400 microbial genomes led us to propose a link between their evolution and the development of multicellularity and organismal complexity (Muley et.al., 2019). We also assigned potential functions to many previously uncharacterized protein families, and proposed their last universal common ancestor. Currently, we are expanding our research to include PDZ domains from animals, plants and viruses and to the highly ubiquitous haloacid domain in over 17,000 genomes across the three domains of life and viruses. These domains play critical roles in regulating important signaling pathways involved in embryonic development, synaptic signaling, and metabolism relevant to human health."
  },
  {
    "objectID": "research.html#systems-biology-of-diseases",
    "href": "research.html#systems-biology-of-diseases",
    "title": "Research",
    "section": "Systems biology of diseases",
    "text": "Systems biology of diseases\nDespite significant progress in the field of biology, the lack of early diagnostic markers or effective treatments for numerous debilitating diseases and infections remains a significant challenge for society. These include neuropsychiatric, neurodegenerative, autoimmune, as well as viral infections such as genital herpes, hepatitis, mononucleosis, papillomavirus infection, AIDS, and COVID-19.\n\n\n\n\n \n\n\nFigure 5: Impaired protein-protein interactions encoded by genes in a red color subnetwork during cerebrum development may lead to neuropsychirtic disorders (left panel). PPI network of autism spectrucm disorder associated proteins identified by integrative analysis of transcriptomic and protein-protein interaction data (right panel)\n\n\nTo address this challenge, we have developed methods for large-scale or meta-analysis of gene expression data, as well as next-generation tissue and single-cell RNA-Seq data, using systems biology of disease approach. This approach aims to understand the underlying mechanisms of diseases using computational methods and data from multiple sources, and to identify the complex interactions between genes, proteins, and other biomolecules that contribute to disease development and progression. By taking a systems-level approach, we are able to identify new insights and connections that might not have been apparent from traditional reductionist methods, leading to a better understanding of the underlying causes of diseases and new strategies for diagnosis, treatment, and prevention."
  },
  {
    "objectID": "posts/mimbgrn/index.html",
    "href": "posts/mimbgrn/index.html",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "",
    "text": "In the past two decades, machine learning has become a powerful tool for predicting gene regulatory networks (GRN) or transcriptional regulatory networks (TRN), which involve transcription factors (TFs) and their target genes [@Marbach2012; @Li2023; @Muley2022]. TFs are a diverse family of DNA-binding proteins that regulate gene expression by binding to specific DNA sequences in the target gene’s upstream regions [@Dynan1983; @Vaquerizas2009]. Understanding GRNs is crucial for studying transcriptional regulation in various cellular processes, such as development, diseases, and response to environmental stimuli [@Muley2020; @Levine2003; @Salah2016].\nMachine learning, a sub-field of artificial intelligence, has revolutionized complex problem-solving across various domains, including finance, marketing, engineering, medicine, and life sciences [@Greener2022; @Lecun2015]. It enables machines to learn and improve from experience without explicit programming. Machine learning has proven to be a successful approach in predicting GRNs by utilizing co-expression patterns among TFs and their target genes [@Marbach2012]. This is achieved by harnessing the vast amount of publickly available microarray and next-generation sequencing RNA-Seq data [@geo]. These datasets, known as expression profiles, are represented as a matrix with dimensions \\(m * n\\). Where, \\(m\\) represents the total number of genes expressed by an organism, and \\(n\\) corresponds to the number of physiological conditions or experimental contexts in which gene expression was measured.\nIn simple terms, gene expression varies across different physiological conditions, resulting in distinct features or attributes. Genes and TFs themselves can be used as training examples for machine learning. Unsupervised machine learning algorithms can classify genes into groups based on similar features, particularly correlated expression. Genes that belong to the same group are often co-regulated and have functional relationships, offering insights into potential TFs that drive their correlated expression when they are part of the same group [@Barzel2013]. Clustering algorithms are examples of unsupervised machine learning methods that can infer TF-gene relationships without prior knowledge.\nIn contrast, supervised machine learning algorithms work with pre-defined groups. For instance, pairs of TFs and genes can be divided into two pre-defined groups: regulatory and non-regulatory pairs. In machine learning, these pre-defined groups are referred to as labels. Supervised algorithms learn a mathematical function, known as a classifier or model, to distinguish between labels by identifying label-specific patterns/rules from the input features [@Marbach2012]. The resulting classifier can then be used to assign the most appropriate labels to unclassified TF-gene pairs. The labeled data used to train the model is known as the gold standard or ground truth. Both types of machine learning utilize the inherent structure of the input data to uncover complex and hidden trends for classifying unlabeled examples, regardless of whether prior knowledge is available.\nIn traditional machine learning, researchers typically manually engineer features to train algorithms. However, deep learning, a subset of machine learning that utilizes neural networks, has the ability to automatically learn features from input data [@SCHMIDHUBER201585; @Lecun2015]. Deep learning models are structured into layers, with each layer responsible for transforming input data, extracting meaningful features that differentiate between labels, and generating output - the predicted labels. The depth of the model is determined by the number of layers it has. Within each layer, nodes or neurons perform calculations using activation functions, gradually transforming the data into more abstract representations [@Glorot2011; @Cybenko1989]. This process continues through the layers until the final output is produced. Through backpropagation, the model adjusts its parameters or weights to correct errors when the output doesn’t match the expected gold standard labels, thereby improving accuracy. The resulting classifier can then be used to classify unclassified TF-gene pairs into either regulatory or non-regulatory groups based on the associated features. Deep learning models excel in analyzing and combining multiple features, leading to higher accuracy and greater capability compared to traditional machine learning models.\nThis chapter focuses on implementing a deep learning classifier using TensorFlow, an open-source software framework developed by Google. The classifier is built using the Keras API within the RStudio environment. The main goal of this chapter is to demonstrate a step-by-step protocol for predicting regulatory interactions between TFs and genes, using their expression profiles as the key features.\nThe protocol utilizes publicly available gene expression data and gold standard information for Escherichia coli, obtained from the DREAM5 project publication [@Marbach2012]. It guides readers through essential steps including data preprocessing, designing the neural network architecture, model training and validation, and making predictions for novel regulatory interactions. Additionally, the chapter provides valuable insights into parameter tuning for deep learning models.\nBy the end of this chapter, readers will have a comprehensive understanding of how to apply deep learning techniques to predict regulatory interactions between TFs and genes. This knowledge can be directly applied to their own research projects, enabling them to harness the power of deep learning in their investigations."
  },
  {
    "objectID": "posts/mimbgrn/index.html#computational-and-software-requirements",
    "href": "posts/mimbgrn/index.html#computational-and-software-requirements",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Computational and software requirements",
    "text": "Computational and software requirements\n\nThis protocol has been tested on a desktop computer/laptop with 8 GB RAM running Windows and Mac OS. The computer’s configuration required to fulfill learning and prediction tasks depend on the problem’s complexity and the size of the training and unlabeled data. It is recommended to avoid running other applications simultaneously.\nWhile this protocol contains sufficient details to be followed by scientists without programming experience, basic to intermediate R programming skills are expected.\nDownload and install R and its graphical user interface, RStudio, from https://www.r-project.org and https://www.rstudio.com, respectively.\nTo efficiently execute R commands and document the entire protocol, create a file in RStudio by navigating to the File menu, selecting New File, and then choosing R script. By default, the file will be named “untitled.R”. It is recommended to save this file with a meaningful name in the folder where you will execute this protocol.\nNext, go to the Session menu in RStudio, select Set Working Directory, and then click on To Source File Location, i.e., the directory where the untitled.R file was saved. This ensures that all the commands will be executed in the same directory, and the location will be used as a reference to import or export the data saved and generated for this protocol.\nGoing forward, the “untitled.R” file will be used to input and save all R commands, allowing you to easily reproduce the results from scratch. To execute the commands, simply select them using the mouse or keyboard, and click on the “Run” option located in the top right corner of the file in RStudio. The output of the executed commands will be displayed in the console panel located below the file panel.\nThis protocol relies on Keras and TensorFlow R packages, i.e., external libraries for deep learning-specific functions [@keras; @tensorflow]. To install these packages, copy the commands described below to your “untitled.R” file, select them, and execute them using the Run option. For more details about the following commands, please refer to their source website: https://tensorflow.rstudio.com/install/, and also https://www.tensorflow.org/install.\nTo install the tensorflow R package, run the following command:\n\n\ninstall.packages(\"tensorflow\")\n\n\nBefore using the tensorflow package, we need to load the reticulate library and configure R to use a Python installation. If you already have Python installed, you can skip the install_python() command and provide the path to the Python executable. Here’s an example of how to do it on macOS or Unix/Linux operating systems:\n\n\n# assuming python is not already installed\n\nlibrary(reticulate)\npath_to_python <- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n\n# assuming that the python is already installed and \n# executable is available in /usr/local/bin/ folder (check with /usr/bin/ also)\n\nlibrary(reticulate)\npath_to_python <- \"/usr/local/bin/python3\" \nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n\n\nNext, use the install_tensorflow() function from the tensorflow package to install TensorFlow.\n\n\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-reticulate\")\n\n\nAlternatively, you can use the install_keras() function from the keras package to install TensorFlow along with some commonly used packages such as “scipy” and “tensorflow-datasets” as shown below.\n\n\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras(envname = \"r-reticulate\")\n\n\nTo confirm that the installation was successful, run the following command, which should return tf.Tensor(b’Hello Tensorflow!’, shape=(), dtype=string). If not then try “install_tensorflow()” command again.\n\n\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow!\")\n\n\nInstall following packages to facilitate data handling and analysis.\n\n\ninstall.packages(c(\"readr\", \"tibble\", \"caret\", \"verification\"))"
  },
  {
    "objectID": "posts/mimbgrn/index.html#data-requirements",
    "href": "posts/mimbgrn/index.html#data-requirements",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Data requirements",
    "text": "Data requirements\n\nTo begin, download Supplementary Data 1 from the DREAM5 network inference challenge publication (https://doi.org/10.1038/nmeth.2016) and unzip it in your current working directory. This action will create a directory named “DREAM5_network_inference_challenge.” For detailed information about this data, please refer to the original publication by Marbach et al. [@Marbach2012]. If you have your own data, it should have the format similar to that of first three files described below.\nThe required data for Escherichia coli is present in the “Network3” folder and its sub-folders. Copy four files listed below from these sub-folders into your current working directory to proceed with the analysis..\nnet3_expression_data.tsv: This file contains the expression profile data.\nnet3_transcription_factors.tsv: This file contains the list of transcription factors.\nDREAM5_NetworkInference_GoldStandard_Network3.tsv: This file contains the gold standard data, which includes both regulatory (positive examples) and non-regulatory (negative examples) TF-gene pairs.\nnet3_gene_ids.tsv: This file serves as a mapping between the anonymous gene codes used in other files and the original gene names."
  },
  {
    "objectID": "posts/mimbgrn/index.html#loading-r-libraries-and-preparing-workspace",
    "href": "posts/mimbgrn/index.html#loading-r-libraries-and-preparing-workspace",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Loading R libraries and preparing workspace",
    "text": "Loading R libraries and preparing workspace\n\nLoad the Keras and TensorFlow libraries for use in the current RStudio session as shown in below along with magrittr. The magrittr library provides a useful function called pipe (i.e., %>%), which allows us to pass the output of one operation to another.\n\n\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\n\nSet a random odd number as a seed in the set.seed function to reproduce the results of this workflow (see Note 1). If you use seed number other than 1979, your results will be slightly different than the discusseed in the following text.\n\n\nset.seed(1979)"
  },
  {
    "objectID": "posts/mimbgrn/index.html#data-preparation-and-exploration",
    "href": "posts/mimbgrn/index.html#data-preparation-and-exploration",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Data preparation and exploration",
    "text": "Data preparation and exploration\n\nImport the gene expression data and store in a exp object and do a routine check up as shown below. Note 2 enlists popular public resources to obtain gene expression data for species of interests.\n\n\nexp <- read.table(\"net3_expression_data.tsv\", header = T)\nexp <- t(exp)\ndim(exp)\nexp[1:5,1:5]\n\nThe exp object is a data.frame containing the genes of Escherichia coli in the rows and their expression levels in diverse physiological conditions and time-points in the columns. Note that the genes are anonymized for the DREAM5 challenge [@Marbach2012], and we will map them back to their original gene names later.\n\nNormalization is a crucial step in data analysis, and gene expression data should be normalized if required (see Note 3). In this case, the data was already normalized, as indicated by the distribution plot generated using the following command and exported to PDF file i.e. DeepLearning_Figure1.pdf. The plot shows that the data is normally distributed and ready for further analysis (Figure 1).\n\n\n# plot\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = 'Expression intensity', main = \"Gene expression\")\n\n# export plot to pdf file in the current directory\npdf(file = 'DeepLearning_Figure1.pdf', \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = 'Expression intensity', main = \"Gene expression\")\ndev.off()\n\n\n\n\nFigure 1: Expression distribution of Escherichia coli genes. The shape of the histogram reveals a normal distribution of expression data. This suggests that the expression data may not require additional processing.\n\n\n\nLoad the gold standard data using the read_table function from the readr package and display its contents, as shown below. Note 4 enlists popular public resources that can be used to create gold standard data for species of interests.\n\n\ngold <- readr::read_table(\"DREAM5_NetworkInference_GoldStandard_Network3.tsv\",\n                          col_names = F)\ngold\n\nIn the gold object, the first two columns respectively list the TFs and genes, while the third column contains binary labels. A value of 1 in the label column indicates that experimental evidence exists for the regulation of the gene in column 2 by the transcription factor in column 1, while a value of 0 indicates that there is no known regulatory interaction between them.\n\nWith the following command, TFs and genes were retained in the gold standard data if they were also present in the expression data. Next, we use the table function from base R to count the number of regulatory and non-regulatory pairs.\n\n\ngold <- gold[gold$X1 %in% rownames(exp) & gold$X2 %in% rownames(exp), ]\ntable(gold$X3)\n\n\nThe gold standard data is imbalanced, with 2066 regulatory pairs and 150,214 non-regulatory pairs. This is usually a common issue in biology, and training deep learning model on imbalanced gold standard can be tricky (see Note 5), which is explained in detail later. For time being, we will select all regulatory pairs and sample two times the number of non-regulatory pairs randomly as shown below. This is called down-sampling of majority class examples.\n\n\nkeep_indices <- c(which(gold$X3==1), \n                  sample(which(gold$X3==0), size = sum(gold$X3)*2))\ngold <- gold[keep_indices,]\ntable(gold$X3)\n\n\nThe data preparation process involves combining expression values for each TF-gene pair, allowing the raw data to be efficiently processed by a deep learning model for feature extraction. This step creates a data.frame that includes TF-gene pairs (optional) and their corresponding expression values. Next, the Pearson Correlation Coefficient (PCC) is calculated between the expression values of the TF and the gene in each pair. This correlation measure is a conventional approach for engineering features from raw data. The resulting correlation coefficients are added as a new column named “pcc” within the data.frame. Subsequently, labels are appended as the last column of the data.frame. The first two columns of the data.frame represent the names of the TF and gene, respectively. The remaining columns contain the expression values for the TF and gene, followed by the calculated PCC values between their expression values, and finally the gold standard label.\n\nTo facilitate further analysis, the column names that correspond to the expression values and PCC are stored in the “featurenames” object, as they will serve as features for subsequent processing.\nThe code snippet below demonstrates these steps:\n\ninputdata <- data.frame(tf = gold$X1, gene = gold$X2, \n                        exp[gold$X1,], exp[gold$X2,])\ninputdata$pcc <- sapply(seq.int(dim(gold)[1]), function(i) \n                        cor(exp[gold$X1[i],], exp[gold$X2[i],]))\ninputdata$class <- gold$X3\n\ninputdata <- tibble::as_tibble(inputdata)\n\nfeaturenames <- colnames(inputdata) %>% setdiff(\"class\")  %>% \n  setdiff(\"tf\") %>% setdiff(\"gene\")\n\n\nMachine learning models are built upon certain assumptions, that the input data inherently contains relevant structure for the classification task at hand. In our specific case, we assume that the expression of TFs is directly proportional to the expression of their target genes, as TFs regulate the activity of these genes. However, it’s important to acknowledge that this assumption may not always hold true due to various factors, such as post-translational modifications [@Deribe2010]. This introduces non-linearity into the input data, making machine learning in non-linear high-dimensional spaces challenging and potentially leading to the creation of poor models.\n\nTherefore, it is crucial to exercise caution when selecting data for modeling purposes. Figure 2 visually illustrates a clear distinction between expression values of TF-gene regulatory pairs and randomly selected pairs, thus indicating the suitability of using supervised learning techniques. The provided code snippet generates Figure 2.\n\nh1 <- hist(as.matrix(inputdata[inputdata$class==\"1\",featurenames]), \n           breaks = 100, plot = FALSE)\nh2 <- hist(as.matrix(inputdata[inputdata$class==\"0\",featurenames]), \n           breaks = 100, plot = FALSE)\n\npdf(file = 'DeepLearning_Figure2.pdf', \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nplot(h2, col = rgb(0,0,1,0.4), freq = FALSE, \n     xlab = 'Expression intensity', \n     main = \"Distribution of feature values for training data\")\nplot(h1, xaxt = 'n', yaxt = 'n', col = rgb(1,0,0,0.4), \n     freq = FALSE, add = TRUE)\ndev.off()\n\n\n\n\nFigure 2: Expression distribution of Escherichia coli TF-gene pairs. The expression value distribution of non-regulatory TF-gene pairs shows unimodal normal distribution (purple color), while regulatory pairs show bimodal distribution, in which expression values surronding the first peak overlaps with the distribution of non-regulatory pairs, and the other peak ranges in a higher expression value zone, and distinct from the other two peaks. One of the explaination for this trend is that TF and genes of the regulatory pairs are dependant and express at higher levels than those form non-regulatory pairs. It indicate that expression data without processing contains regulatory signals and can be used for deep learning.\n\n\n\nIn general, gold standard data is divided into training, validation, and test sets, which is for training the model, validating the performance while model is being produced, and to estimate the accuracy final model in real world data. The division is often based on availability of the data. Typically, 60-80% of the input data retains for training, while 40-20% is equally divided between validation and test datasets. The following code implements this process and create train,validation and test datasets in 80:10:10 proportions.\n\n\nindices <- nrow(inputdata) %>% sample.int(., ceiling( . * 0.8))\ntraindata <- inputdata[indices, ]\ntestdata <- inputdata[-indices, ]\nindices <- nrow(testdata) %>% sample.int(., ceiling( . * 0.5))\nvaldata <- testdata[-indices, ]\ntestdata <- testdata[indices, ]\ntable(traindata$class)\ntable(valdata$class)\ntable(testdata$class)"
  },
  {
    "objectID": "posts/mimbgrn/index.html#defining-deep-learning-model-architecture",
    "href": "posts/mimbgrn/index.html#defining-deep-learning-model-architecture",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Defining deep learning model architecture",
    "text": "Defining deep learning model architecture\n\nTo define the architecture of our deep learning model, we first create a sequential deep neural network (DNN) using the keras_model_sequential() function from the Keras library. We then add two dense layers using the layer_dense() function. The input layer is defined with the input_shape parameter, which needs to be equal to the total number of features in the input data or represents the shape of the features. Next, the single hidden layer contains 806 neurons and applies the “relu” activation function to introduce non-linearity in the layer’s output @Glorot2011. The second layer_dense() function adds another fully connected layer with a single output unit. This output layer computes a weighted sum of the inputs from the previous layer feeds and applies the sigmoid activation function to produce a probability value between 0 and 1 @Cybenko1989, indicating the likelihood that the input corresponds to the positive class (regulatory pairs, in this case) based on input features. It is worth mentioning that successive layers are able to dynamically interpret the number of expected inputs based on the previous layer. Below is a typical code to define the DNN architecture.\n\n\nset_random_seed(1979)\n\nmodel <- keras_model_sequential(\n  input_shape = c(length(featurenames))) %>%\n  layer_dense(units = 806, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsummary(model)\n\n\nThe code above includes a set_random_seed function for setting a random seed for TensorFlow operations to reproduce deep learning model (see Note 6).\nThe number of layers and units can be adjusted to improve the model’s capacity to learn, but too much capacity can lead to overfitting, and too low underfit the data (see Note 7). The single hidden layer contains 806 neurons which is equivalent to the total number of physiological conditions in the expression data (columns) and the pcc column representing PCC for each TF-gene pair.\nThe summary function gives an overview of the model’s architecture. The model has a simple architecture with input layer which feeds feature matrix and labels to fully connected layers hidden layer with “relu” activation, and a another fully connected output layer with “sigmoid” activation to produces a single probability value for each training example of being regulatory pair."
  },
  {
    "objectID": "posts/mimbgrn/index.html#defining-model-compilation-parameters",
    "href": "posts/mimbgrn/index.html#defining-model-compilation-parameters",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Defining Model Compilation Parameters",
    "text": "Defining Model Compilation Parameters\n\nTo configure the learning process, the model’s compilation parameters are specified using the following code. During training, the DNN randomly assigns weights to all the neurons and their connections, and predicts the output. The DNN then assesses the accuracy of the output, and if necessary, automatically adjusts these weights to improve accuracy via the backpropagation process. The DNN requires an objective function to measure performance, which is referred to as the loss or objective function, as well as an optimizer function that directs learning towards the minimum loss by keeping track of previous best performances. The following code defines the compilation parameters for the DNN.\n\n\nmodel %>% compile(optimizer = optimizer_adam(),\n loss = \"binary_crossentropy\",\n metrics = c(\"accuracy\"))\n\n\nThe compile() function configures the model.\nThe optimizer_adam() function is a widely used optimization algorithm that computes adaptive learning rates for each parameter based on the moment estimates of the gradient and past gradients and helps neural network to converge faster @kingma2017adam. See Note 8 for more details on learning and optimizer.\nThe loss function measures the difference between predicted and actual values. The “binary_crossentropy” loss function is commonly used for binary classification tasks.\nIn the above example, the “accuracy” metric is used to calculate the percentage of correctly predicted labels by the model. Accuracy metric is fine for prototyping DNN, but use more reliable metrics for practical application (see Note 9)."
  },
  {
    "objectID": "posts/mimbgrn/index.html#training-deep-learning-model",
    "href": "posts/mimbgrn/index.html#training-deep-learning-model",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Training deep learning model",
    "text": "Training deep learning model\n\nThe following code chunk fits the DNN model to the training data and validates its performance using a validation dataset by using the fit() function:\n\n\nhistory <- model %>% fit(as.matrix(traindata[,featurenames]), \n                         traindata$class,\n                         epochs=100, batch_size = 64,\n                         validation_data = list(as.matrix(valdata[,featurenames]), \n                                                valdata$class))\n\n\nThe fit() function takes the training data in the form of a matrix of predictor variables, and the corresponding binary class labels, epochs, batch size, and validation data.\nEpochs determine how many times the entire dataset is used for training the DNN. More epochs can improve performance, but there is a risk of overfitting to the training data (see Note 10).\nBatch size refers to the number of examples processed together in each training iteration. Smaller batch sizes can improve generalization, but convergence may be slower. Larger batch sizes speed up training but may result in poorer generalization. The optimal batch size depends on the dataset size and available computational resources, especially memory (see Note 11).\nThe code uses validation data (optional) to assess the model’s performance during training. Alternatively, use the validation_split parameter to specify the percentage of training data for validation (see Note 12). These parameters are optional and can be adjusted to suit the model and data requirements."
  },
  {
    "objectID": "posts/mimbgrn/index.html#estimating-the-accuracy-of-deep-learning-model",
    "href": "posts/mimbgrn/index.html#estimating-the-accuracy-of-deep-learning-model",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Estimating the accuracy of deep learning model",
    "text": "Estimating the accuracy of deep learning model\n\nThe history object records the loss and accuracy during the training over specified number of epochs. This information can be used to quickly check the performance of the model during training. The print(history) command shows that at the final epoch, the loss was 0.4628 on training data which is higher than the validation data i.e. 0.4296. Lower loss is better fit. Then, the accuracy was 0.7937 and 0.8174 respectively. We can also use evaluate function to compute accuracy and loss function over all epochs as shown below. The code plot(history) generates a plot of the training and validation loss and accuracy over the course of the training process. This visualization can help identify trends or patterns in the performance of the model.\n\n\nprint(history)\nevaluate(model, x = as.matrix(traindata[,featurenames]), y = traindata$class)\nevaluate(model, x = as.matrix(valdata[,featurenames]), y = valdata$class)\nplot(history)\n\n\nHowever, I prefer to access the ‘loss’ and ‘accuracy’ attributes from the history object, as this allows me the flexibility to visualize the data in a customized manner. For instance, Figure 3 was generated using the customized code provided below.\n\n\npdf(file = 'DeepLearning_Figure3.pdf',\n    width = 7, height = 4, pointsize = 10, useDingbats = FALSE)\n\npar(mfrow = c(1,2), cex = 0.8)\n\nmaxLoss <- max(c(history$metrics$loss, history$metrics$val_loss))\n\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"Epoch\", ylab=\"Loss\", \n     xlim = c(0, length(history$metrics$loss)), ylim = c(0, maxLoss),\n     col=\"red2\", type=\"b\",lwd=1)\nlines(history$metrics$val_loss, col=\"steelblue1\", type=\"b\",lwd=1)\nlegend(\"topright\", c(\"Training\",\"Validation\"), col=c(\"red2\", \"steelblue1\"), \n       lty=c(1,1), bty = \"n\")\n\nplot(history$metrics$accuracy, col=\"red2\", type=\"b\",lwd=1, \n     main=\"Model Accuracy\", xlab = \"Epoch\", ylab=\"Accuracy\", \n     xlim = c(0, length(history$metrics$accuracy)), ylim = c(0, 1))\nlines(history$metrics$val_accuracy, col=\"steelblue1\", type=\"b\",lwd=1)\nabline(h = 0.5, col = \"grey\", lty = 3)\ndev.off()\n\n\n\n\nFigure 3: Expression distribution of Escherichia coli TF-gene pairs. The distribution of expression values for non-regulatory TF-gene pairs follows a unimodal normal distribution (purple color). However, regulatory pairs exhibit a bimodal distribution (showed in reddish-orange colors). The first peak of the regulatory pairs’ distribution overlaps with the distribution of non-regulatory pairs, while the second peak corresponds to higher expression values (orange color) and is distinct from the other two peaks. One possible explanation for this trend is that the TF and genes in regulatory pairs are dependent on each other and express at higher levels compared to non-regulatory pairs.\n\n\nUpon observing the plot, we can identify certain inconsistencies (Figure 3). In a typical training process, the loss and accuracy gradually improve over multiple epochs, as evident in the plot. Initially, the loss experiences a steep decline up to the fifth epoch, after which it stabilizes from the tenth epoch onwards. The overall accuracy demonstrates a similar trend. These step-wise improvements indicate the model’s effectiveness as it undergoes more training. The accuracy and loss after 30 epochs do not change much and can be set as the optimum number for epochs.\nIdeally, the accuracy and loss on the validation data should not exceed those on the training data since the model is trained on the latter. However, our results violate this assumption because validation data has low loss and high accuracy than the training data. If such discrepancies occur, it could indicate issues with the training process, model architecture, or the training and validation data itself and often referred to as over- or under-fitting of the input data.\nIn our specific case, multiple issues may coexist due to arbitrary parameter settings, incomplete gold standard data, an uneven distribution of positive and negative examples in the training and validation data, and the accuracy metric may not be good for our imbalanced data. However, these problems can be addressed during the model parameter tuning process, and explained in the next section. It is important to note that despite the model’s performance not being ideal, the features do support the assumed classification task. The improvements in the loss and accuracy functions during training are smooth and indicative of good performance. While there is an issue with training and validation accuracy, it is marginal and may be a result of the imbalance in the gold standard data. Hence, it is crucial to evaluate the model’s performance on the test data to estimate its practical accuracy.\n\nThe code provided below utilizes our model to predict the probabilities of outcomes for the test dataset, which is unlabelled and completely unknown to the model. This prediction is achieved by using the predict() function, and the resulting probabilities are stored in the variable predprob for further utilization.\n\n\npredprob <- model %>% predict(as.matrix(testdata[,featurenames]))\n\n\nTo evaluate the performance of the DNN model on the test data and compare it with the training and validation data, the confusionMatrix() function from the caret package can be used. This function takes the predicted class probabilities in a binary format, specifically using a threshold of 0.5 or above (i.e., as.numeric(predprob > 0.5)), along with the true class labels. It then generates a table containing various important performance measures as shown below.\n\n\nf1 <- function(indata, truelabels){\n  res <-  model %>% predict(as.matrix(indata[,featurenames]))\n  res <- as.numeric(res>0.5)\n  res <- factor(res, c(1,0))\n  caret::confusionMatrix(res, factor(truelabels, c(1,0)))\n}\n\ntrainAccuracy <- f1(traindata, traindata$class)\nvalidationAccuracy <- f1(valdata, valdata$class)\ntestAccuracy <-f1(testdata, testdata$class)\n\nperformance <- round(t(as.data.frame(rbind(t(trainAccuracy$byClass), \n                                           t(validationAccuracy$byClass), \n                                           t(testAccuracy$byClass)))),3)\ncolnames(performance) <- c(\"Traning\", \"Validation\", \"Test\")\n\nperformance\n\nThe DNN model has an accuracy of 0.8323 on test data, which is again better than the 0.8072 of training data, with a sensitivity of 0.5638 and specificity of 0.9491. In other words, the features used for training are likely to distinguish 55.96% of real regulatory interactions and 94.91% of non-regulatory pairs in the unlabelled data.\n\nBiologists may prefer to use a higher probability cutoff to avoid false positive results, even if it means predicting fewer regulatory interactions. The ROC curve is a perfect instrument which shows how the classifier’s performance changes at different threshold settings for guidance. In the following code, roc.plot function from verification package is used to plot ROC curve\n\n\npdf(file = 'DeepLearning_Figure4.pdf',\n    width = 5, height = 5, pointsize = 12, useDingbats = FALSE)\n\nverification::roc.plot(testdata$class, predprob, \n                       ylab = \"True Positive Rate\", \n                       xlab = \"False Positive Rate\")\nabline(v = 0.5596, col = \"red2\", lty = 3)\nabline(h = 0.0509, col = \"steelblue1\", lty = 3)\n\ndev.off()\n\n\n\n\nFigure 4: The Receiver Operating Characteristic (ROC) curve for the deep learning model’s performance on the test data. The ROC curve shows that the model’s discriminatory power and performance is very good at the standard probability value cutoff of 0.5, at which the proportion of actual negative cases incorrectly classified as positive i.e. false positive rate by the model are very low, and over 55% actual positive cases correctly classified as positive by the model, indicating that the model may perform well in practical applications.\n\n\nThe commands provided above generate Figure 4, which illustrates the trade-off between the true positive rate (TPR or sensitivity) and the false positive rate (FPR or 1-specificity) at various probability cutoffs. This plot assists in determining the optimal probability cutoff value to achieve the best results. The 0.5 probability cutoff, which we used to estimate accuracy, is indicated on the plot. It shows that at this cutoff, only 5.09% of the predicted regulatory pairs are expected to be incorrect, while 55.96% of the predicted regulatory pairs will be correct.\n\nIn summary, all of the aforementioned results indicate that our model shows promising potential in practical applications. The following code shows that how to save the model for future use, and we also save performance table, expression and gold standard data which we all need to reproduce all the results or to make new better deep learning model.\n\n\nsave_model_hdf5(model, \"EcoliRegModel.h5\")\nsave(exp, performance, gold, history, file = \"EcoliRegModel.RData\")\n\nThe save_model_hdf5() function saves the model in the Hierarchical Data Format version 5 (HDF5) file format. HDF5 is a file format designed to store and organize large amounts of numerical data. It is a binary file format that can store the model architecture, weights, optimizer state, and any other information related to the model.\n\nTo prepare for the next section, it is assumed that all the commands mentioned above have been saved in a file named “Untitled.R” or with a similarly informative name. If necessary, restart RStudio to ensure a smooth transition. Restarting may not be essential but it can potentially expedite the subsequent analysis by clearing data dump from memory, particularly the prediction of the global gene regulatory network (GRN). Alternatively, you can proceed to the next section after executing the following commands.\n\n\ngc()\nrm(history, testdata, traindata, valdata, predprob)\ngc()"
  },
  {
    "objectID": "posts/mimbgrn/index.html#predicting-genome-wide-regulatory-interactions",
    "href": "posts/mimbgrn/index.html#predicting-genome-wide-regulatory-interactions",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Predicting genome-wide regulatory interactions",
    "text": "Predicting genome-wide regulatory interactions\n\nIn this section, we will demonstrate how to evaluate the pairwise combinations of transcription factors (TFs) and genes in Escherichia coli using our model. We will assess the probability of these pairs being potential regulatory pairs. It’s important to note that the total number of TF-gene pairs for Escherichia coli exceeds a million and a half, and in higher organisms, the number is even greater. Consequently, most computers may struggle to handle the feature data for all these pairs, potentially causing crashes. Therefore, the following code is specifically provided for users who do not possess high-end computational resources. For those with ample computational resources, I recommend referring to the code provided in Note 13. As these codes can be used across different sessions of RStudio, they assume that the computer or RStudio has been restarted and the same directory has been set (can be verified using the getwd() function).\n\n\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\nset.seed(1979)\n\n\nmodel <- load_model_hdf5(\"EcoliRegModel.h5\")\n\n# import expression data file\nexp <- t(read.table(\"net3_expression_data.tsv\", header = T))\n\n# import original gene names mapping \ngenenames <- read.table(\"net3_gene_ids.tsv\", header = F)\ngenes <- genenames$V2; names(genes) <- genenames$V1\n\n# import list of all transcription factors of Escherichia coli\n# tfs <- names(genes)[genes %in% c(\"gadX\", \"flhC\", \"flhD\",\"dnaA\")] # trail run\ntfs <- read.table(\"net3_transcription_factors.tsv\", header = F)$V1\n\nlength(tfs)*nrow(exp)\n\n\npredictions <- NULL\n\nfor(i in tfs){\n  \n  tfdata <-data.frame(tf = i, gene = rownames(exp), \n                      tfname = genes[i],\n                      genename = genes[rownames(exp)])\n  tfdata <- tibble::as_tibble(tfdata[tfdata$tf != tfdata$gene,])\n  \n  inpreddata <- cbind(exp[tfdata$tf,], exp[tfdata$gene,])\n  inpreddata <- cbind(inpreddata, \n                      sapply(seq.int(dim(tfdata)[1]), \n                             function(i) \n                               cor(exp[tfdata$tf[i],], \n                                   exp[tfdata$gene[i],])))\n  \n  tfdata$probability <- (model %>% predict(inpreddata))[,1]\n  \n  predictions <- rbind(predictions, tfdata[tfdata$probability>0.5,])\n  \n}\n\npredictions <- predictions[rev(order(predictions$probability)),]\npredictions\n\n\nOur model predicted only 80,225 as regulatory out of 1,506,674 TF-gene pairs. There are number of sophisticated ways to evaluate the predicted GRN [@Muley2022]. Here, however, we will quickly select few TF-gene pairs with high probability of being regulatory, and will study them. Escherichi coli has many acid inducible systems which protect cells against acid stress to pH 2 or below. Of them, the glutamate depending system is the most effect and relies on two glutamate decarboxylases (GadA and GadB) combined with a putative glutamate:gamma-aminobutyric acid antiporter (GadC). According to our predictions, it appears to be regluated by OsmE, which is osmolarity inducible factor, however, there is no information exists that it regulates glutamate-dependant acid resistance system. though functionally it would make sense. The GadX is a known regulator of the acid resistance system. Also, master transcription factors associated with flagella biosynthesis and chemotaxis were among the highly ranked TF-gene pairs. So, lets check target genes of GadX, FlhC, and FlhD transcription factors.\n\n\npredictions[predictions$tfname==\"gadX\",]\npredictions[predictions$tfname==\"flhC\",]\npredictions[predictions$tfname==\"flhD\",]\n\nOur results suggests that GadX may regulate 16 genes at the probability cutoff of 0.5 or above, and of 16, five genes i.e. gadA, gadE gadB, gadC and gadW belong to glutamate dependant acid protection system, confirming the accuracy of our prediction system. Likewise, FlhC and FlhD regulates a large number of genes involved in flagellar assembly and chemotaxis reassuring our prediction system. The overlap of their target genes is 652, which is highly expected. One of the problem of deep learning algorithm is that they can memorize the training dataset and many of the predictions could be part of training or validation dataset. Therefore, lets remove all predictions that were part of the training, and again cross-check the results using following command. As you can see, there are still many predictions which were novel, and make perfect sense at functional level.\n\npredictions[!paste0(predictions$tf,predictions$gene) %in%  \n              paste0(inputdata$tf[inputdata$class==1], \n                     inputdata$gene[inputdata$class==1]),] \n\n\nTo export results from prediction object to a tabular, csv or excel file, use following code. The third command requires writexl R package which should be installed using install.packages function. Likewise, you can also export the performance table.\n\n\nwrite.table(predictions, file = \"grnPredictionsEcoli.txt\", \n            col.names = T, row.names = F, quote = F, sep = \"\\t\")\nwrite.csv(predictions, file = \"grnPredictionsEcoli.csv\", row.names = F)\nwritexl::write_xlsx(list(Table1 = predictions), \n                    path = \"grnPredictionsEcoli.xlsx\", col_names = T, )"
  },
  {
    "objectID": "posts/mimbgrn/index.html#tuning-deep-learning-for-improved-performance",
    "href": "posts/mimbgrn/index.html#tuning-deep-learning-for-improved-performance",
    "title": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R",
    "section": "Tuning Deep Learning for Improved Performance",
    "text": "Tuning Deep Learning for Improved Performance\n\nDeveloping an accurate DNN requires considering crucial aspects and parameters. Deep learning deals with high-dimensional spaces and vast data in a complex representations, making it challenging to find the optimal model configuration. Classification accuracy depends on problem complexity, data quality, and training accuracy. Fine-tuning hyperparameters that affect training process through practice and experimentation is necessary. Various approaches can enhance model performance.\nBy experimenting with different activation functions, optimization algorithms, and loss functions, you can fine-tune your deep learning model to achieve better performance and more accurate predictions.\nBy changing the activation function, you can influence how the model learns and represents complex patterns. Experimenting with different activation functions like sigmoid, ReLU, or tanh can help improve the model’s performance in capturing non-linear relationships and making accurate predictions.\nThe choice of the optimization algorithm affects how the model learns and converges to an optimal solution. Trying different optimization algorithms such as Adam, RMSprop, or stochastic gradient descent (SGD) can impact the speed of convergence and the quality of the final model. See Note 8 for details and functions in Keras.\nThe loss function measures the model’s performance during training and guides the learning process. Switching the loss function, such as using binary cross-entropy instead of mean squared error, can improve the model’s ability to handle classification tasks and optimize for the desired output.\nThe design of the DNN involves carefully considering the number of layers and units, striking a balance between capturing complex relationships and avoiding overfitting for optimal configuration. Layers transform input data into meaningful representations, extracting relevant features. Deeper models with more layers learn intricate data representations, capture complex relationships, and discover abstract patterns. Most problems can be handled with one to three hidden layers, depending on linear and non-linear relationships in the features. The number of units within each layer impacts the model’s capacity. Setting the number of units based on the total input feature shape, gradually decreasing, enables capturing finer details in the data. Adding more layers or units can enhance learning and generalization, but overfitting to training data becomes a risk. Finding the right balance involves iterative refinement and experimentation for optimal configuration that achieves desired performance without overfitting. See Note 7 also.\nDropout is a regularization technique commonly used in deep learning models to prevent overfitting [@dropout; @JMLRv15srivastava14a]. It involves randomly dropping a certain percentage of neurons or units in a layer during the training process. This dropout of neurons helps the network learn more robust and generalizable representations by reducing the reliance on specific input patterns. By randomly removing neurons, layer_dropout encourages the model to learn from different subsets of neurons, making it less sensitive to individual neurons and reducing the risk of overfitting. The dropout rate, which represents the proportion of neurons to be dropped, is an important hyperparameter that can be tuned to achieve optimal performance and prevent overfitting.\nThe learning rate is a crucial parameter that determines the size of the steps taken by the model during training, influencing both the speed of convergence and the overall performance. If the learning rate is set too high, it can cause divergence, leading to a deterioration in the model’s performance. On the other hand, a very low learning rate can result in slow convergence, prolonging the training process. It is common to start with default learning rates, which typically yield satisfactory results, and then fine-tune them as needed within the range of 0 to 1. The learning rate can be adjusted using the optimizer object, such as optimizer_adam(learning_rate = 0.001) in the compile function.\nDuring each epoch, the model goes through one complete iteration of the training dataset, updating its parameters (weights and biases) based on observed patterns and the optimization algorithm. Performing multiple epochs is essential for the model to improve its predictions and overall performance. Choosing the appropriate number of epochs is a critical hyperparameter selection, ensuring a balance between learning meaningful patterns and preventing overfitting.\nIn deep learning, training is commonly performed using mini-batches, which involve dividing the entire training dataset into smaller subsets. The batch size refers to the number of training examples processed together before the model updates its internal parameters (weights and biases) based on computed gradients. Larger batch sizes can lead to faster training as more samples are processed in parallel, but they also require more memory. Conversely, smaller batch sizes allow for more frequent model updates and may potentially improve generalization. Choosing an optimal batch size depends on factors such as computational resources, dataset size, and model complexity. It often involves striking a balance between training speed and memory usage. For example, if the training data consists of 10 examples and a batch size of 5 is used, one epoch would require two passes of the training examples (10/5). The weights are adjusted batch size times per epoch. Experimenting with different batch sizes is recommended to find the optimal balance for a specific deep learning task. See Note 11 for more details.\nBatch normalization has been shown to reduce the need for a large number of training steps and, in some cases, the use of dropout, resulting in stable and faster convergence, particularly with higher learning rates [@pmlrv37ioffe15]. However, it has also been observed that using batch normalization in conjunction with dropout can sometimes lead to a decrease in performance. It is worth noting that our model, when included batch normalization for training, achieves 100% accuracy on the training set. Nevertheless, it is essential to interpret such results with caution and not solely rely on exact accuracy as an indicator of model performance. Achieving perfect accuracy may be indicative of overfitting, where the model memorizes the training data without truly understanding the underlying patterns. Therefore, it is crucial to evaluate the model’s ability to generalize to unseen examples by assessing its performance on separate validation or test data. Additionally, considering other evaluation metrics and techniques, such as cross-validation or precision and recall analysis, can provide a more comprehensive understanding of the model’s capabilities. Adopting this mindset will enable the development of reliable and robust machine learning models. It can be implemented in code using layer_batch_normalization() function, right after each hidden layer.\nImbalanced training data, where positive and negative examples are unevenly represented, can introduce bias in the learning process. To mitigate this issue, various techniques such as stratification through over- or down-sampling and assigning different weights to classes have been proposed and extensively discussed in the literature. For coding examples related to handling imbalanced data, please refer to Note 5.\nData augmentation is a technique commonly used in deep learning to expand the training dataset artificially. It involves applying various transformations, such as rotation, scaling, flipping, cropping, or adding noise, to the existing data. By diversifying the training data, data augmentation helps prevent overfitting and enhances the model’s ability to generalize to new, unseen examples. Please refer to Note 5 also.\nWhen evaluating model performance, it is important to consider both data-driven and field-specific preferences. For imbalanced training data, precision and recall metrics provide more meaningful insights than overall accuracy. It is crucial to use performance metrics wisely to guide model training, ensuring that the model is not biased towards the majority class. For coding examples related to handling performance metrics, please refer to Note 9.\nCallbacks are functions that offer customization options for the training process of a deep learning model. They allow for monitoring the model’s progress during training and performing specific actions based on certain conditions. Examples of actions that can be taken using callbacks include saving model weights, adjusting the learning rate, early stopping, and logging metrics. By leveraging callbacks, deep learning models can be tailored to achieve improved performance, efficiency, and customization. For coding examples related to callbacks, please refer to Note 14.\nThe R ecosystem provides extensive support and packages that can be utilized with the Keras R API for finding optimal hyperparameters in deep learning model development. One such package is kerasTuner, which is specifically designed for hyperparameter tuning with Keras models. It offers a user-friendly interface for defining a search space of hyperparameters and performing hyperparameter optimization using techniques like random search, hyperband, or Bayesian optimization. The seamless integration of kerasTuner with the Keras R API enables efficient identification of the best hyperparameters for deep learning models.\nI recommend users to explore prototype examples of deep learning available at https://tensorflow.rstudio.com/examples/. These examples serve as valuable resources for understanding and implementing deep learning techniques."
  },
  {
    "objectID": "positions.html",
    "href": "positions.html",
    "title": "Join us",
    "section": "",
    "text": "Research Opportunities in Bioinformatics, Computational biology, Data science, and AI\nJoin our cutting-edge research lab that uses bioinformatics, computational biology, and machine learning to investigate long-standing questions in medicine, agriculture, and life sciences with the goal of benefiting humankind. Our team of experienced scientists offers opportunities for Ph.D. students, masters thesis students, bachelors thesis students, interns, and paid research trainees.\n\n\nOpportunities include\n\nConducting cutting-edge research using AI and bioinformatics techniques\nProfessional growth and career development in a dynamic and supportive research environment\nOpportunity to work abroad under the supervision of an experienced researcher\nHands-on experience in laboratory techniques and research methods\nPublish findings in peer-reviewed journals and present at conferences\n\n\n\nTo Apply:\nPlease send your CV or express your interest through at vijay dot muley at gmail dot com\nJoin us in making a difference in the fields of medicine, agriculture, and life sciences."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Members",
    "section": "",
    "text": "About me\n muleylab at gmail dot com"
  },
  {
    "objectID": "bsVijay.html",
    "href": "bsVijay.html",
    "title": "Biographical Sketch",
    "section": "",
    "text": "I received my B.Sc. degree in Chemistry, Zoology, and Fishery Science in 2000 and my M.Sc. in Biotechnology in 2002 from Dr. Babasaheb Ambedkar Marathwada University in Aurangabad, India. In 2005, I completed my second M.Sc. degree in Bioinformatics at the University of Pune in Pune, India. In 2012, I obtained my Ph.D. degree in Bioinformatics from the Computational and Functional Genomics Laboratory at the Centre for Fingerprinting and DNA Diagnostics (CDFD) in Hyderabad, India, under the supervision of Dr. Akash Ranjan.\nIn April 2012, I joined the Indian Institute of Science Education and Research (IISER) in Pune, India as a postdoctoral scientist in Dr. Sanjeev Galande’s laboratory, where I worked on animal evolution, embryonic development, and epigenetic regulation. In October 2013, I moved to Jena, Germany as a staff scientist and worked on mathematical modeling of gene expression and regulation in breast cancers at the Leibniz Institute for Natural Product Research and Infection Biology, in the Network Modelling group headed by Professor Rainer Koenig. In February 2016, I was appointed as an assistant professor on a permanent basis at the Central University of Punjab in Bathinda, India in the Centre for Computational Sciences. However, I resigned from the position after a while and decided to work as a full-time researcher at University College Cork, National University of Ireland in Cork City, Ireland from September 2017. There, I worked on IFN-gamma and TNF-alpha signaling pathways involved in immune responses with Dr. Ken Nally. In April 2018, I was invited to join the Institute of Neurobiology at the National Autonomous University of Mexico (UNAM) in Queretaro, Mexico, as an assistant professor and senior scientist in the neural laboratory of neural development. At UNAM, I worked on various research projects in the field of neurobiology and studied the molecular basis of embryonic brain development, neuropsychiatric disorders, neurodegenerative and infectious diseases at the systems level. The National Council for Science and Technology (CONACYT), Government of Mexico, awarded me with a lifetime honorary distinction called Sistema Nacional de Investigadores nivel 1 (System of National Investigator level 1) and fellowship for three subsequent years.\nI have published several research papers, book chapters, and a book in prestigious journals from Nature, Oxford, Elsevier, Springer, BioMedCentral, PLoS, and Frontiers publication houses. Currently, I have contracts to write and edit two books. I am also an editorial board member of Frontiers and Dove Medical Press journals and review research manuscripts for over 30 journals. I have received several national and international fellowships by competition, including UGC-NET, DBT-JRF, and UGC-JRF from the Indian government, travel fellowships from the European Science Foundation, and the International Society for Computational Biologists."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laboratory of organismal science!",
    "section": "",
    "text": "Our research is dedicated to delving into the multifaceted realm of gene functionality, with a specific focus on infectious, autoimmune, neurodegenerative, and neuropsychiatric diseases and disorders. Utilizing cutting-edge methodologies, we conduct in-depth analyses of extensive data to identify essential genetic modules that play pivotal roles in the genesis and progression of these conditions."
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2023",
    "text": "2023\nMuley VY*. (2023): Centrality Analysis of Protein-Protein Interaction Networks Using R, Methods in Molecular Biology. 2023 Jul 15;2690:445-456\nPubMed Journal\nMuley VY*. (2023): Search, Retrieve, Visualize and Analyze Protein-Protein Interactions From Multiple Databases: A Guide to Experimental Biologists, Methods in Molecular Biology. 2023 Jul 15;2690:429-443\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\nMuley VY*, König R. (2022): Human transcriptional gene regulatory network compiled from 14 data resources, Biochimie, 2022 Feb 1;193:115-125\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\nMuley VY*. (2021): Mathematical Linear Programming to Model MicroRNAs-Mediated Gene Regulation Using Gurobi Optimizer, Methods in Molecular Biology, 2021 Jul 13;2328:287-301\nPubMed Journal\nMuley VY*. (2021): Mathematical Programming for Modeling Expression of a Gene Using Gurobi Optimizer to Identify Its Transcriptional Regulators, Methods in Molecular Biology, 2021 Jul 13;2328:99-113\nPubMed Journal\nAviña-Padilla K, Ramírez-Rafael JA, Herrera-Oropeza GE, Muley VY, Valdivia DI, Díaz-Valenzuela E, García-García A, Varela-Echavarría A, Hernández-Rosales M. (2021): Evolutionary Perspective and Expression Analysis of Intronless Genes Highlight the Conservation of Their Regulatory Role, Frontiers in Genetics, 2021 Jul 9;12:654256\nPubMed Journal\nMuley VY*, Bojórquez SAF, Kamble KD. (2021): Nervous System of Invertebrates, Encyclopedia of Animal Cognition and Behavior, 2021 Mar 12: Springer, Cham\nWebLink"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "2020",
    "text": "2020\nMuley VY*, López-Victorio CJ, Ayala-Sumuano JT, González-Gallardo A, González-Santos L, Lozano-Flores C, Wray G, Hernández-Rosales M, Varela-Echavarría A. (2020): Conserved and divergent expression dynamics during early patterning of the telencephalon in mouse and chick embryos, Progress in Neurobiology, 2020 Mar 1;186:101735\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "2019",
    "text": "2019\nMuley VY*, Larriva-Sánchez F. (2019): Nervous System, The, Encyclopedia of Animal Cognition and Behavior, 2019 Aug 29: Springer, Cham\nWebLink\nMuley VY*, Gijón CET, García IM, Cueva LX. (2019): Vertebrate Nervous System, Encyclopedia of Animal Cognition and Behavior, 2019 Jun 26: Springer, Cham\nWebLink\nMuley VY*, Akhter Y, Galande S. (2019): PDZ Domains Across the Microbial World: Molecular Link to the Proteases, Stress Response, and Protein Synthesis, Genome Biology and Evolution, 2019 Mar 1;11(3):644-659\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-5",
    "href": "publications.html#section-5",
    "title": "Publications",
    "section": "2018",
    "text": "2018\nMuley VY*, Pathania A. (2018): Gene Expression, Encyclopedia of Animal Cognition and Behavior, 2018 Oct 20: Springer, Cham\nWebLink\nJangid RK, Kelkar A, Muley VY, Galande S. (2018): Bidirectional promoters exhibit characteristic chromatin modification signature associated with transcription elongation in both sense and antisense directions, BMC Genomics, 2018 May 2;19(1):313\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-6",
    "href": "publications.html#section-6",
    "title": "Publications",
    "section": "2017",
    "text": "2017\nMuley VY*. (2017): Bioinformatics, Encyclopedia of Animal Cognition and Behavior, 2017 Sept 5: Springer, Cham\nWebLink\nPathania A, Muley VY*. (2017): Gene Expression Profiling, Encyclopedia of Animal Cognition and Behavior, 2017 Aug 22: Springer, Cham\nWebLink\nVenkatRao V, Kumar SK, Sridevi P, Muley VY, Chaitanya RK. (2017): Cloning, characterization and transmission blocking potential of midgut carboxypeptidase A in Anopheles stephensi, Acta Tropica, 2017 Apr;168:21-28\nPubMed Journal\nHaldar KK, Muley VY, Datar S, Patra A. (2017): Structural and electronic investigation of metal-semiconductor hybrid tetrapod hetero-structures, Gold Bulletin, 2017 Mar 9;50:105–110\nJournal"
  },
  {
    "objectID": "publications.html#section-7",
    "href": "publications.html#section-7",
    "title": "Publications",
    "section": "2016",
    "text": "2016\nMuley VY*, Hahn A. (2016): Protein-protein functional linkage predictions: bringing regulation to context, Computational Biology & Bioinformatics: Gene Regulation, 2016 May 12, Taylor & Francis\nWebLink\nSalah FS, Ebbinghaus M, Muley VY, Zhou Z, Al-Saadi KR, Pacyna-Gengelbach M, O’Sullivan GA, Betz H, König R, Wang ZQ, Bräuer R, Petersen I. (2016): Tumor suppression in mice lacking GABARAP, an Atg8/LC3 family member implicated in autophagy, is associated with alterations in cytokine secretion and cell death, Cell Death & Disease, 2016 Apr 28;7(4):e2205\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-8",
    "href": "publications.html#section-8",
    "title": "Publications",
    "section": "2014",
    "text": "2014\nKarmodiya K, Anamika K, Muley VY, Pradhan SJ, Bhide Y, Galande S. (2014): Camello, a novel family of Histone Acetyltransferases that acetylate histone H4 and is essential for zebrafish development, Scientific Reports, 2014 Aug 15;4:6076\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-9",
    "href": "publications.html#section-9",
    "title": "Publications",
    "section": "2013",
    "text": "2013\nMuley VY, Ranjan A. (2013): Evaluation of physical and functional protein-protein interaction prediction methods for detecting biological pathways, PLOS One, 2013 Jan 17;8(1):e54325\nPubMed Journal"
  },
  {
    "objectID": "publications.html#section-10",
    "href": "publications.html#section-10",
    "title": "Publications",
    "section": "2012",
    "text": "2012\nMuley VY, Acharya V. (2012): Genome-wide prediction and analysis of protein-protein functional linkages in bacteria, SpringerBriefs in Systems Biology, 2012 Jul 28:Volume 2, Springer Nature (ISBN 978-1-4614-4705-4), Book\nWebLink\nMuley VY, Ranjan A. (2012): Effect of reference genome selection on the performance of computational methods for genome-wide protein-protein interaction prediction, PLOS One, 2012 Jul 26;7(7):e42057\nPubMed Journal\nKamble KD, Bidwe PR, Muley VY, Kamble LH, Bhadange DG, Musaddiq M. (2012): Characterization of L-asparaginase producing bacteria from water, farm and saline soil, Bioscience discovery, 2012 Jan 1;3 (1), 116-119\nJournal\nKamble KD, More A, Muley VY. (2012): Effect of incubation on DNase production by a moderate thermophilic bacterium screened from arid land, Journal of Pure and Applied Microbiology, 2012 Mar 31;6(1):265-269\nJournal"
  },
  {
    "objectID": "publications.html#preprints-upcoming",
    "href": "publications.html#preprints-upcoming",
    "title": "Publications",
    "section": "Preprints (Upcoming)",
    "text": "Preprints (Upcoming)\nMuley VY*, Singh A, Gruber K, Kamble KD, Varela-Echavarría A. (2023): Potential functions of the amino-terminal cytoplasmic region of SARS-CoV-2 entry protein TMPRSS2 in virion assembly and early secretory pathway, In preparation, A part of this work is available as a preprint at BioRxiv\nPreprint"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "aboutVijay.html",
    "href": "aboutVijay.html",
    "title": "Laboratory of organismal science!",
    "section": "",
    "text": "I specialize in the cutting-edge field of computational biology, utilizing my expertise in bioinformatics and data science to uncover the mysteries of molecular biology, microbiology, neurobiology, immunology, evolution, and developmental biology. With a wealth of experience working in government and public universities in India, Germany, Ireland, and Mexico, I have a deep understanding of the advanced computational methods required to study the intricate inner workings of living organisms, cells, subcellular structures, genes, proteins, and their interactions.\nI received my B.Sc. degree in Chemistry, Zoology, and Fishery Science in 2000 and my M.Sc. in Biotechnology in 2002 from Dr. Babasaheb Ambedkar Marathwada University in Aurangabad, India. In 2005, I completed my second M.Sc. degree in Bioinformatics at the University of Pune in Pune, India. In 2012, I obtained my Ph.D. degree in Bioinformatics from the Computational and Functional Genomics Laboratory at the Centre for Fingerprinting and DNA Diagnostics (CDFD) in Hyderabad, India, under the supervision of Dr. Akash Ranjan.\nIn April 2012, I joined the Indian Institute of Science Education and Research (IISER) in Pune, India as a postdoctoral scientist in Dr. Sanjeev Galande’s laboratory, where I worked on animal evolution, embryonic development, and epigenetic regulation. In October 2013, I moved to Jena, Germany as a staff scientist and worked on mathematical modeling of gene expression and regulation in breast cancers at the Leibniz Institute for Natural Product Research and Infection Biology, in the Network Modelling group headed by Professor Rainer Koenig. In February 2016, I was appointed as an assistant professor on a permanent basis at the Central University of Punjab in Bathinda, India in the Centre for Computational Sciences. However, I resigned from the position after a while and decided to work as a full-time researcher at University College Cork, National University of Ireland in Cork City, Ireland from September 2017. There, I worked on IFN-gamma and TNF-alpha signaling pathways involved in immune responses with Dr. Ken Nally. In April 2018, I was invited to join the Institute of Neurobiology at the National Autonomous University of Mexico (UNAM) in Queretaro, Mexico, as an assistant professor and senior scientist in the neural laboratory of neural development. At UNAM, I worked on various research projects in the field of neurobiology and studied the molecular basis of embryonic brain development, neuropsychiatric disorders, neurodegenerative and infectious diseases at the systems level. The National Council for Science and Technology (CONACYT), Government of Mexico, awarded me with a lifetime honorary distinction called Sistema Nacional de Investigadores nivel 1 (System of National Investigator level 1) and fellowship for three subsequent years.\nIf you share my passion for using science and data to improve productivity and living standards, I would love to hear from you. Whether you are interested in research, internships, collaboration, or consultancy, please don’t hesitate to contact me via the social media links provided. I am always open to discussing new opportunities and exciting projects."
  },
  {
    "objectID": "cvVijay.html",
    "href": "cvVijay.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Apr 2018 – Nov 2022 | Assistant Professor |\nInstitute of Neurobiology, National Autonomous University of Mexico (UNAM), Queretaro, Mexico\nSep 2017 – Apr 2018 | Post-doctoral Researcher |\nAPC Microbiome, University College Cork, National University of Ireland, Cork, Ireland\nFeb 2016 – Apr 2017 | Assistant Professor |\nCentre for Computational Sciences, Central University of Punjab, Bathinda, India\nOct 2013 – Dec 2015 | Post-doctoral Researcher |\nLeibniz Institute for Natural Product Research and Infection Biology - Hans-Knöll-Institut, Jena, Germany\nApr 2012 – Aug 2013 | Post-doctoral Researcher |\nCentre for Excellence in Epigenetics, Indian Institute of Science, Education and Research, Pune, India"
  },
  {
    "objectID": "cvVijay.html#education",
    "href": "cvVijay.html#education",
    "title": "Curriculum vitae",
    "section": "Education",
    "text": "Education\n2005-2012 | Ph.D. Bioinformatics | Centre for DNA Fingerprinting and Diagnostics, India\n2003-2005 | M.Sc. Bioinformatics | Savitribai Phule Pune University, India\n2000-2002 | M.Sc. Biotechnology | Dr. Babasaheb Ambedkar Marathwada University, India\n1997-2000 | Bachelor of Science | Dr. Babasaheb Ambedkar Marathwada University, India"
  },
  {
    "objectID": "cvVijay.html#honours-awards-and-fellowships",
    "href": "cvVijay.html#honours-awards-and-fellowships",
    "title": "Curriculum vitae",
    "section": "Honours, awards and fellowships",
    "text": "Honours, awards and fellowships\nJan 2020 - Nov 2022 | Sistema Nacional de Investigadores Nivel I (National System of Investigators) |\nHonorary distinction and fellowship from National Council for Science and Technology (CONACyT), Government of Mexico\nSep 2010 | Travel fellowship to Belgium | International Society for Computational Biology\nApr 2010 | Travel fellowship to Germany, | European Science Foundation\nApr 2010 | Best poster presentation | 4th ESF conference on Functional Genomics and Diseases, Germany\nSep 2005 - Sep 2010 | Junior Research Fellowship | Department of Biotechnology, Government of India\nDec 2005 - Dec 2010 | Junior Research Fellowship | University Grant Commission, Government of India, Not availed\nJun 2004 - Lifetime | National Eligibility Test for Lectureship | University Grant Commission, Government of India"
  },
  {
    "objectID": "cvVijay.html#research-grants-and-projects",
    "href": "cvVijay.html#research-grants-and-projects",
    "title": "Curriculum vitae",
    "section": "Research grants and projects",
    "text": "Research grants and projects\n1 Jan 2020 – 31 Dec 2021 | Principal Investigator |\nIdentification of brain-specific cell death genetic modules and their relevance in neurodegeneration\nFunded by General Directorate for Academic Personnel Affairs, National Autonomous University of México, Mexico\n1 Jan 2011 – 31 Dec 2013 | Co-investigator | Genetic diversity and first generation linkage map of Clarias batrachus | Funded by Department of Biotechnology, Government of India, India"
  },
  {
    "objectID": "cvVijay.html#software-and-database-developed",
    "href": "cvVijay.html#software-and-database-developed",
    "title": "Curriculum vitae",
    "section": "Software and database developed",
    "text": "Software and database developed\nNeurodegenerative Disease Gene Expression database (NDGEx) : A database of differential expression results in 25 neurodegenerative diseases from 155 independent microarray datasets\nCell Death and Survival gene database (CDSG) : A database of 8,380 predicted genes from 175 species linked with literature on cell death and survival\nEscherichia coli functional protein-protein interactions (EcoFunPPI) : A database of functional protein-protein interactions in Escherichia coli K12 predicted using a combination of 9 machine learning methods\npub2path : R package to reconstruct biological pathway or process of your interest without a prior knowledge of gene functions from PubMed literature. Access will be given on request since its in final stage of development. For example, above- mentioned CDSG database created using pub2path."
  },
  {
    "objectID": "cvVijay.html#invited-talks",
    "href": "cvVijay.html#invited-talks",
    "title": "Curriculum vitae",
    "section": "Invited talks",
    "text": "Invited talks\n2 Jan 2023 | Probing TMPRSS2 and ACE2 SARS-CoV-2 cell entry receptors to reveal COVID-19 pathogenesis and potential drug targets at the International Conference on Drug Development and Drug Discovery, Swami Ramanand Teerth Marathwada University, Nanded, India\n16 Apr 2021 | Cross-species transcriptomic analysis of SARS-CoV-2 cell entry receptors reveal potential targets at the Institute of Neurobiology, National Autonomous University of Mexico, Queretaro, Mexico\n16 Jun 2018 | Differential Expression Analysis using R at the III interactional Summer School in Bioinformatics, Institute of Mathematics, National Autonomous University of Mexico, Queretaro, Mexico"
  },
  {
    "objectID": "cvVijay.html#publications",
    "href": "cvVijay.html#publications",
    "title": "Curriculum vitae",
    "section": "Publications",
    "text": "Publications"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Page under construction"
  },
  {
    "objectID": "resources.html#publications-to-pathway-mapping-pub2path",
    "href": "resources.html#publications-to-pathway-mapping-pub2path",
    "title": "Resources",
    "section": "Publications to Pathway mapping (pub2path)",
    "text": "Publications to Pathway mapping (pub2path)\npub2path is a R package to reconstruct biological pathway or process of your interest without a prior knowledge of gene functions from PubMed literature. Access will be given on request since pub2path manuscript is under preparation. CDSG database (see below) was created using pub2path results."
  },
  {
    "objectID": "resources.html#neurodegenerative-disease-gene-expression-database-ndgex",
    "href": "resources.html#neurodegenerative-disease-gene-expression-database-ndgex",
    "title": "Resources",
    "section": "Neurodegenerative Disease Gene Expression database (NDGEx)",
    "text": "Neurodegenerative Disease Gene Expression database (NDGEx)\nNDGEx is a database of differential expression results in 25 neurodegenerative diseases from 155 independent microarray datasets."
  },
  {
    "objectID": "resources.html#cell-death-and-survival-gene-database-cdsg",
    "href": "resources.html#cell-death-and-survival-gene-database-cdsg",
    "title": "Resources",
    "section": "Cell Death and Survival gene database (CDSG)",
    "text": "Cell Death and Survival gene database (CDSG)\nCDSG ia a database of 8,380 predicted genes from 175 species linked with literature pub2path algorithm (unpublished) on cell death and survival."
  },
  {
    "objectID": "resources.html#escherichia-coli-functional-protein-protein-interactions-ecofunppi",
    "href": "resources.html#escherichia-coli-functional-protein-protein-interactions-ecofunppi",
    "title": "Resources",
    "section": "Escherichia coli functional protein-protein interactions (EcoFunPPI)",
    "text": "Escherichia coli functional protein-protein interactions (EcoFunPPI)\nEcofunPPI ia a database of functional protein-protein interactions in Escherichia coli K12 predicted using a combination of 9 machine learning methods. It is highly reliable database than several existing published database on PPI and could provide a experimentally testable hypothesis on proteins of interest."
  },
  {
    "objectID": "protocols.html",
    "href": "protocols.html",
    "title": "Protocols",
    "section": "",
    "text": "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R\n\n\nTranscriptional regulatory network predictions\n\n\n\n\n\n\nVijaykumar Yogesh Muley\n\n\n55 min\n\n\n\n\n\n\nNo matching items"
  }
]