{
  "hash": "d3154e2822d3a8a9e783f894f2c0e0b0",
  "result": {
    "markdown": "---\ntitle: \"Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R\"\nsubtitle: Transcriptional regulatory network predictions\nauthor:\n  - name: Vijaykumar Yogesh Muley\ncategories: [Quarto, R, MEDS] # self-defined categories\ncitation: \n  url: https://doi.org/10.1007/978-1-0716-3461-5_15 \nimage: preview_image.jpg\ndraft: false \n---\n\n\n# Introduction\n\nIn the past two decades, machine learning has become a\npowerful tool for predicting gene regulatory networks (GRN)\nor transcriptional regulatory networks (TRN), which involve\ntranscription factors (TFs) and their target genes\n[@Marbach2012; @Li2023; @Muley2022]. TFs\nare a diverse family of DNA-binding proteins that regulate\ngene expression by binding to specific DNA sequences in the\ntarget gene's upstream regions [@Dynan1983;\n@Vaquerizas2009]. Understanding GRNs is crucial for studying\ntranscriptional regulation in various cellular processes,\nsuch as development, diseases, and response to\nenvironmental stimuli [@Muley2020; @Levine2003; @Salah2016].\n\nMachine learning, a sub-field of artificial intelligence,\nhas revolutionized complex problem-solving across various\ndomains, including finance, marketing, engineering,\nmedicine, and life sciences [@Greener2022; @Lecun2015]. It\nenables machines to learn and improve from experience\nwithout explicit programming. Machine learning has proven to\nbe a successful approach in predicting GRNs by utilizing co-expression\npatterns among TFs and their target genes [@Marbach2012]. This is achieved by harnessing the vast amount of\npublickly available microarray and next-generation\nsequencing RNA-Seq data [@geo]. These\ndatasets, known as expression profiles, are represented\nas a matrix with dimensions $m * n$. Where, $m$ represents the\ntotal number of genes expressed by an organism, and $n$\ncorresponds to the number of physiological conditions or\nexperimental contexts in which gene expression was measured.\n\nIn simple terms, gene expression varies across different physiological conditions, resulting in distinct features or attributes. Genes and TFs themselves can be used as training examples for machine learning. Unsupervised machine learning algorithms can classify genes into groups based on similar features, particularly correlated expression. Genes that belong to the same group are often co-regulated and have functional relationships, offering insights into potential TFs that drive their correlated expression when they are part of the same group [@Barzel2013]. Clustering algorithms are examples of unsupervised machine learning methods that can infer TF-gene relationships without prior knowledge.\n\nIn contrast, supervised machine learning algorithms work with pre-defined groups. For instance, pairs of TFs and genes can be divided into two pre-defined groups: regulatory and non-regulatory pairs. In machine learning, these pre-defined groups are referred to as labels. Supervised algorithms learn a mathematical function, known as a classifier or model, to distinguish between labels by identifying label-specific patterns/rules from the input features [@Marbach2012]. The resulting classifier can then be used to assign the most appropriate labels to unclassified TF-gene pairs. The labeled data used to train the model is known as the gold standard or ground truth. Both types of machine learning utilize the inherent structure of the input data to uncover complex and hidden trends for classifying unlabeled examples, regardless of whether prior knowledge is available.\n\nIn traditional machine learning, researchers typically manually engineer features to train algorithms. However, deep learning, a subset of machine learning that utilizes neural networks, has the ability to automatically learn features from input data [@SCHMIDHUBER201585; @Lecun2015]. Deep learning models are structured into layers, with each layer responsible for transforming input data, extracting meaningful features that differentiate between labels, and generating output - the predicted labels. The depth of the model is determined by the number of layers it has. Within each layer, nodes or neurons perform calculations using activation functions, gradually transforming the data into more abstract representations [@Glorot2011; @Cybenko1989]. This process continues through the layers until the final output is produced. Through backpropagation, the model adjusts its parameters or weights to correct errors when the output doesn't match the expected gold standard labels, thereby improving accuracy. The resulting classifier can then be used to classify unclassified TF-gene pairs into either regulatory or non-regulatory groups based on the associated features. Deep learning models excel in analyzing and combining multiple features, leading to higher accuracy and greater capability compared to traditional machine learning models.\n\n\nThis chapter focuses on implementing a deep learning classifier using TensorFlow, an open-source software framework developed by Google. The classifier is built using the Keras API within the RStudio environment. The main goal of this chapter is to demonstrate a step-by-step protocol for predicting regulatory interactions between TFs and genes, using their expression profiles as the key features.\n\nThe protocol utilizes publicly available gene expression data and gold standard information for *Escherichia coli*, obtained from the DREAM5 project publication [@Marbach2012]. It guides readers through essential steps including data preprocessing, designing the neural network architecture, model training and validation, and making predictions for novel regulatory interactions. Additionally, the chapter provides valuable insights into parameter tuning for deep learning models.\n\nBy the end of this chapter, readers will have a comprehensive understanding of how to apply deep learning techniques to predict regulatory interactions between TFs and genes. This knowledge can be directly applied to their own research projects, enabling them to harness the power of deep learning in their investigations.\n\n# Material\n\n## Computational and software requirements\n\n1.  This protocol has been tested on a desktop\n    computer/laptop with 8 GB RAM running Windows and Mac\n    OS. The computer's configuration required to fulfill\n    learning and prediction tasks depend on the problem's\n    complexity and the size of the training and unlabeled\n    data. It is recommended to avoid running other\n    applications simultaneously.\n\n2.  While this protocol contains sufficient details to be\n    followed by scientists without programming experience,\n    basic to intermediate R programming skills are expected.\n\n3.  Download and install R and its graphical user interface,\n    RStudio, from <https://www.r-project.org> and\n    <https://www.rstudio.com>, respectively.\n\n4.  To efficiently execute R commands and document the\n    entire protocol, create a file in RStudio by navigating\n    to the File menu, selecting New File, and then choosing\n    R script. By default, the file will be named\n    \"untitled.R\". It is recommended to save this file with a\n    meaningful name in the folder where you will execute\n    this protocol.\n\n5.  Next, go to the Session menu in RStudio, select Set\n    Working Directory, and then click on To Source File\n    Location, i.e., the directory where the untitled.R file\n    was saved. This ensures that all the commands will be\n    executed in the same directory, and the location will be\n    used as a reference to import or export the data saved\n    and generated for this protocol.\n\n6.  Going forward, the \"untitled.R\" file will be used to\n    input and save all R commands, allowing you to easily\n    reproduce the results from scratch. To execute the\n    commands, simply select them using the mouse or\n    keyboard, and click on the \"Run\" option located in the\n    top right corner of the file in RStudio. The output of\n    the executed commands will be displayed in the console\n    panel located below the file panel.\n\n7.  This protocol relies on Keras and TensorFlow R packages,\n    i.e., external libraries for deep learning-specific\n    functions [@keras; @tensorflow]. To install these\n    packages, copy the commands described below to your\n    \"untitled.R\" file, select them, and execute them using\n    the Run option. For more details about the following\n    commands, please refer to their source website:\n    <https://tensorflow.rstudio.com/install/>, and also\n    <https://www.tensorflow.org/install>.\n\n8.  To install the tensorflow R package, run the following\n    command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tensorflow\")\n```\n:::\n\n\n9.  Before using the tensorflow package, we need to load the\n    reticulate library and configure R to use a Python\n    installation. If you already have Python installed, you\n    can skip the install_python() command and provide the\n    path to the Python executable. Here's an example of how\n    to do it on macOS or Unix/Linux operating systems:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# assuming python is not already installed\n\nlibrary(reticulate)\npath_to_python <- install_python()\nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n\n# assuming that the python is already installed and \n# executable is available in /usr/local/bin/ folder (check with /usr/bin/ also)\n\nlibrary(reticulate)\npath_to_python <- \"/usr/local/bin/python3\" \nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n```\n:::\n\n\n10. Next, use the install_tensorflow() function from the\n    tensorflow package to install TensorFlow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-reticulate\")\n```\n:::\n\n\n11. Alternatively, you can use the install_keras() function\n    from the keras package to install TensorFlow along with\n    some commonly used packages such as \"scipy\" and\n    \"tensorflow-datasets\" as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras(envname = \"r-reticulate\")\n```\n:::\n\n\n12. To confirm that the installation was successful, run the\n    following command, which should return\n    *tf.Tensor(b'Hello Tensorflow!', shape=(),\n    dtype=string)*. If not then try \"install_tensorflow()\" command again. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow!\")\n```\n:::\n\n\n13. Install following packages to facilitate data handling\n    and analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"readr\", \"tibble\", \"caret\", \"verification\"))\n```\n:::\n\n\n## Data requirements\n\n1. To begin, download Supplementary Data 1 from the DREAM5 network inference challenge publication (https://doi.org/10.1038/nmeth.2016) and unzip it in your current working directory. This action will create a directory named \"DREAM5_network_inference_challenge.\" For detailed information about this data, please refer to the original publication by Marbach et al. [@Marbach2012]. If you have your own data, it should have the format similar to that of first three files described below. \n\n2. The required data for Escherichia coli is present in the \"Network3\" folder and its sub-folders. Copy four files listed below from these sub-folders into your current working directory to proceed with the analysis..\n\n3. *net3_expression_data.tsv*: This file contains the expression profile data.\n\n4. *net3_transcription_factors.tsv*: This file contains the list of transcription factors.\n\n5. *DREAM5_NetworkInference_GoldStandard_Network3.tsv*: This file contains the gold standard data, which includes both regulatory (positive examples) and non-regulatory (negative examples) TF-gene pairs.\n\n6. *net3_gene_ids.tsv*: This file serves as a mapping between the anonymous gene codes used in other files and the original gene names.\n\n\n# Methods\n\n## Loading R libraries and preparing workspace\n\n1.  Load the Keras and\n    TensorFlow libraries for use in the current RStudio\n    session as shown in below along with magrittr. The magrittr library provides a useful function\n    called *pipe* (i.e., %>%), which allows us to pass the\n    output of one operation to another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n```\n:::\n\n\n2.  Set a random odd number as a seed in the set.seed\n    function to reproduce the results of this workflow (see\n    **Note 1**). If you use seed number other than 1979, your results will be slightly different than the discusseed in the following text.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1979)\n```\n:::\n\n\n## Data preparation and exploration\n\n1.  Import the gene expression data and store in a exp\n    object and do a routine check up as shown below. **Note\n    2** enlists popular public resources to obtain gene\n    expression data for species of interests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp <- read.table(\"net3_expression_data.tsv\", header = T)\nexp <- t(exp)\ndim(exp)\nexp[1:5,1:5]\n```\n:::\n\n\nThe exp object is a data.frame containing the genes of\n*Escherichia coli* in the rows and their expression levels\nin diverse physiological conditions and time-points in the\ncolumns. Note that the genes are anonymized for the DREAM5\nchallenge [@Marbach2012], and we will map them back to their original gene\nnames later.\n\n2.  Normalization is a crucial step in data analysis, and\n    gene expression data should be normalized if required\n    (see **Note 3**). In this case, the data was already\n    normalized, as indicated by the distribution plot\n    generated using the following command and exported to PDF file i.e. *DeepLearning_Figure1.pdf*. The plot shows\n    that the data is normally distributed and ready for\n    further analysis (@fig-figure1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = 'Expression intensity', main = \"Gene expression\")\n\n# export plot to pdf file in the current directory\npdf(file = 'DeepLearning_Figure1.pdf', \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = 'Expression intensity', main = \"Gene expression\")\ndev.off()\n```\n:::\n\n\n![**Expression distribution of Escherichia coli genes**. The shape of the histogram reveals a normal distribution of expression data. This suggests that the expression data may not require additional processing.](DeepLearning_Figure1.pdf){#fig-figure1 fig.pos='b' height=100% }\n\n\n3.  Load the gold standard data using the read_table\n    function from the readr package and display its\n    contents, as shown below. **Note 4** enlists popular\n    public resources that can be used to create gold\n    standard data for species of interests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold <- readr::read_table(\"DREAM5_NetworkInference_GoldStandard_Network3.tsv\",\n                          col_names = F)\ngold\n```\n:::\n\n\nIn the gold object, the first two columns respectively list\nthe TFs and genes, while the third column\ncontains binary labels. A value of 1 in the label column\nindicates that experimental evidence exists for the\nregulation of the gene in column 2 by the transcription\nfactor in column 1, while a value of 0 indicates that there\nis no known regulatory interaction between them.\n\n3.  With the following command, TFs and genes were retained\n    in the gold standard data if they were also present in\n    the expression data. Next, we use the table function\n    from base R to count the number of regulatory and\n    non-regulatory pairs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold <- gold[gold$X1 %in% rownames(exp) & gold$X2 %in% rownames(exp), ]\ntable(gold$X3)\n```\n:::\n\n\n4.  The gold standard data is imbalanced, with 2066\n    regulatory pairs and 150,214 non-regulatory pairs. This\n    is usually a common issue in biology, and training deep\n    learning model on imbalanced gold standard can be tricky\n    (see **Note 5**), which is explained in detail later.\n    For time being, we will select all regulatory pairs and\n    sample two times the number of non-regulatory pairs\n    randomly as shown below. This is called down-sampling of majority class examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkeep_indices <- c(which(gold$X3==1), \n                  sample(which(gold$X3==0), size = sum(gold$X3)*2))\ngold <- gold[keep_indices,]\ntable(gold$X3)\n```\n:::\n\n\n5. The data preparation process involves combining expression values for each TF-gene pair, allowing the raw data to be efficiently processed by a deep learning model for feature extraction. This step creates a data.frame that includes TF-gene pairs (optional) and their corresponding expression values. Next, the Pearson Correlation Coefficient (PCC) is calculated between the expression values of the TF and the gene in each pair. This correlation measure is a conventional approach for engineering features from raw data. The resulting correlation coefficients are added as a new column named \"pcc\" within the data.frame. Subsequently, labels are appended as the last column of the data.frame. The first two columns of the data.frame represent the names of the TF and gene, respectively. The remaining columns contain the expression values for the TF and gene, followed by the calculated PCC values between their expression values, and finally the gold standard label.\n\nTo facilitate further analysis, the column names that correspond to the expression values and PCC are stored in the \"featurenames\" object, as they will serve as features for subsequent processing.\n\nThe code snippet below demonstrates these steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputdata <- data.frame(tf = gold$X1, gene = gold$X2, \n                        exp[gold$X1,], exp[gold$X2,])\ninputdata$pcc <- sapply(seq.int(dim(gold)[1]), function(i) \n                        cor(exp[gold$X1[i],], exp[gold$X2[i],]))\ninputdata$class <- gold$X3\n\ninputdata <- tibble::as_tibble(inputdata)\n\nfeaturenames <- colnames(inputdata) %>% setdiff(\"class\")  %>% \n  setdiff(\"tf\") %>% setdiff(\"gene\")\n```\n:::\n\n\n6. Machine learning models are built upon certain assumptions, that the input data inherently contains relevant structure for the classification task at hand. In our specific case, we assume that the expression of TFs is directly proportional to the expression of their target genes, as TFs regulate the activity of these genes. However, it's important to acknowledge that this assumption may not always hold true due to various factors, such as post-translational modifications [@Deribe2010]. This introduces non-linearity into the input data, making machine learning in non-linear high-dimensional spaces challenging and potentially leading to the creation of poor models.\n\nTherefore, it is crucial to exercise caution when selecting data for modeling purposes. @fig-figure2 visually illustrates a clear distinction between expression values of TF-gene regulatory pairs and randomly selected pairs, thus indicating the suitability of using supervised learning techniques. The provided code snippet generates @fig-figure2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh1 <- hist(as.matrix(inputdata[inputdata$class==\"1\",featurenames]), \n           breaks = 100, plot = FALSE)\nh2 <- hist(as.matrix(inputdata[inputdata$class==\"0\",featurenames]), \n           breaks = 100, plot = FALSE)\n\npdf(file = 'DeepLearning_Figure2.pdf', \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nplot(h2, col = rgb(0,0,1,0.4), freq = FALSE, \n     xlab = 'Expression intensity', \n     main = \"Distribution of feature values for training data\")\nplot(h1, xaxt = 'n', yaxt = 'n', col = rgb(1,0,0,0.4), \n     freq = FALSE, add = TRUE)\ndev.off()\n```\n:::\n\n\n![**Expression distribution of Escherichia coli TF-gene pairs**. The expression value distribution of non-regulatory TF-gene pairs shows unimodal normal distribution (purple color), while regulatory pairs show bimodal distribution, in which expression values surronding the first peak overlaps with the distribution of non-regulatory pairs, and the other peak ranges in a higher expression value zone, and distinct from the other two peaks. One of the explaination for this trend is that TF and genes of the regulatory pairs are dependant and express at higher levels than those form non-regulatory pairs. It indicate that expression data without processing contains regulatory signals and can be used for deep learning. ](DeepLearning_Figure2){#fig-figure2 fig.pos='b' height=100% }\n\n\n7.  In general, gold standard data is divided into training, validation,\n    and test sets, which is for training the model, validating the performance while model is being produced, and to estimate the accuracy final model in real world data. The division is often based on availability of the data. Typically, 60-80% of the input data retains for training, while\n    40-20% is equally divided between validation and test datasets. The following code implements this process and create train,validation and test datasets in 80:10:10 proportions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindices <- nrow(inputdata) %>% sample.int(., ceiling( . * 0.8))\ntraindata <- inputdata[indices, ]\ntestdata <- inputdata[-indices, ]\nindices <- nrow(testdata) %>% sample.int(., ceiling( . * 0.5))\nvaldata <- testdata[-indices, ]\ntestdata <- testdata[indices, ]\ntable(traindata$class)\ntable(valdata$class)\ntable(testdata$class)\n```\n:::\n\n\n## Defining deep learning model architecture\n\n1.  To define the architecture of our deep learning model, we first\n    create a sequential deep neural network (DNN) using the\n    keras_model_sequential() function from the Keras\n    library. We then add two dense layers using the\n    layer_dense() function. The input layer is defined with\n    the input_shape parameter, which needs to be equal to\n    the total number of features in the input data or represents the shape of the features. Next, the\n    single hidden layer contains 806 neurons and applies the\n    \"relu\" activation function to introduce non-linearity in\n    the layer's output @Glorot2011. The second layer_dense()\n    function adds another fully connected layer with a\n    single output unit. This output layer computes a\n    weighted sum of the inputs from the previous layer feeds\n    and applies the sigmoid activation function to produce a\n    probability value between 0 and 1 @Cybenko1989,\n    indicating the likelihood that the input corresponds to\n    the positive class (regulatory pairs, in this case) based on input features. It\n    is worth mentioning that successive layers are able to\n    dynamically interpret the number of expected inputs\n    based on the previous layer. Below is a typical code to\n    define the DNN architecture.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_random_seed(1979)\n\nmodel <- keras_model_sequential(\n  input_shape = c(length(featurenames))) %>%\n  layer_dense(units = 806, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsummary(model)\n```\n:::\n\n\n2.  The code above includes a set_random_seed function for\n    setting a random seed for TensorFlow operations to\n    reproduce deep learning model (see **Note 6**). \n\n3.  The number of layers and units can be adjusted to\n    improve the model's capacity to learn, but too much\n    capacity can lead to overfitting, and too low underfit\n    the data (see **Note 7**). The single hidden layer contains 806 neurons which is equivalent to the total number of physiological conditions in the expression data (columns) and the pcc column representing PCC for each TF-gene pair. \n\n4.  The summary function gives an overview of the model's\n    architecture. The model has a simple architecture with\n    input layer which feeds feature matrix and labels to fully connected layers hidden layer with \"relu\"\n    activation, and a another fully connected output layer with \"sigmoid\"\n    activation to produces a single probability value for\n    each training example of being regulatory pair.\n\n## Defining Model Compilation Parameters\n\n1.  To configure the learning process, the model's\n    compilation parameters are specified using the following\n    code. During training, the DNN\n    randomly assigns weights to all the neurons and their\n    connections, and predicts the output. The DNN then\n    assesses the accuracy of the output, and if necessary,\n    automatically adjusts these weights to improve accuracy\n    via the backpropagation process. The DNN requires an\n    objective function to measure performance, which is\n    referred to as the loss or objective function, as well\n    as an optimizer function that directs learning towards\n    the minimum loss by keeping track of previous best\n    performances. The following code defines the compilation\n    parameters for the DNN.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(optimizer = optimizer_adam(),\n loss = \"binary_crossentropy\",\n metrics = c(\"accuracy\"))\n```\n:::\n\n\n2.  The compile() function configures the model.\n\n3.  The optimizer_adam() function is a widely used\n    optimization algorithm that computes adaptive learning\n    rates for each parameter based on the moment estimates\n    of the gradient and past gradients and helps neural\n    network to converge faster @kingma2017adam. See **Note\n    8** for more details on learning and optimizer.\n\n4.  The loss function measures the difference between\n    predicted and actual values. The \"binary_crossentropy\"\n    loss function is commonly used for binary classification\n    tasks.\n\n5.  In the above example, the \"accuracy\" metric is used to\n    calculate the percentage of correctly predicted labels\n    by the model. Accuracy metric is fine for prototyping\n    DNN, but use more reliable metrics for practical\n    application (see **Note 9**).\n\n## Training deep learning model\n\n1.  The following code chunk fits the DNN model to the\n    training data and validates its performance using a\n    validation dataset by using the fit() function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(as.matrix(traindata[,featurenames]), \n                         traindata$class,\n                         epochs=100, batch_size = 64,\n                         validation_data = list(as.matrix(valdata[,featurenames]), \n                                                valdata$class))\n```\n:::\n\n\n2.  The fit() function takes the training data in the form\n    of a matrix of predictor variables, and the\n    corresponding binary class labels, epochs, batch size, and validation data.\n\n3.  Epochs determine how many times the entire dataset is\n    used for training the DNN. More epochs can\n    improve performance, but there is a risk of overfitting\n    to the training data (see **Note 10**).\n\n4.  Batch size refers to the number of examples processed\n    together in each training iteration. Smaller batch sizes\n    can improve generalization, but convergence may be\n    slower. Larger batch sizes speed up training but may\n    result in poorer generalization. The optimal batch size\n    depends on the dataset size and available computational resources, especially memory\n    (see **Note 11**).\n\n5.  The code uses validation data (optional) to assess the\n    model's performance during training. Alternatively, use\n    the validation_split parameter to specify the percentage\n    of training data for validation (see **Note 12**). These\n    parameters are optional and can be adjusted to suit the\n    model and data requirements.\n\n## Estimating the accuracy of deep learning model\n\n1.  The history object records the loss and accuracy during the training over specified number of epochs. This information can be used to quickly check the performance of the model during training. The print(history) command shows that at the final epoch, the loss was 0.4628 on training data which is higher than the validation data i.e. 0.4296. Lower loss is better fit. Then, the accuracy was 0.7937 and 0.8174 respectively. We can also use evaluate function to compute accuracy and loss function over all epochs as shown below. The code plot(history)  generates a plot of the training and validation loss and  accuracy over the course of the training process. This  visualization can help identify trends or patterns in  the performance of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(history)\nevaluate(model, x = as.matrix(traindata[,featurenames]), y = traindata$class)\nevaluate(model, x = as.matrix(valdata[,featurenames]), y = valdata$class)\nplot(history)\n```\n:::\n\n\n2.  However, I prefer to access the 'loss' and 'accuracy'\n    attributes from the history object, as this allows me\n    the flexibility to visualize the data in a customized\n    manner. For instance, @fig-figure3 was generated using the\n    customized code provided below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf(file = 'DeepLearning_Figure3.pdf',\n    width = 7, height = 4, pointsize = 10, useDingbats = FALSE)\n\npar(mfrow = c(1,2), cex = 0.8)\n\nmaxLoss <- max(c(history$metrics$loss, history$metrics$val_loss))\n\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"Epoch\", ylab=\"Loss\", \n     xlim = c(0, length(history$metrics$loss)), ylim = c(0, maxLoss),\n     col=\"red2\", type=\"b\",lwd=1)\nlines(history$metrics$val_loss, col=\"steelblue1\", type=\"b\",lwd=1)\nlegend(\"topright\", c(\"Training\",\"Validation\"), col=c(\"red2\", \"steelblue1\"), \n       lty=c(1,1), bty = \"n\")\n\nplot(history$metrics$accuracy, col=\"red2\", type=\"b\",lwd=1, \n     main=\"Model Accuracy\", xlab = \"Epoch\", ylab=\"Accuracy\", \n     xlim = c(0, length(history$metrics$accuracy)), ylim = c(0, 1))\nlines(history$metrics$val_accuracy, col=\"steelblue1\", type=\"b\",lwd=1)\nabline(h = 0.5, col = \"grey\", lty = 3)\ndev.off()\n```\n:::\n\n\n![**Expression distribution of Escherichia coli TF-gene pairs**. The distribution of expression values for non-regulatory TF-gene pairs follows a unimodal normal distribution (purple color). However, regulatory pairs exhibit a bimodal distribution (showed in reddish-orange colors). The first peak of the regulatory pairs' distribution overlaps with the distribution of non-regulatory pairs, while the second peak corresponds to higher expression values (orange color) and is distinct from the other two peaks. One possible explanation for this trend is that the TF and genes in regulatory pairs are dependent on each other and express at higher levels compared to non-regulatory pairs.](DeepLearning_Figure3){#fig-figure3 fig.pos='b' height=100% }\n\nUpon observing the plot, we can identify certain inconsistencies (@fig-figure3). In a typical training process, the loss and accuracy gradually improve over multiple epochs, as evident in the plot. Initially, the loss experiences a steep decline up to the fifth epoch, after which it stabilizes from the tenth epoch onwards. The overall accuracy demonstrates a similar trend. These step-wise improvements indicate the model's effectiveness as it undergoes more training. The accuracy and loss after 30 epochs do not change much and can be set as the optimum number for epochs.  \n\nIdeally, the accuracy and loss on the validation data should not exceed those on the training data since the model is trained on the latter. However, our results violate this assumption because validation data has low loss and high accuracy than the training data. If such discrepancies occur, it could indicate issues with the training process, model architecture, or the training and validation data itself and often referred to as over- or under-fitting of the input data.\n\nIn our specific case, multiple issues may coexist due to arbitrary parameter settings, incomplete gold standard data, an uneven distribution of positive and negative examples in the training and validation data, and the accuracy metric may not be good for our imbalanced data. However, these problems can be addressed during the model parameter tuning process, and explained in the next section. It is important to note that despite the model's performance not being ideal, the features do support the assumed classification task. The improvements in the loss and accuracy functions during training are smooth and indicative of good performance. While there is an issue with training and validation accuracy, it is marginal and may be a result of the imbalance in the gold standard data. Hence, it is crucial to evaluate the model's performance on the test data to estimate its practical accuracy. \n\n2.  The code provided below utilizes our model to predict the probabilities of outcomes for the test dataset, which is unlabelled and completely unknown to the model. This prediction is achieved by using the predict() function, and the resulting probabilities are stored in the variable predprob for further utilization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredprob <- model %>% predict(as.matrix(testdata[,featurenames]))\n```\n:::\n\n\n6.  To evaluate the performance of the DNN model on the test data and compare it with the training and validation data, the confusionMatrix() function from the caret package can be used. This function takes the predicted class probabilities in a binary format, specifically using a threshold of 0.5 or above (i.e., as.numeric(predprob > 0.5)), along with the true class labels. It then generates a table containing various important performance measures as shown below. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nf1 <- function(indata, truelabels){\n  res <-  model %>% predict(as.matrix(indata[,featurenames]))\n  res <- as.numeric(res>0.5)\n  res <- factor(res, c(1,0))\n  caret::confusionMatrix(res, factor(truelabels, c(1,0)))\n}\n\ntrainAccuracy <- f1(traindata, traindata$class)\nvalidationAccuracy <- f1(valdata, valdata$class)\ntestAccuracy <-f1(testdata, testdata$class)\n\nperformance <- round(t(as.data.frame(rbind(t(trainAccuracy$byClass), \n                                           t(validationAccuracy$byClass), \n                                           t(testAccuracy$byClass)))),3)\ncolnames(performance) <- c(\"Traning\", \"Validation\", \"Test\")\n\nperformance\n```\n:::\n\n\nThe DNN model has an accuracy of 0.8323 on test data, which is\nagain better than the 0.8072 of training data, with a sensitivity of\n0.5638 and specificity of 0.9491. In other words, the features\nused for training are likely to distinguish 55.96% of real\nregulatory interactions and 94.91% of non-regulatory pairs in the unlabelled data.  \n\n7.  Biologists may prefer to use a higher probability cutoff\n    to avoid false positive results, even if it means\n    predicting fewer regulatory interactions. The ROC curve\n    is a perfect instrument which shows how the classifier's\n    performance changes at different threshold settings for\n    guidance. In the following code, roc.plot function from\n    verification package is used to plot ROC curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf(file = 'DeepLearning_Figure4.pdf',\n    width = 5, height = 5, pointsize = 12, useDingbats = FALSE)\n\nverification::roc.plot(testdata$class, predprob, \n                       ylab = \"True Positive Rate\", \n                       xlab = \"False Positive Rate\")\nabline(v = 0.5596, col = \"red2\", lty = 3)\nabline(h = 0.0509, col = \"steelblue1\", lty = 3)\n\ndev.off()\n```\n:::\n\n\n![**The Receiver Operating Characteristic (ROC) curve for the deep learning model's performance on the test data**. The ROC curve shows that the model's discriminatory power and performance is very good at the standard probability value cutoff of 0.5, at which the proportion of actual negative cases incorrectly classified as positive i.e. false positive rate by the model are very low, and over 55% actual positive cases correctly classified as positive by the model, indicating that the model may perform well in practical applications.](DeepLearning_Figure4){#fig-figure4 fig.pos='b' height=100% }\n\n\nThe commands provided above generate @fig-figure4, which illustrates the trade-off between the true positive rate (TPR or sensitivity) and the false positive rate (FPR or 1-specificity) at various probability cutoffs. This plot assists in determining the optimal probability cutoff value to achieve the best results. The 0.5 probability cutoff, which we used to estimate accuracy, is indicated on the plot. It shows that at this cutoff, only 5.09% of the predicted regulatory pairs are expected to be incorrect, while 55.96% of the predicted regulatory pairs will be correct. \n\n8. In summary, all of the aforementioned results indicate that our model shows promising potential in practical applications. The following code shows that how to save the model for future use, and we also save performance table, expression and gold standard data which we all need to reproduce all the results or to make new better deep learning model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_hdf5(model, \"EcoliRegModel.h5\")\nsave(exp, performance, gold, history, file = \"EcoliRegModel.RData\")\n```\n:::\n\n \nThe save_model_hdf5() function saves the model in the Hierarchical Data Format version 5 (HDF5) file format. HDF5 is a file format designed to store and organize large amounts of numerical data. It is a binary file format that can store the model architecture, weights, optimizer state, and any other information related to the model.\n \n \n10. To prepare for the next section, it is assumed that all the commands mentioned above have been saved in a file named \"Untitled.R\" or with a similarly informative name. If necessary, restart RStudio to ensure a smooth transition. Restarting may not be essential but it can potentially expedite the subsequent analysis by clearing data dump from memory, particularly the prediction of the global gene regulatory network (GRN). Alternatively, you can proceed to the next section after executing the following commands.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngc()\nrm(history, testdata, traindata, valdata, predprob)\ngc()\n```\n:::\n\n\n\n## Predicting genome-wide regulatory interactions\n\n1. In this section, we will demonstrate how to evaluate the pairwise combinations of transcription factors (TFs) and genes in *Escherichia coli* using our model. We will assess the probability of these pairs being potential regulatory pairs. It's important to note that the total number of TF-gene pairs for *Escherichia coli* exceeds a million and a half, and in higher organisms, the number is even greater. Consequently, most computers may struggle to handle the feature data for all these pairs, potentially causing crashes. Therefore, the following code is specifically provided for users who do not possess high-end computational resources. For those with ample computational resources, I recommend referring to the code provided in **Note 13**. As these codes can be used across different sessions of RStudio, they assume that the computer or RStudio has been restarted and the same directory has been set (can be verified using the getwd() function). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\nset.seed(1979)\n\n\nmodel <- load_model_hdf5(\"EcoliRegModel.h5\")\n\n# import expression data file\nexp <- t(read.table(\"net3_expression_data.tsv\", header = T))\n\n# import original gene names mapping \ngenenames <- read.table(\"net3_gene_ids.tsv\", header = F)\ngenes <- genenames$V2; names(genes) <- genenames$V1\n\n# import list of all transcription factors of Escherichia coli\n# tfs <- names(genes)[genes %in% c(\"gadX\", \"flhC\", \"flhD\",\"dnaA\")] # trail run\ntfs <- read.table(\"net3_transcription_factors.tsv\", header = F)$V1\n\nlength(tfs)*nrow(exp)\n\n\npredictions <- NULL\n\nfor(i in tfs){\n  \n  tfdata <-data.frame(tf = i, gene = rownames(exp), \n                      tfname = genes[i],\n                      genename = genes[rownames(exp)])\n  tfdata <- tibble::as_tibble(tfdata[tfdata$tf != tfdata$gene,])\n  \n  inpreddata <- cbind(exp[tfdata$tf,], exp[tfdata$gene,])\n  inpreddata <- cbind(inpreddata, \n                      sapply(seq.int(dim(tfdata)[1]), \n                             function(i) \n                               cor(exp[tfdata$tf[i],], \n                                   exp[tfdata$gene[i],])))\n  \n  tfdata$probability <- (model %>% predict(inpreddata))[,1]\n  \n  predictions <- rbind(predictions, tfdata[tfdata$probability>0.5,])\n  \n}\n\npredictions <- predictions[rev(order(predictions$probability)),]\npredictions\n```\n:::\n\n\n\n4. Our model predicted only 80,225 as regulatory out of 1,506,674 TF-gene pairs. There are number of sophisticated ways to evaluate the predicted GRN [@Muley2022]. Here, however, we will quickly select few TF-gene pairs with high probability of being regulatory, and will study them. Escherichi coli has many acid inducible systems which protect cells  against acid stress to pH 2 or below. Of them, the glutamate depending system is the most effect and relies on two glutamate decarboxylases (GadA and GadB) combined with a putative glutamate:gamma-aminobutyric acid antiporter (GadC). According to our predictions, it appears to be regluated by OsmE, which is osmolarity inducible factor, however, there is no information exists that it regulates glutamate-dependant acid resistance system. though functionally it would make sense. The GadX is a known regulator of the acid resistance system. Also, master transcription factors associated with flagella biosynthesis and chemotaxis were among the highly ranked TF-gene pairs. So, lets check target genes of GadX, FlhC, and FlhD transcription factors.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions[predictions$tfname==\"gadX\",]\npredictions[predictions$tfname==\"flhC\",]\npredictions[predictions$tfname==\"flhD\",]\n```\n:::\n\n\nOur results suggests that GadX may regulate 16 genes at the probability cutoff of 0.5 or above, and of 16, five genes i.e. gadA, gadE gadB, gadC and gadW belong to glutamate dependant acid protection system, confirming the accuracy of our prediction system. Likewise, FlhC and FlhD regulates a large number of genes involved in flagellar assembly and chemotaxis reassuring our prediction system. The overlap of their target genes is 652, which is highly expected. One of the problem of deep learning algorithm is that they can memorize the training dataset and many of the predictions could be part of training or validation dataset. Therefore, lets remove all predictions that were part of the training, and again cross-check the results using following command. As you can see, there are still many predictions which were novel, and make perfect sense at functional level.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions[!paste0(predictions$tf,predictions$gene) %in%  \n              paste0(inputdata$tf[inputdata$class==1], \n                     inputdata$gene[inputdata$class==1]),] \n```\n:::\n\n\n\n4. To export results from prediction object to a tabular, csv or excel file, use following code. The third command requires writexl R package which should be installed using install.packages function. Likewise, you can also export the performance table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.table(predictions, file = \"grnPredictionsEcoli.txt\", \n            col.names = T, row.names = F, quote = F, sep = \"\\t\")\nwrite.csv(predictions, file = \"grnPredictionsEcoli.csv\", row.names = F)\nwritexl::write_xlsx(list(Table1 = predictions), \n                    path = \"grnPredictionsEcoli.xlsx\", col_names = T, )\n```\n:::\n\n\n\n## Tuning Deep Learning for Improved Performance\n\n1.  Developing an accurate DNN\n    requires considering crucial aspects and parameters.\n    Deep learning deals with high-dimensional spaces and\n    vast data in a complex representations, making it challenging to find the optimal\n    model configuration. Classification accuracy depends on\n    problem complexity, data quality, and training accuracy.\n    Fine-tuning hyperparameters that affect training process through practice and\n    experimentation is necessary. Various approaches can\n    enhance model performance.\n\n2.  By experimenting with different activation functions,\n    optimization algorithms, and loss functions, you can\n    fine-tune your deep learning model to achieve better\n    performance and more accurate predictions.\n\n3. By changing the activation function,\nyou can influence how the model learns and represents\ncomplex patterns. Experimenting with different activation\nfunctions like sigmoid, ReLU, or tanh can help improve the\nmodel's performance in capturing non-linear relationships\nand making accurate predictions.\n\n4. The choice of the optimization\nalgorithm affects how the model learns and converges to an\noptimal solution. Trying different optimization algorithms\nsuch as Adam, RMSprop, or stochastic gradient descent (SGD)\ncan impact the speed of convergence and the quality of the\nfinal model. See **Note 8** for details and functions in Keras.\n\n5. The loss function measures the model's\nperformance during training and guides the learning process.\nSwitching the loss function, such as using binary\ncross-entropy instead of mean squared error, can improve the\nmodel's ability to handle classification tasks and\noptimize for the desired output.\n\n3.  The design of the DNN involves carefully considering the number of layers and units, striking a balance between capturing complex relationships and avoiding overfitting for optimal configuration. Layers transform input data into meaningful representations, extracting relevant features. Deeper models with more layers learn intricate data representations, capture complex relationships, and discover abstract patterns. Most problems can be handled with one to three hidden layers, depending on linear and non-linear relationships in the features. The number of units within each layer impacts the model's capacity. Setting the number of units based on the total input feature shape, gradually decreasing, enables capturing finer details in the data. Adding more layers or units can enhance learning and generalization, but overfitting to training data becomes a risk. Finding the right balance involves iterative refinement and experimentation for optimal configuration that achieves desired performance without overfitting. See **Note 7** also.\n\n\n4.  Dropout is a regularization technique commonly used in\n    deep learning models to prevent overfitting [@dropout;\n    @JMLRv15srivastava14a]. It involves randomly dropping a\n    certain percentage of neurons or units in a layer during\n    the training process. This dropout of neurons helps the\n    network learn more robust and generalizable\n    representations by reducing the reliance on specific\n    input patterns. By randomly removing neurons,\n    layer_dropout encourages the model to learn from\n    different subsets of neurons, making it less sensitive\n    to individual neurons and reducing the risk of\n    overfitting. The dropout rate, which represents the\n    proportion of neurons to be dropped, is an important\n    hyperparameter that can be tuned to achieve optimal\n    performance and prevent overfitting.\n\n5.  The learning rate is a crucial parameter that determines the size of the steps taken by the model during training, influencing both the speed of convergence and the overall performance. If the learning rate is set too high, it can cause divergence, leading to a deterioration in the model's performance. On the other hand, a very low learning rate can result in slow convergence, prolonging the training process. It is common to start with default learning rates, which typically yield satisfactory results, and then fine-tune them as needed within the range of 0 to 1. The learning rate can be adjusted using the optimizer object, such as optimizer_adam(learning_rate = 0.001) in the compile function. \n\n6.  During each epoch, the model goes through one complete iteration of the training dataset, updating its parameters (weights and biases) based on observed patterns and the optimization algorithm. Performing multiple epochs is essential for the model to improve its predictions and overall performance. Choosing the appropriate number of epochs is a critical hyperparameter selection, ensuring a balance between learning meaningful patterns and preventing overfitting.\n\n7.  In deep learning, training is commonly performed using\n    mini-batches, which involve dividing the entire training\n    dataset into smaller subsets. The batch size refers to\n    the number of training examples processed together\n    before the model updates its internal parameters\n    (weights and biases) based on computed gradients. Larger\n    batch sizes can lead to faster training as more samples\n    are processed in parallel, but they also require more\n    memory. Conversely, smaller batch sizes allow for more\n    frequent model updates and may potentially improve\n    generalization. Choosing an optimal batch size depends\n    on factors such as computational resources, dataset\n    size, and model complexity. It often involves striking a\n    balance between training speed and memory usage. For\n    example, if the training data consists of 10 examples\n    and a batch size of 5 is used, one epoch would require\n    two passes of the training examples (10/5). The weights\n    are adjusted batch size times per epoch. Experimenting\n    with different batch sizes is recommended to find the\n    optimal balance for a specific deep learning task. See **Note 11** for more details.\n\n8. Batch normalization has been shown to reduce the need for a large number of training steps and, in some cases, the use of dropout, resulting in stable and faster convergence, particularly with higher learning rates [@pmlrv37ioffe15]. However, it has also been observed that using batch normalization in conjunction with dropout can sometimes lead to a decrease in performance. It is worth noting that our model, when included batch normalization for training, achieves 100% accuracy on the training set. Nevertheless, it is essential to interpret such results with caution and not solely rely on exact accuracy as an indicator of model performance. Achieving perfect accuracy may be indicative of overfitting, where the model memorizes the training data without truly understanding the underlying patterns. Therefore, it is crucial to evaluate the model's ability to generalize to unseen examples by assessing its performance on separate validation or test data. Additionally, considering other evaluation metrics and techniques, such as cross-validation or precision and recall analysis, can provide a more comprehensive understanding of the model's capabilities. Adopting this mindset will enable the development of reliable and robust machine learning models. It can be implemented in code using layer_batch_normalization() function, right after each hidden layer.\n    \n\n9.  Imbalanced training data, where positive and negative examples are unevenly represented, can introduce bias in the learning process. To mitigate this issue, various techniques such as stratification through over- or down-sampling and assigning different weights to classes have been proposed and extensively discussed in the literature. For coding examples related to handling imbalanced data, please refer to **Note 5**. \n\n10. Data augmentation is a technique commonly used in deep learning to expand the training dataset artificially. It involves applying various transformations, such as rotation, scaling, flipping, cropping, or adding noise, to the existing data. By diversifying the training data, data augmentation helps prevent overfitting and enhances the model's ability to generalize to new, unseen examples. Please refer to **Note 5** also. \n\n11. When evaluating model performance, it is important to consider both data-driven and field-specific preferences. For imbalanced training data, precision and recall metrics provide more meaningful insights than overall accuracy. It is crucial to use performance metrics wisely to guide model training, ensuring that the model is not biased towards the majority class. For coding examples related to handling performance metrics, please refer to **Note 9**. \n\n12. Callbacks are functions that offer customization options for the training process of a deep learning model. They allow for monitoring the model's progress during training and performing specific actions based on certain conditions. Examples of actions that can be taken using callbacks include saving model weights, adjusting the learning rate, early stopping, and logging metrics. By leveraging callbacks, deep learning models can be tailored to achieve improved performance, efficiency, and customization. For coding examples related to callbacks, please refer to **Note 14**.\n\n13. The R ecosystem provides extensive support and packages that can be utilized with the Keras R API for finding optimal hyperparameters in deep learning model development. One such package is kerasTuner, which is specifically designed for hyperparameter tuning with Keras models. It offers a user-friendly interface for defining a search space of hyperparameters and performing hyperparameter optimization using techniques like random search, hyperband, or Bayesian optimization. The seamless integration of kerasTuner with the Keras R API enables efficient identification of the best hyperparameters for deep learning models.\n\n14. I recommend users to explore prototype examples of deep learning available at https://tensorflow.rstudio.com/examples/. These examples serve as valuable resources for understanding and implementing deep learning techniques.\n\n\n\n# Notes\n\n1.  Randomization is often involved in data analysis or\n    machine learning tasks, such as shuffling datasets or\n    initializing weights in neural networks. Setting the\n    seed ensures that the same set of random numbers is\n    generated each time the code is run. This is\n    particularly useful when you need to reproduce and debug\n    results or share code with others, ensuring consistent\n    outcomes. The seed should be declared at the beginning\n    of your script or function with any non-negative integer\n    value, to ensure consistent results throughout your\n    code.\n\n2.  DNA microarray, RNA-Seq, and single-cell RNA-Seq data\n    are accessible for several species and almost all model\n    organisms through several public repositories. I\n    recommend users to explore the NCBI Gene Expression\n    Omnibus (GEO) [@geo], EBI ArrayExpress [@arrayexpress],\n    PanglaoDB [@panglaodb], and recount[@recount] databases\n    which offer processed gene expression and transcriptomic\n    data that have undergone quality control, normalization,\n    and sometimes additional analysis. For time series,\n    cells, and tissues data, the FANTOM5 consortium\n    [@fantom5] and The Genotype-Tissue Expression Project\n    (GTEx) [@gtex] databases provide valuable resources.\n    Additionally, for next-generation sequencing raw data,\n    the NCBI Sequence Read Archive (SRA) is a valuable\n    repository [@sra].\n\n3.  Normalization of gene expression, RNA-seq, and scRNA-seq\n    data is a statistical process that adjusts the\n    expression values to remove systematic biases and\n    variability, allowing for meaningful comparisons and\n    accurate analysis of gene expression levels across\n    samples or cells [@rnaseqnorm; @arraynorm;\n    @scRNAseqanalysis]. R/Bioconductor has several excellent\n    packages to perform normalization seamlessly, which\n    users should explore on their own.\n\n4.  Researchers assemble GRN gold standard usually from\n    experimentally derived regulatory interactions reported\n    in literature and databases [@GarciaAlonso01082019]. For instance,\n    I have assembled human transcriptional regulatory\n    network from 14 resources which can be used for benchmarking predictions\n    in humans [@Muley2022], while TFLink database provide\n    such information for few more model organisms [@tflink].\n    JASPAR database is worth to explore to create a GRN\n    based on information available in other species\n    [@jaspar].\n\n5.  Imbalanced data is a common challenge in biology, making\n    training deep learning models tricky\n    [@golddeeplearning]. R packages such as caret, ROSE, and\n    smote [@caret; @rose; @smote] are popular resources for\n    addressing imbalanced data. While the smotefamily package\n    utilizes the SMOTE algorithm to generate synthetic\n    samples of the minority class. Another approach is to down-size the majority class examples to make them compatible with minority class. I have used this approach for our problem. Also, I often prefer the class\n    weight approach. In the provided code snippet, the\n    \"class_weight\" parameter is used during model training\n    for binary classification. By calculating class weights\n    based on the class distribution in the training data,\n    and passing them as the \"class_weight\" argument in the\n    fit function, you can assign higher weights to the\n    minority class and lower weights to the majority class.\n    This enables the model to effectively handle class\n    imbalance during optimization, prioritizing the minority\n    class samples for better learning outcomes. \n\n\n::: {.cell}\n\n:::\n\n\n6.  The purpose of set_random_seed() is similar to that of\n    set.seed(), as mentioned in **Note 1**. The key\n    distinction is that set_random_seed() specifically sets\n    a seed for TensorFlow operations, ensuring\n    reproducibility of the training process. However, in practice users should confirmed robustness of the obtained results with many seed number. Its rare but sometimes set_random_seed function does not work, and throw error about version of TensorFlow, in that case, it should work after installing tensorflow again using \"install_tensorflow()\" command.\n\n7.  The number of layers and neurons (units) in a neural\n    network can be adjusted to enhance its learning capacity\n    [@layersneurons]. However, there is no definitive rule\n    on how many layers and neurons a network should have for\n    a given dataset. If the data is linearly separable,\n    multiple hidden layers may not be necessary. It is\n    generally recommended to keep the number of hidden\n    layers as minimal as possible. The number of neurons in\n    a layer can be determined\n    based on the shape of your training data. Typically, the\n    number of neurons in a layer is equal to the number of\n    features (columns) in your data. Some network\n    configurations add an additional node for a bias term.\n    It is crucial to maintain a low number of nodes to\n    ensure the network's ability to generalize well. Having\n    an excessive number of nodes may cause the network to\n    perfectly recall the training set but perform poorly on\n    new, unseen samples. The following code shows how to include more layer but there are numbe of different ways one can add layers and decide on number of neurons. Usually in each successive layers, the number of neurons are decreased.\n    \n\n::: {.cell}\n\n:::\n\n\n\n8.  Optimization functions in Keras R library are algorithms\n    that optimize the weights and biases of a neural network\n    during the training process. These functions help in\n    finding the optimal set of parameters that minimize the\n    loss function and improve the model's performance. Keras R API provides several\n    commonly used optimization functions such as optimizer_adam(),\n    optimizer_sgd() for Stochastic Gradient Descent and\n    optimizer_rmsprop() for RMSprop. Learning rate is an important hyperparameter that defines the speed of learning and can be set with learning_rate parameter. For example, optimizer_adam(learning_rate = 0.00001).\n\n9.  Accuracy can be misleading when dealing with imbalanced\n    datasets as it does not account for class imbalance.\n    Recall and precision metrics are often preferred over\n    accuracy in most scenarios, especially when the cost of false\n    positives and false negatives differs significantly.\n    Recall measures the proportion of true positive cases\n    correctly identified, while precision measures the\n    proportion of correctly identified positive cases. These\n    metrics provide a more comprehensive understanding of\n    the model's performance, especially when the focus is on\n    correctly identifying specific classes or minimizing\n    false positives/negatives. These metrics could be set in\n    with the complie() function as shown below. Remove\n    comment (#) character to include the metric for\n    training.\n\n\n::: {.cell}\n\n:::\n\n\n10. To set the number of epochs for training, start with a small value like 10 or 20 to\n    evaluate initial performance and identify any issues.\n    Since the optimal number of epochs varies based on the\n    dataset and problem, it's crucial to experiment and\n    fine-tune this parameter for optimal results. Monitor\n    the training and validation loss during training;\n    ideally, the loss should decrease. However, if the\n    training loss continues to decrease while the validation\n    loss increases, it suggests overfitting. To prevent\n    overfitting, implement early stopping (see callbacks\n    function), which allows you to halt training if the\n    validation loss stagnates or deteriorates after a\n    certain number of epochs. Lastly, consider the\n    computational resources and time constraints available,\n    as training deep learning models can be computationally\n    demanding. Striking a balance between model performance\n    and practical limitations is key.\n\n11. The batch size in deep learning determines the number of training examples processed in each iteration. It plays a crucial role in efficient model training. Choosing an appropriate batch size is important, as it impacts convergence speed and computational efficiency. Smaller batch sizes, such as 32 or 64, enable faster updates of model parameters but may introduce more noise and slower convergence. On the other hand, larger batch sizes enhance stability and convergence but require more memory and computational resources. It is recommended to experiment with different batch sizes based on the available resources, considering the trade-off between convergence speed and computational efficiency.\n\n\n12. The code uses validation data (optional) to assess the\n    model's performance during training. Alternatively, use\n    the validation_split parameter to specify the percentage\n    of training data for validation. For example, in the fit\n    function, you can set validation_split = 0.15 parameter\n    to use 15% of training data for validation. These\n    parameters are optional and can be adjusted to suit the\n    model and data requirements.\n\n13. An alternative code for prediction of GRN on high-end computational resources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\nset.seed(1979)\n\n\nmodel <- load_model_hdf5(\"EcoliRegModel.h5\")\n\n# import expression data file\nexp <- t(read.table(\"net3_expression_data.tsv\", header = T))\n\n# import original gene names mapping \ngenenames <- read.table(\"net3_gene_ids.tsv\", header = F)\ngenes <- genenames$V2; names(genes) <- genenames$V1\n\n# import list of all transcription factors of Escherichia coli\ntfs <- read.table(\"net3_transcription_factors.tsv\", header = F)$V1\n\n# create all possible pairs between tanscription factors and genes\n\npredictions <- expand.grid(tfs,  rownames(exp), stringsAsFactors = FALSE)\npredictions <- tibble::as_tibble(all_pairs[predictions$Var1 != predictions$Var2,])\n\n# add original gene names\n\npredictions$tfname <- genes[predictions$Var1]\npredictions$genename <- genes[predictions$Var2]\n\n# create feature table\ninpreddata <- cbind(exp[predictions$Var1,], exp[predictions$Var2,])\ninpreddata$pcc  <- sapply(seq.int(dim(predictions)[1]), \n                          function(i) cor(exp[predictions$Var1[i],], \n                                          exp[predictions$Var2[i],])))\n\n# predict regulatory pairs\npredictions$probability <- (model %>% predict(inpreddata))[,1]\npredictions <- predictions[predictions$probability>0.5,]\npredictions <- predictions[rev(order(predictions$probability)),]\npredictions \n```\n:::\n\n\n14. Callback functions are handy to control the training as\n    per our desires. In the following code, I have provided\n    examples of small functions that we will use to control\n    the training process dynamically.\n\nEarlyStopping Callback: This callback monitors the\nvalidation loss and stops the training if the validation\nloss does not improve after a certain number of epochs\n(defined by the \"patience\" parameter).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nearly_stopping <- callback_early_stopping(monitor = \"val_loss\", patience = 3)\n```\n:::\n\n\nModelCheckpoint Callback: This callback saves the model\nweights to a file (\"best_model.h5\") whenever the validation\naccuracy improves. By setting \"save_best_only\" to True, it\nonly saves the best model based on the monitored metric..\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_checkpoint <- callback_model_checkpoint(filepath = \"best_model.h5\", \n                                              monitor = \"val_accuracy\", \n                                              save_best_only = TRUE)\n```\n:::\n\n\nReduceLROnPlateau Callback: This callback reduces the\nlearning rate when the validation loss plateaus. The\nlearning rate is multiplied by the \"factor\" parameter, and\n\"patience\" determines the number of epochs to wait before\nreducing the learning rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce_lr <- callback_reduce_lr_on_plateau(monitor = \"val_loss\", \n                                           factor = 0.1, patience = 2)\n```\n:::\n\n\nTensorBoard: This callback enables logging for TensorBoard,\nwhich is a visualization tool for monitoring the training\nprocess. It creates logs that can be visualized using\nTensorBoard.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorboard <- callback_tensorboard(log_dir = \"logs\")\n```\n:::\n\n\nOnce callback functions are defined they can be incorporated during training as shown below for early_stopping and reduce_lr callback functions, but you can have as many functions as you want.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class,\n                           epochs=40, batch_size = 50, validation_split=0.2,\n                           callbacks = list(early_stopping, reduce_lr))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}