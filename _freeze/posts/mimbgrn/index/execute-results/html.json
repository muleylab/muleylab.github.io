{
  "hash": "055f7aa467daafd8f3520b5bfd8d15a1",
  "result": {
    "markdown": "---\ntitle: \"Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R\"\nsubtitle: Transcriptional regulatory network predictions\nauthor:\n  - name: Vijaykumar Yogesh Muley\ncategories: [Deep learning, GRN] # self-defined categories\ncitation:\n  type: article-journal\n  container-title: Methods in Molecular Biology\n  volume: 2719\n#  issue: 2\n  issued: 2023-08\n#  issn: 1539-9087\n  url: https://doi.org/10.1007/978-1-0716-3461-5_15 \n#image: preview_image.jpg\ndraft: false \n---\n\n\n## Computational and software requirements\n\n1.  Install tensorflow R package\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tensorflow\")\n```\n:::\n\n\n2.  TensorFlow uses Python programming language. Therefore, R should be configured to use Python\n\n3. If Python is already installed on computer then use following commands to configure R to use Python installation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\npath_to_python <- \"/usr/local/bin/python3\" \nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n```\n:::\n\n\nHere, we just provide a path to python executable (in this case, it is Python version 3). On macOS or Unix/Linux systems python\"s default location is /usr/local/bin/ folder. However, users should confirm the correct path on computers. \n\n3. If Python is not installed on computer then use following commands to install Python and then configure R. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\npath_to_python <- \"/usr/local/bin/python3\" \nvirtualenv_create(\"r-reticulate\", python = path_to_python)\n```\n:::\n\n\n\n10. Next, use the install_tensorflow() function from the tensorflow package to install TensorFlow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\ninstall_tensorflow(envname = \"r-reticulate\")\n```\n:::\n\n\n11. Use the install_keras() function\n    from the keras package to install TensorFlow along with\n    some commonly used packages such as \"scipy\" and\n    \"tensorflow-datasets\" as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras(envname = \"r-reticulate\")\n```\n:::\n\n\n12. To confirm that the installation was successful, run the\n    following command, which should return\n    *tf.Tensor(b\"Hello Tensorflow!\", shape=(),\n    dtype=string)*. If not then try \"install_tensorflow()\" command again. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\ntf$constant(\"Hello Tensorflow!\")\n```\n:::\n\n\n13. Install following packages to facilitate data handling\n    and analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\"readr\", \"tibble\", \"caret\", \"verification\"))\n```\n:::\n\n\n## Data requirements\n\n1. Download Supplementary Data 1 from the DREAM5 network inference challenge publication (https://doi.org/10.1038/nmeth.2016) and unzip it in your current working directory. Go to nealy created \"DREAM5_network_inference_challenge\" directory. \n\n2. The required data for Escherichia coli is present in the \"Network3\" folder and its sub-folders. Copy four files listed below from these sub-folders into your current working directory to proceed with the analysis..\n\n3. *net3_expression_data.tsv*: This file contains the expression profile data.\n\n4. *net3_transcription_factors.tsv*: This file contains the list of transcription factors.\n\n5. *DREAM5_NetworkInference_GoldStandard_Network3.tsv*: This file contains the gold standard data, which includes both regulatory (positive examples) and non-regulatory (negative examples) TF-gene pairs.\n\n6. *net3_gene_ids.tsv*: This file serves as a mapping between the anonymous gene codes used in other files and the original gene names.\n\n\n# Methods\n\n## Load R libraries\n\n1. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n```\n:::\n\n\n2.  Set a random odd number as a seed in the set.seed \n    function to reproduce the results of this workflow. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1979)\n```\n:::\n\n\n## Data preparation and exploration\n\n1.  Import the gene expression data and store in a exp\n    object and do a routine check up as shown below. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp <- read.table(\"net3_expression_data.tsv\", header = T)\nexp <- t(exp)\ndim(exp)\nexp[1:5,1:5]\n```\n:::\n\n\n2.  Plot distribution of expression values to check trends in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = \"Expression intensity\", main = \"Gene expression\")\n\n# export plot to pdf file in the current directory\npdf(file = \"DeepLearning_Figure1.pdf\", \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nhist(exp, 100, col = \"darkgrey\", border = \"white\", \n     xlab = \"Expression intensity\", main = \"Gene expression\")\ndev.off()\n```\n:::\n\n\n\n3.  Load the gold standard data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold <- readr::read_table(\"DREAM5_NetworkInference_GoldStandard_Network3.tsv\",\n                          col_names = F)\ngold\n```\n:::\n\n\nIn the gold object, the first two columns respectively list\nthe TFs and genes, while the third column\ncontains binary labels. A value of 1 in the label column\nindicates that experimental evidence exists for the\nregulation of the gene in column 2 by the transcription\nfactor in column 1, while a value of 0 indicates that there\nis no known regulatory interaction between them.\n\n3.  Make expression and gold standard data compatible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngold <- gold[gold$X1 %in% rownames(exp) & gold$X2 %in% rownames(exp), ]\ntable(gold$X3)\n```\n:::\n\n\n4.  The gold standard data is imbalanced, with 2066\n    regulatory pairs and 150,214 non-regulatory pairs. Lets down-sample majority class examples randomly so that they are two times more than that of regulatory pairs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkeep_indices <- c(which(gold$X3==1), \n                  sample(which(gold$X3==0), size = sum(gold$X3)*2))\ngold <- gold[keep_indices,]\ntable(gold$X3)\n```\n:::\n\n\n5. In the next step, we create a feature table containing raw expression data for TF-gene pair, and we will manually create a feature which represent pearson correlation coefficients between expression profiles of transcription factors and genes. This correlation measure is a conventional approach for engineering features from raw data. The resulting correlation coefficients are added as a new column named \"pcc\" within the data.frame. Subsequently, labels are appended as the last column of the data.frame. The first two columns of the data.frame represent the names of the TF and gene, respectively. The remaining columns contain the expression values for the TF and gene, followed by the calculated PCC values between their expression values, and finally the gold standard label. To facilitate further analysis, the column names that correspond to the expression values and PCC are stored in the \"featurenames\" object, as they will serve as features for subsequent processing.\n\nThe code snippet below demonstrates these steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputdata <- data.frame(tf = gold$X1, gene = gold$X2, \n                        exp[gold$X1,], exp[gold$X2,])\ninputdata$pcc <- sapply(seq.int(dim(gold)[1]), function(i) \n                        cor(exp[gold$X1[i],], exp[gold$X2[i],]))\ninputdata$class <- gold$X3\n\ninputdata <- tibble::as_tibble(inputdata)\n\nfeaturenames <- colnames(inputdata) %>% setdiff(\"class\")  %>% \n  setdiff(\"tf\") %>% setdiff(\"gene\")\n```\n:::\n\n\n6. Lets plot feature data points for regulatory and non-regulatory pairs to see if there are any visible trend which can distinguish them based on our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh1 <- hist(as.matrix(inputdata[inputdata$class==\"1\",featurenames]), \n           breaks = 100, plot = FALSE)\nh2 <- hist(as.matrix(inputdata[inputdata$class==\"0\",featurenames]), \n           breaks = 100, plot = FALSE)\n\npdf(file = \"DeepLearning_Figure2.pdf\", \n    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)\nplot(h2, col = rgb(0,0,1,0.4), freq = FALSE, \n     xlab = \"Expression intensity\", \n     main = \"Distribution of feature values for training data\")\nplot(h1, xaxt = \"n\", yaxt = \"n\", col = rgb(1,0,0,0.4), \n     freq = FALSE, add = TRUE)\ndev.off()\n```\n:::\n\n\n7.  The following code implements division of gold standard data into training, validation and test datasets in 80:10:10 proportions. The proportions are subjective. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nindices <- nrow(inputdata) %>% sample.int(., ceiling( . * 0.8))\ntraindata <- inputdata[indices, ]\ntestdata <- inputdata[-indices, ]\nindices <- nrow(testdata) %>% sample.int(., ceiling( . * 0.5))\nvaldata <- testdata[-indices, ]\ntestdata <- testdata[indices, ]\ntable(traindata$class)\ntable(valdata$class)\ntable(testdata$class)\n```\n:::\n\n\n## Defining deep learning model architecture\n\n1.  Lets create a sequential deep neural network (DNN) using the\n    keras_model_sequential() function from the Keras\n    library. We then add two dense layers using the\n    layer_dense() function. The input layer is defined with\n    the input_shape parameter, which needs to be equal to\n    the total number of features in the input data or represents the shape of the features. Next, the\n    single hidden layer contains 806 neurons and applies the\n    \"relu\" activation function to introduce non-linearity in\n    the layer\"s output. The second layer_dense()\n    function adds another fully connected layer with a\n    single output unit. This output layer computes a\n    weighted sum of the inputs from the previous layer feeds\n    and applies the sigmoid activation function to produce a\n    probability value between 0 and 1,\n    indicating the likelihood that the input corresponds to\n    the positive class (regulatory pairs, in this case) based on input features. \n\n2.  The code below sets a random seed for tensorflow using set_random_seed function. Then, creates a sequential deep neural network (DNN) using the keras_model_sequential() function from the Keras library. The DNN model has a simple architecture with\n    input layer which feeds feature matrix and labels to fully connected layers hidden layer with \"relu\"\n    activation, and a another fully connected output layer with \"sigmoid\"\n    activation to produces a single probability value for\n    each training example of being regulatory pair. The summary function shows an overview of the model\"s\n    architecture. \n    \n\n::: {.cell}\n\n```{.r .cell-code}\nset_random_seed(1979)\n\nmodel <- keras_model_sequential(\n  input_shape = c(length(featurenames))) %>%\n  layer_dense(units = 806, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsummary(model)\n```\n:::\n\n\n\n## Defining Model Compilation Parameters\n\n1.  To configure the learning process, the model\"s\n    compilation parameters are specified using the following\n    code. The compile() function configures the model. The optimizer_adam() function is a widely used\n    optimization algorithm that computes adaptive learning\n    rates for each parameter based on the moment estimates\n    of the gradient and past gradients and helps neural\n    network to converge faster. The loss function measures the difference between\n    predicted and actual values. The \"binary_crossentropy\"\n    loss function is commonly used for binary classification\n    tasks. IThe \"accuracy\" metric is used to\n    calculate the percentage of correctly predicted labels\n    by the model. Accuracy metric is fine for prototyping\n    DNN, but use more reliable metrics for practical\n    application.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(optimizer = optimizer_adam(),\n loss = \"binary_crossentropy\",\n metrics = c(\"accuracy\"))\n```\n:::\n\n\n## Training deep learning model\n\n1.  The following code chunk fits the DNN model to the\n    training data and validates its performance using a\n    validation dataset by using the fit() function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(as.matrix(traindata[,featurenames]), \n                         traindata$class,\n                         epochs=100, batch_size = 64,\n                         validation_data = list(as.matrix(valdata[,featurenames]), \n                                                valdata$class))\n```\n:::\n\n\n\n## Estimating the accuracy of deep learning model\n\n1.  Use evaluate function to compute accuracy and loss function over all epochs as shown below. The code plot(history)  generates a plot of the training and validation loss and  accuracy over the course of the training process. This  visualization can help identify trends or patterns in  the performance of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(history)\nevaluate(model, x = as.matrix(traindata[,featurenames]), y = traindata$class)\nevaluate(model, x = as.matrix(valdata[,featurenames]), y = valdata$class)\nplot(history)\n```\n:::\n\n\n2.  You can customize plots using following code. For instance,\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf(file = \"DeepLearning_Figure3.pdf\",\n    width = 7, height = 4, pointsize = 10, useDingbats = FALSE)\n\npar(mfrow = c(1,2), cex = 0.8)\n\nmaxLoss <- max(c(history$metrics$loss, history$metrics$val_loss))\n\nplot(history$metrics$loss, main=\"Model Loss\", xlab = \"Epoch\", ylab=\"Loss\", \n     xlim = c(0, length(history$metrics$loss)), ylim = c(0, maxLoss),\n     col=\"red2\", type=\"b\",lwd=1)\nlines(history$metrics$val_loss, col=\"steelblue1\", type=\"b\",lwd=1)\nlegend(\"topright\", c(\"Training\",\"Validation\"), col=c(\"red2\", \"steelblue1\"), \n       lty=c(1,1), bty = \"n\")\n\nplot(history$metrics$accuracy, col=\"red2\", type=\"b\",lwd=1, \n     main=\"Model Accuracy\", xlab = \"Epoch\", ylab=\"Accuracy\", \n     xlim = c(0, length(history$metrics$accuracy)), ylim = c(0, 1))\nlines(history$metrics$val_accuracy, col=\"steelblue1\", type=\"b\",lwd=1)\nabline(h = 0.5, col = \"grey\", lty = 3)\ndev.off()\n```\n:::\n\n\n## Predicting labels for unlabelled pairs of transcription factors and genes. \n\n1.  The code provided below utilizes our model to predict the probabilities of outcomes for the test dataset, which is unlabelled and completely unknown to the model. This prediction is achieved by using the predict() function, and the resulting probabilities are stored in the variable predprob for further utilization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredprob <- model %>% predict(as.matrix(testdata[,featurenames]))\n```\n:::\n\n\n6.  To evaluate the performance of the DNN model on the test data and compare it with the training and validation data, the confusionMatrix() function from the caret package can be used. This function takes the predicted class probabilities in a binary format, specifically using a threshold of 0.5 or above (i.e., as.numeric(predprob > 0.5)), along with the true class labels. It then generates a table containing various important performance measures as shown below. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nf1 <- function(indata, truelabels){\n  res <-  model %>% predict(as.matrix(indata[,featurenames]))\n  res <- as.numeric(res>0.5)\n  res <- factor(res, c(1,0))\n  caret::confusionMatrix(res, factor(truelabels, c(1,0)))\n}\n\ntrainAccuracy <- f1(traindata, traindata$class)\nvalidationAccuracy <- f1(valdata, valdata$class)\ntestAccuracy <-f1(testdata, testdata$class)\n\nperformance <- round(t(as.data.frame(rbind(t(trainAccuracy$byClass), \n                                           t(validationAccuracy$byClass), \n                                           t(testAccuracy$byClass)))),3)\ncolnames(performance) <- c(\"Traning\", \"Validation\", \"Test\")\n\nperformance\n```\n:::\n\n\n7.  Biologists may prefer to use a higher probability cutoff\n    to avoid false positive results, even if it means\n    predicting fewer regulatory interactions. The ROC curve\n    is a perfect instrument which shows how the classifier\"s\n    performance changes at different threshold settings for\n    guidance. In the following code, roc.plot function from\n    verification package is used to plot ROC curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\npdf(file = \"DeepLearning_Figure4.pdf\",\n    width = 5, height = 5, pointsize = 12, useDingbats = FALSE)\n\nverification::roc.plot(testdata$class, predprob, \n                       ylab = \"True Positive Rate\", \n                       xlab = \"False Positive Rate\")\nabline(v = 0.5596, col = \"red2\", lty = 3)\nabline(h = 0.0509, col = \"steelblue1\", lty = 3)\n\ndev.off()\n```\n:::\n\n\n\n8. The following code shows that how to save the model for future use, and we also save performance table, expression and gold standard data which we all need to reproduce all the results or to make new better deep learning model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_hdf5(model, \"EcoliRegModel.h5\")\nsave(exp, performance, gold, history, file = \"EcoliRegModel.RData\")\n```\n:::\n\n \nThe save_model_hdf5() function saves the model in the Hierarchical Data Format version 5 (HDF5) file format. HDF5 is a file format designed to store and organize large amounts of numerical data. It is a binary file format that can store the model architecture, weights, optimizer state, and any other information related to the model.\n \n## Predicting genome-wide regulatory interactions\n\n1. In this section, we will demonstrate how to evaluate the pairwise combinations of transcription factors (TFs) and genes in *Escherichia coli* using our model. We will assess the probability of these pairs being potential regulatory pairs. It\"s important to note that the total number of TF-gene pairs for *Escherichia coli* exceeds a million and a half, and in higher organisms, the number is even greater. Consequently, most computers may struggle to handle the feature data for all these pairs, potentially causing crashes. Therefore, the following code is specifically provided for users who do not possess high-end computational resources. For those with ample computational resources, I recommend referring to the code provided in **Note 13**. As these codes can be used across different sessions of RStudio, they assume that the computer or RStudio has been restarted and the same directory has been set (can be verified using the getwd() function). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\nset.seed(1979)\n\n\nmodel <- load_model_hdf5(\"EcoliRegModel.h5\")\n\n# import expression data file\nexp <- t(read.table(\"net3_expression_data.tsv\", header = T))\n\n# import original gene names mapping \ngenenames <- read.table(\"net3_gene_ids.tsv\", header = F)\ngenes <- genenames$V2; names(genes) <- genenames$V1\n\n# import list of all transcription factors of Escherichia coli\n# tfs <- names(genes)[genes %in% c(\"gadX\", \"flhC\", \"flhD\",\"dnaA\")] # trail run\ntfs <- read.table(\"net3_transcription_factors.tsv\", header = F)$V1\n\nlength(tfs)*nrow(exp)\n\n\npredictions <- NULL\n\nfor(i in tfs){\n  \n  tfdata <-data.frame(tf = i, gene = rownames(exp), \n                      tfname = genes[i],\n                      genename = genes[rownames(exp)])\n  tfdata <- tibble::as_tibble(tfdata[tfdata$tf != tfdata$gene,])\n  \n  inpreddata <- cbind(exp[tfdata$tf,], exp[tfdata$gene,])\n  inpreddata <- cbind(inpreddata, \n                      sapply(seq.int(dim(tfdata)[1]), \n                             function(i) \n                               cor(exp[tfdata$tf[i],], \n                                   exp[tfdata$gene[i],])))\n  \n  tfdata$probability <- (model %>% predict(inpreddata))[,1]\n  \n  predictions <- rbind(predictions, tfdata[tfdata$probability>0.5,])\n  \n}\n\npredictions <- predictions[rev(order(predictions$probability)),]\npredictions\n```\n:::\n\n\n\n4. Our model predicted only 80,225 as regulatory out of 1,506,674 TF-gene pairs. There are number of sophisticated ways to evaluate the predicted GRN. Here, however, we will quickly select few TF-gene pairs with high probability of being regulatory, and will study them. Escherichi coli has many acid inducible systems which protect cells  against acid stress to pH 2 or below. Of them, the glutamate depending system is the most effect and relies on two glutamate decarboxylases (GadA and GadB) combined with a putative glutamate:gamma-aminobutyric acid antiporter (GadC). According to our predictions, it appears to be regluated by OsmE, which is osmolarity inducible factor, however, there is no information exists that it regulates glutamate-dependant acid resistance system. though functionally it would make sense. The GadX is a known regulator of the acid resistance system. Also, master transcription factors associated with flagella biosynthesis and chemotaxis were among the highly ranked TF-gene pairs. So, lets check target genes of GadX, FlhC, and FlhD transcription factors.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions[predictions$tfname==\"gadX\",]\npredictions[predictions$tfname==\"flhC\",]\npredictions[predictions$tfname==\"flhD\",]\n```\n:::\n\n\nOur results suggests that GadX may regulate 16 genes at the probability cutoff of 0.5 or above, and of 16, five genes i.e. gadA, gadE gadB, gadC and gadW belong to glutamate dependant acid protection system, confirming the accuracy of our prediction system. Likewise, FlhC and FlhD regulates a large number of genes involved in flagellar assembly and chemotaxis reassuring our prediction system. The overlap of their target genes is 652, which is highly expected. One of the problem of deep learning algorithm is that they can memorize the training dataset and many of the predictions could be part of training or validation dataset. Therefore, lets remove all predictions that were part of the training, and again cross-check the results using following command. As you can see, there are still many predictions which were novel, and make perfect sense at functional level.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions[!paste0(predictions$tf,predictions$gene) %in%  \n              paste0(inputdata$tf[inputdata$class==1], \n                     inputdata$gene[inputdata$class==1]),] \n```\n:::\n\n\n\n4. To export results from prediction object to a tabular, csv or excel file, use following code. The third command requires writexl R package which should be installed using install.packages function. Likewise, you can also export the performance table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.table(predictions, file = \"grnPredictionsEcoli.txt\", \n            col.names = T, row.names = F, quote = F, sep = \"\\t\")\nwrite.csv(predictions, file = \"grnPredictionsEcoli.csv\", row.names = F)\nwritexl::write_xlsx(list(Table1 = predictions), \n                    path = \"grnPredictionsEcoli.xlsx\", col_names = T, )\n```\n:::\n\n\n\n## Handiling imbalanced gold standard data \n\n5.  Imbalanced data is a common challenge in biology, making\n    training deep learning models tricky\n    [@golddeeplearning]. R packages such as caret, ROSE, and\n    smote [@caret; @rose; @smote] are popular resources for\n    addressing imbalanced data. While the smotefamily package\n    utilizes the SMOTE algorithm to generate synthetic\n    samples of the minority class. Another approach is to down-size the majority class examples to make them compatible with minority class. I have used this approach for our problem. Also, I often prefer the class\n    weight approach. In the provided code snippet, the\n    \"class_weight\" parameter is used during model training\n    for binary classification. By calculating class weights\n    based on the class distribution in the training data,\n    and passing them as the \"class_weight\" argument in the\n    fit function, you can assign higher weights to the\n    minority class and lower weights to the majority class.\n    This enables the model to effectively handle class\n    imbalance during optimization, prioritizing the minority\n    class samples for better learning outcomes. \n\n\n::: {.cell}\n\n:::\n\n\n## Increasing generalization and depth of DNN\n\n7.  The number of layers and neurons (units) in a neural\n    network can be adjusted to enhance its learning capacity\n    [@layersneurons]. However, there is no definitive rule\n    on how many layers and neurons a network should have for\n    a given dataset. If the data is linearly separable,\n    multiple hidden layers may not be necessary. It is\n    generally recommended to keep the number of hidden\n    layers as minimal as possible. The number of neurons in\n    a layer can be determined\n    based on the shape of your training data. Typically, the\n    number of neurons in a layer is equal to the number of\n    features (columns) in your data. Some network\n    configurations add an additional node for a bias term.\n    It is crucial to maintain a low number of nodes to\n    ensure the network\"s ability to generalize well. Having\n    an excessive number of nodes may cause the network to\n    perfectly recall the training set but perform poorly on\n    new, unseen samples. The following code shows how to include more layer but there are numbe of different ways one can add layers and decide on number of neurons. Usually in each successive layers, the number of neurons are decreased.\n    \n\n::: {.cell}\n\n:::\n\n\n\n## Performance measures\n\n\n9.  Accuracy can be misleading when dealing with imbalanced\n    datasets as it does not account for class imbalance.\n    Recall and precision metrics are often preferred over\n    accuracy in most scenarios, especially when the cost of false\n    positives and false negatives differs significantly.\n    Recall measures the proportion of true positive cases\n    correctly identified, while precision measures the\n    proportion of correctly identified positive cases. These\n    metrics provide a more comprehensive understanding of\n    the model\"s performance, especially when the focus is on\n    correctly identifying specific classes or minimizing\n    false positives/negatives. These metrics could be set in\n    with the complie() function as shown below. Remove\n    comment (#) character to include the metric for\n    training.\n\n\n::: {.cell}\n\n:::\n\n\n## An alternative code for prediction of GRN on high-end computational resources.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nlibrary(keras)\nlibrary(tensorflow)\n\nset.seed(1979)\n\n\nmodel <- load_model_hdf5(\"EcoliRegModel.h5\")\n\n# import expression data file\nexp <- t(read.table(\"net3_expression_data.tsv\", header = T))\n\n# import original gene names mapping \ngenenames <- read.table(\"net3_gene_ids.tsv\", header = F)\ngenes <- genenames$V2; names(genes) <- genenames$V1\n\n# import list of all transcription factors of Escherichia coli\ntfs <- read.table(\"net3_transcription_factors.tsv\", header = F)$V1\n\n# create all possible pairs between tanscription factors and genes\n\npredictions <- expand.grid(tfs,  rownames(exp), stringsAsFactors = FALSE)\npredictions <- tibble::as_tibble(all_pairs[predictions$Var1 != predictions$Var2,])\n\n# add original gene names\n\npredictions$tfname <- genes[predictions$Var1]\npredictions$genename <- genes[predictions$Var2]\n\n# create feature table\ninpreddata <- cbind(exp[predictions$Var1,], exp[predictions$Var2,])\ninpreddata$pcc  <- sapply(seq.int(dim(predictions)[1]), \n                          function(i) cor(exp[predictions$Var1[i],], \n                                          exp[predictions$Var2[i],])))\n\n# predict regulatory pairs\npredictions$probability <- (model %>% predict(inpreddata))[,1]\npredictions <- predictions[predictions$probability>0.5,]\npredictions <- predictions[rev(order(predictions$probability)),]\npredictions \n```\n:::\n\n\n\n## Controlling DNN model training by Callback functions\n\nEarlyStopping Callback: This callback monitors the\nvalidation loss and stops the training if the validation\nloss does not improve after a certain number of epochs\n(defined by the \"patience\" parameter).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nearly_stopping <- callback_early_stopping(monitor = \"val_loss\", patience = 3)\n```\n:::\n\n\nModelCheckpoint Callback: This callback saves the model\nweights to a file (\"best_model.h5\") whenever the validation\naccuracy improves. By setting \"save_best_only\" to True, it\nonly saves the best model based on the monitored metric..\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_checkpoint <- callback_model_checkpoint(filepath = \"best_model.h5\", \n                                              monitor = \"val_accuracy\", \n                                              save_best_only = TRUE)\n```\n:::\n\n\nReduceLROnPlateau Callback: This callback reduces the\nlearning rate when the validation loss plateaus. The\nlearning rate is multiplied by the \"factor\" parameter, and\n\"patience\" determines the number of epochs to wait before\nreducing the learning rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduce_lr <- callback_reduce_lr_on_plateau(monitor = \"val_loss\", \n                                           factor = 0.1, patience = 2)\n```\n:::\n\n\nTensorBoard: This callback enables logging for TensorBoard,\nwhich is a visualization tool for monitoring the training\nprocess. It creates logs that can be visualized using\nTensorBoard.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorboard <- callback_tensorboard(log_dir = \"logs\")\n```\n:::\n\n\nOnce callback functions are defined they can be incorporated during training as shown below for early_stopping and reduce_lr callback functions, but you can have as many functions as you want.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class,\n                           epochs=40, batch_size = 50, validation_split=0.2,\n                           callbacks = list(early_stopping, reduce_lr))\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}