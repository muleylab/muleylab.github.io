---
title: "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R"
subtitle: Transcriptional regulatory network predictions
author:
  - name: Vijaykumar Yogesh Muley
categories: [Deep learning, GRN] # self-defined categories
citation:
  type: article-journal
  container-title: Methods in Molecular Biology
  volume: 2719
#  issue: 2
  issued: 2023-08
#  issn: 1539-9087
  url: https://doi.org/10.1007/978-1-0716-3461-5_15 
#image: preview_image.jpg
draft: false 
---

## Computational and software requirements

1.  Install tensorflow R package

```{r, eval=FALSE}
install.packages("tensorflow")
```

2.  TensorFlow uses Python programming language. Therefore, R should be configured to use Python

3. If Python is already installed on computer then use following commands to configure R to use Python installation. 

```{r, eval=FALSE}
library(reticulate)
path_to_python <- "/usr/local/bin/python3" 
virtualenv_create("r-reticulate", python = path_to_python)

```

Here, we just provide a path to python executable (in this case, it is Python version 3). On macOS or Unix/Linux systems python"s default location is /usr/local/bin/ folder. However, users should confirm the correct path on computers. 

3. If Python is not installed on computer then use following commands to install Python and then configure R. 

```{r, eval=FALSE}

library(reticulate)
path_to_python <- "/usr/local/bin/python3" 
virtualenv_create("r-reticulate", python = path_to_python)

```


10. Next, use the install_tensorflow() function from the tensorflow package to install TensorFlow.

```{r, eval=FALSE}
library(tensorflow)
install_tensorflow(envname = "r-reticulate")
```

11. Use the install_keras() function
    from the keras package to install TensorFlow along with
    some commonly used packages such as "scipy" and
    "tensorflow-datasets" as shown below.

```{r, eval=FALSE}
install.packages("keras")
library(keras)
install_keras(envname = "r-reticulate")
```

12. To confirm that the installation was successful, run the
    following command, which should return
    *tf.Tensor(b"Hello Tensorflow!", shape=(),
    dtype=string)*. If not then try "install_tensorflow()" command again. 

```{r, eval=FALSE}
library(tensorflow)
tf$constant("Hello Tensorflow!")
```

13. Install following packages to facilitate data handling
    and analysis.

```{r, eval=FALSE}
install.packages(c("readr", "tibble", "caret", "verification"))
```

## Data requirements

1. Download Supplementary Data 1 from the DREAM5 network inference challenge publication (https://doi.org/10.1038/nmeth.2016) and unzip it in your current working directory. Go to nealy created "DREAM5_network_inference_challenge" directory. 

2. The required data for Escherichia coli is present in the "Network3" folder and its sub-folders. Copy four files listed below from these sub-folders into your current working directory to proceed with the analysis..

3. *net3_expression_data.tsv*: This file contains the expression profile data.

4. *net3_transcription_factors.tsv*: This file contains the list of transcription factors.

5. *DREAM5_NetworkInference_GoldStandard_Network3.tsv*: This file contains the gold standard data, which includes both regulatory (positive examples) and non-regulatory (negative examples) TF-gene pairs.

6. *net3_gene_ids.tsv*: This file serves as a mapping between the anonymous gene codes used in other files and the original gene names.


# Methods

## Load R libraries

1. 

```{r, eval=FALSE}
library(magrittr)
library(keras)
library(tensorflow)
```

2.  Set a random odd number as a seed in the set.seed 
    function to reproduce the results of this workflow. 

```{r, eval=FALSE}
set.seed(1979)
```

## Data preparation and exploration

1.  Import the gene expression data and store in a exp
    object and do a routine check up as shown below. 

```{r, eval=FALSE}
exp <- read.table("net3_expression_data.tsv", header = T)
exp <- t(exp)
dim(exp)
exp[1:5,1:5]
```

2.  Plot distribution of expression values to check trends in the data.

```{r, eval=FALSE}
# plot
hist(exp, 100, col = "darkgrey", border = "white", 
     xlab = "Expression intensity", main = "Gene expression")

# export plot to pdf file in the current directory
pdf(file = "DeepLearning_Figure1.pdf", 
    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)
hist(exp, 100, col = "darkgrey", border = "white", 
     xlab = "Expression intensity", main = "Gene expression")
dev.off()

```


3.  Load the gold standard data.

```{r, eval=FALSE}
gold <- readr::read_table("DREAM5_NetworkInference_GoldStandard_Network3.tsv",
                          col_names = F)
gold

```

In the gold object, the first two columns respectively list
the TFs and genes, while the third column
contains binary labels. A value of 1 in the label column
indicates that experimental evidence exists for the
regulation of the gene in column 2 by the transcription
factor in column 1, while a value of 0 indicates that there
is no known regulatory interaction between them.

3.  Make expression and gold standard data compatible.

```{r, eval=FALSE}
gold <- gold[gold$X1 %in% rownames(exp) & gold$X2 %in% rownames(exp), ]
table(gold$X3)

```

4.  The gold standard data is imbalanced, with 2066
    regulatory pairs and 150,214 non-regulatory pairs. Lets down-sample majority class examples randomly so that they are two times more than that of regulatory pairs.

```{r, eval=FALSE}
keep_indices <- c(which(gold$X3==1), 
                  sample(which(gold$X3==0), size = sum(gold$X3)*2))
gold <- gold[keep_indices,]
table(gold$X3)

```

5. In the next step, we create a feature table containing raw expression data for TF-gene pair, and we will manually create a feature which represent pearson correlation coefficients between expression profiles of transcription factors and genes. This correlation measure is a conventional approach for engineering features from raw data. The resulting correlation coefficients are added as a new column named "pcc" within the data.frame. Subsequently, labels are appended as the last column of the data.frame. The first two columns of the data.frame represent the names of the TF and gene, respectively. The remaining columns contain the expression values for the TF and gene, followed by the calculated PCC values between their expression values, and finally the gold standard label. To facilitate further analysis, the column names that correspond to the expression values and PCC are stored in the "featurenames" object, as they will serve as features for subsequent processing.

The code snippet below demonstrates these steps:

```{r, eval=FALSE}

inputdata <- data.frame(tf = gold$X1, gene = gold$X2, 
                        exp[gold$X1,], exp[gold$X2,])
inputdata$pcc <- sapply(seq.int(dim(gold)[1]), function(i) 
                        cor(exp[gold$X1[i],], exp[gold$X2[i],]))
inputdata$class <- gold$X3

inputdata <- tibble::as_tibble(inputdata)

featurenames <- colnames(inputdata) %>% setdiff("class")  %>% 
  setdiff("tf") %>% setdiff("gene")

```

6. Lets plot feature data points for regulatory and non-regulatory pairs to see if there are any visible trend which can distinguish them based on our data.

```{r, eval=FALSE}
h1 <- hist(as.matrix(inputdata[inputdata$class=="1",featurenames]), 
           breaks = 100, plot = FALSE)
h2 <- hist(as.matrix(inputdata[inputdata$class=="0",featurenames]), 
           breaks = 100, plot = FALSE)

pdf(file = "DeepLearning_Figure2.pdf", 
    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)
plot(h2, col = rgb(0,0,1,0.4), freq = FALSE, 
     xlab = "Expression intensity", 
     main = "Distribution of feature values for training data")
plot(h1, xaxt = "n", yaxt = "n", col = rgb(1,0,0,0.4), 
     freq = FALSE, add = TRUE)
dev.off()


```

7.  The following code implements division of gold standard data into training, validation and test datasets in 80:10:10 proportions. The proportions are subjective. 

```{r, eval=FALSE}

indices <- nrow(inputdata) %>% sample.int(., ceiling( . * 0.8))
traindata <- inputdata[indices, ]
testdata <- inputdata[-indices, ]
indices <- nrow(testdata) %>% sample.int(., ceiling( . * 0.5))
valdata <- testdata[-indices, ]
testdata <- testdata[indices, ]
table(traindata$class)
table(valdata$class)
table(testdata$class)

```

## Defining deep learning model architecture

1.  Lets create a sequential deep neural network (DNN) using the
    keras_model_sequential() function from the Keras
    library. We then add two dense layers using the
    layer_dense() function. The input layer is defined with
    the input_shape parameter, which needs to be equal to
    the total number of features in the input data or represents the shape of the features. Next, the
    single hidden layer contains 806 neurons and applies the
    "relu" activation function to introduce non-linearity in
    the layer"s output. The second layer_dense()
    function adds another fully connected layer with a
    single output unit. This output layer computes a
    weighted sum of the inputs from the previous layer feeds
    and applies the sigmoid activation function to produce a
    probability value between 0 and 1,
    indicating the likelihood that the input corresponds to
    the positive class (regulatory pairs, in this case) based on input features. 

2.  The code below sets a random seed for tensorflow using set_random_seed function. Then, creates a sequential deep neural network (DNN) using the keras_model_sequential() function from the Keras library. The DNN model has a simple architecture with
    input layer which feeds feature matrix and labels to fully connected layers hidden layer with "relu"
    activation, and a another fully connected output layer with "sigmoid"
    activation to produces a single probability value for
    each training example of being regulatory pair. The summary function shows an overview of the model"s
    architecture. 
    
```{r, eval=FALSE}
set_random_seed(1979)

model <- keras_model_sequential(
  input_shape = c(length(featurenames))) %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

```


## Defining Model Compilation Parameters

1.  To configure the learning process, the model"s
    compilation parameters are specified using the following
    code. The compile() function configures the model. The optimizer_adam() function is a widely used
    optimization algorithm that computes adaptive learning
    rates for each parameter based on the moment estimates
    of the gradient and past gradients and helps neural
    network to converge faster. The loss function measures the difference between
    predicted and actual values. The "binary_crossentropy"
    loss function is commonly used for binary classification
    tasks. IThe "accuracy" metric is used to
    calculate the percentage of correctly predicted labels
    by the model. Accuracy metric is fine for prototyping
    DNN, but use more reliable metrics for practical
    application.
```{r, eval=FALSE}

model %>% compile(optimizer = optimizer_adam(),
 loss = "binary_crossentropy",
 metrics = c("accuracy"))

```

## Training deep learning model

1.  The following code chunk fits the DNN model to the
    training data and validates its performance using a
    validation dataset by using the fit() function:

```{r, eval=FALSE}
history <- model %>% fit(as.matrix(traindata[,featurenames]), 
                         traindata$class,
                         epochs=100, batch_size = 64,
                         validation_data = list(as.matrix(valdata[,featurenames]), 
                                                valdata$class))

```


## Estimating the accuracy of deep learning model

1.  Use evaluate function to compute accuracy and loss function over all epochs as shown below. The code plot(history)  generates a plot of the training and validation loss and  accuracy over the course of the training process. This  visualization can help identify trends or patterns in  the performance of the model.

```{r, eval=FALSE}
print(history)
evaluate(model, x = as.matrix(traindata[,featurenames]), y = traindata$class)
evaluate(model, x = as.matrix(valdata[,featurenames]), y = valdata$class)
plot(history)


```

2.  You can customize plots using following code. For instance,

```{r, eval=FALSE}

pdf(file = "DeepLearning_Figure3.pdf",
    width = 7, height = 4, pointsize = 10, useDingbats = FALSE)

par(mfrow = c(1,2), cex = 0.8)

maxLoss <- max(c(history$metrics$loss, history$metrics$val_loss))

plot(history$metrics$loss, main="Model Loss", xlab = "Epoch", ylab="Loss", 
     xlim = c(0, length(history$metrics$loss)), ylim = c(0, maxLoss),
     col="red2", type="b",lwd=1)
lines(history$metrics$val_loss, col="steelblue1", type="b",lwd=1)
legend("topright", c("Training","Validation"), col=c("red2", "steelblue1"), 
       lty=c(1,1), bty = "n")

plot(history$metrics$accuracy, col="red2", type="b",lwd=1, 
     main="Model Accuracy", xlab = "Epoch", ylab="Accuracy", 
     xlim = c(0, length(history$metrics$accuracy)), ylim = c(0, 1))
lines(history$metrics$val_accuracy, col="steelblue1", type="b",lwd=1)
abline(h = 0.5, col = "grey", lty = 3)
dev.off()


```

## Predicting labels for unlabelled pairs of transcription factors and genes. 

1.  The code provided below utilizes our model to predict the probabilities of outcomes for the test dataset, which is unlabelled and completely unknown to the model. This prediction is achieved by using the predict() function, and the resulting probabilities are stored in the variable predprob for further utilization.

```{r, eval=FALSE}

predprob <- model %>% predict(as.matrix(testdata[,featurenames]))

```

6.  To evaluate the performance of the DNN model on the test data and compare it with the training and validation data, the confusionMatrix() function from the caret package can be used. This function takes the predicted class probabilities in a binary format, specifically using a threshold of 0.5 or above (i.e., as.numeric(predprob > 0.5)), along with the true class labels. It then generates a table containing various important performance measures as shown below. 

```{r, eval=FALSE}

f1 <- function(indata, truelabels){
  res <-  model %>% predict(as.matrix(indata[,featurenames]))
  res <- as.numeric(res>0.5)
  res <- factor(res, c(1,0))
  caret::confusionMatrix(res, factor(truelabels, c(1,0)))
}

trainAccuracy <- f1(traindata, traindata$class)
validationAccuracy <- f1(valdata, valdata$class)
testAccuracy <-f1(testdata, testdata$class)

performance <- round(t(as.data.frame(rbind(t(trainAccuracy$byClass), 
                                           t(validationAccuracy$byClass), 
                                           t(testAccuracy$byClass)))),3)
colnames(performance) <- c("Traning", "Validation", "Test")

performance

```

7.  Biologists may prefer to use a higher probability cutoff
    to avoid false positive results, even if it means
    predicting fewer regulatory interactions. The ROC curve
    is a perfect instrument which shows how the classifier"s
    performance changes at different threshold settings for
    guidance. In the following code, roc.plot function from
    verification package is used to plot ROC curve

```{r, eval=FALSE}

pdf(file = "DeepLearning_Figure4.pdf",
    width = 5, height = 5, pointsize = 12, useDingbats = FALSE)

verification::roc.plot(testdata$class, predprob, 
                       ylab = "True Positive Rate", 
                       xlab = "False Positive Rate")
abline(v = 0.5596, col = "red2", lty = 3)
abline(h = 0.0509, col = "steelblue1", lty = 3)

dev.off()

```


8. The following code shows that how to save the model for future use, and we also save performance table, expression and gold standard data which we all need to reproduce all the results or to make new better deep learning model. 

```{r, eval=FALSE}
save_model_hdf5(model, "EcoliRegModel.h5")
save(exp, performance, gold, history, file = "EcoliRegModel.RData")
```
 
The save_model_hdf5() function saves the model in the Hierarchical Data Format version 5 (HDF5) file format. HDF5 is a file format designed to store and organize large amounts of numerical data. It is a binary file format that can store the model architecture, weights, optimizer state, and any other information related to the model.
 
## Predicting genome-wide regulatory interactions

1. In this section, we will demonstrate how to evaluate the pairwise combinations of transcription factors (TFs) and genes in *Escherichia coli* using our model. We will assess the probability of these pairs being potential regulatory pairs. It"s important to note that the total number of TF-gene pairs for *Escherichia coli* exceeds a million and a half, and in higher organisms, the number is even greater. Consequently, most computers may struggle to handle the feature data for all these pairs, potentially causing crashes. Therefore, the following code is specifically provided for users who do not possess high-end computational resources. For those with ample computational resources, I recommend referring to the code provided in **Note 13**. As these codes can be used across different sessions of RStudio, they assume that the computer or RStudio has been restarted and the same directory has been set (can be verified using the getwd() function). 


```{r, eval=FALSE}

library(magrittr)
library(keras)
library(tensorflow)

set.seed(1979)


model <- load_model_hdf5("EcoliRegModel.h5")

# import expression data file
exp <- t(read.table("net3_expression_data.tsv", header = T))

# import original gene names mapping 
genenames <- read.table("net3_gene_ids.tsv", header = F)
genes <- genenames$V2; names(genes) <- genenames$V1

# import list of all transcription factors of Escherichia coli
# tfs <- names(genes)[genes %in% c("gadX", "flhC", "flhD","dnaA")] # trail run
tfs <- read.table("net3_transcription_factors.tsv", header = F)$V1

length(tfs)*nrow(exp)


predictions <- NULL

for(i in tfs){
  
  tfdata <-data.frame(tf = i, gene = rownames(exp), 
                      tfname = genes[i],
                      genename = genes[rownames(exp)])
  tfdata <- tibble::as_tibble(tfdata[tfdata$tf != tfdata$gene,])
  
  inpreddata <- cbind(exp[tfdata$tf,], exp[tfdata$gene,])
  inpreddata <- cbind(inpreddata, 
                      sapply(seq.int(dim(tfdata)[1]), 
                             function(i) 
                               cor(exp[tfdata$tf[i],], 
                                   exp[tfdata$gene[i],])))
  
  tfdata$probability <- (model %>% predict(inpreddata))[,1]
  
  predictions <- rbind(predictions, tfdata[tfdata$probability>0.5,])
  
}

predictions <- predictions[rev(order(predictions$probability)),]
predictions

```


4. Our model predicted only 80,225 as regulatory out of 1,506,674 TF-gene pairs. There are number of sophisticated ways to evaluate the predicted GRN. Here, however, we will quickly select few TF-gene pairs with high probability of being regulatory, and will study them. Escherichi coli has many acid inducible systems which protect cells  against acid stress to pH 2 or below. Of them, the glutamate depending system is the most effect and relies on two glutamate decarboxylases (GadA and GadB) combined with a putative glutamate:gamma-aminobutyric acid antiporter (GadC). According to our predictions, it appears to be regluated by OsmE, which is osmolarity inducible factor, however, there is no information exists that it regulates glutamate-dependant acid resistance system. though functionally it would make sense. The GadX is a known regulator of the acid resistance system. Also, master transcription factors associated with flagella biosynthesis and chemotaxis were among the highly ranked TF-gene pairs. So, lets check target genes of GadX, FlhC, and FlhD transcription factors.  

```{r, eval=FALSE}
predictions[predictions$tfname=="gadX",]
predictions[predictions$tfname=="flhC",]
predictions[predictions$tfname=="flhD",]
```

Our results suggests that GadX may regulate 16 genes at the probability cutoff of 0.5 or above, and of 16, five genes i.e. gadA, gadE gadB, gadC and gadW belong to glutamate dependant acid protection system, confirming the accuracy of our prediction system. Likewise, FlhC and FlhD regulates a large number of genes involved in flagellar assembly and chemotaxis reassuring our prediction system. The overlap of their target genes is 652, which is highly expected. One of the problem of deep learning algorithm is that they can memorize the training dataset and many of the predictions could be part of training or validation dataset. Therefore, lets remove all predictions that were part of the training, and again cross-check the results using following command. As you can see, there are still many predictions which were novel, and make perfect sense at functional level.  

```{r, eval=FALSE}
predictions[!paste0(predictions$tf,predictions$gene) %in%  
              paste0(inputdata$tf[inputdata$class==1], 
                     inputdata$gene[inputdata$class==1]),] 
```


4. To export results from prediction object to a tabular, csv or excel file, use following code. The third command requires writexl R package which should be installed using install.packages function. Likewise, you can also export the performance table. 

```{r, eval=FALSE}
write.table(predictions, file = "grnPredictionsEcoli.txt", 
            col.names = T, row.names = F, quote = F, sep = "\t")
write.csv(predictions, file = "grnPredictionsEcoli.csv", row.names = F)
writexl::write_xlsx(list(Table1 = predictions), 
                    path = "grnPredictionsEcoli.xlsx", col_names = T, )

```


## Handiling imbalanced gold standard data 

5.  Imbalanced data is a common challenge in biology, making
    training deep learning models tricky
    [@golddeeplearning]. R packages such as caret, ROSE, and
    smote [@caret; @rose; @smote] are popular resources for
    addressing imbalanced data. While the smotefamily package
    utilizes the SMOTE algorithm to generate synthetic
    samples of the minority class. Another approach is to down-size the majority class examples to make them compatible with minority class. I have used this approach for our problem. Also, I often prefer the class
    weight approach. In the provided code snippet, the
    "class_weight" parameter is used during model training
    for binary classification. By calculating class weights
    based on the class distribution in the training data,
    and passing them as the "class_weight" argument in the
    fit function, you can assign higher weights to the
    minority class and lower weights to the majority class.
    This enables the model to effectively handle class
    imbalance during optimization, prioritizing the minority
    class samples for better learning outcomes. 

```{r, eval=FALSE, echo=FALSE}

weights <- as.list((1/table(traindata$class)))

history <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class, 
                         epochs = 10, batch_size = 16, 
                         validation_data = list(as.matrix(valdata[,featurenames]), valdata$class),
                         class_weight = weights)

```

## Increasing generalization and depth of DNN

7.  The number of layers and neurons (units) in a neural
    network can be adjusted to enhance its learning capacity
    [@layersneurons]. However, there is no definitive rule
    on how many layers and neurons a network should have for
    a given dataset. If the data is linearly separable,
    multiple hidden layers may not be necessary. It is
    generally recommended to keep the number of hidden
    layers as minimal as possible. The number of neurons in
    a layer can be determined
    based on the shape of your training data. Typically, the
    number of neurons in a layer is equal to the number of
    features (columns) in your data. Some network
    configurations add an additional node for a bias term.
    It is crucial to maintain a low number of nodes to
    ensure the network"s ability to generalize well. Having
    an excessive number of nodes may cause the network to
    perfectly recall the training set but perform poorly on
    new, unseen samples. The following code shows how to include more layer but there are numbe of different ways one can add layers and decide on number of neurons. Usually in each successive layers, the number of neurons are decreased.
    
```{r, eval=FALSE, echo=FALSE}

model <- keras_model_sequential(input_shape = c(length(featurenames))) %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 56, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(0.1) %>%
  layer_dense(units = 1, activation = "sigmoid")

```


## Performance measures


9.  Accuracy can be misleading when dealing with imbalanced
    datasets as it does not account for class imbalance.
    Recall and precision metrics are often preferred over
    accuracy in most scenarios, especially when the cost of false
    positives and false negatives differs significantly.
    Recall measures the proportion of true positive cases
    correctly identified, while precision measures the
    proportion of correctly identified positive cases. These
    metrics provide a more comprehensive understanding of
    the model"s performance, especially when the focus is on
    correctly identifying specific classes or minimizing
    false positives/negatives. These metrics could be set in
    with the complie() function as shown below. Remove
    comment (#) character to include the metric for
    training.

```{r, eval=FALSE, echo=FALSE}

metrics <- list(
  "fn" = metric_false_negatives(),
  "fp" = metric_false_positives(),
  "tn" = metric_true_negatives(),
  "tp" = metric_true_positives(),
  "accuracy" = metric_accuracy(),
  "precision" = metric_precision(),
  "recall" = metric_recall(),
  "auc" = metric_auc())

model %>% compile(optimizer = optimizer_adam(),
                  loss = "binary_crossentropy",
                  metrics = metrics)

```

## An alternative code for prediction of GRN on high-end computational resources.


```{r, eval=FALSE}

library(magrittr)
library(keras)
library(tensorflow)

set.seed(1979)


model <- load_model_hdf5("EcoliRegModel.h5")

# import expression data file
exp <- t(read.table("net3_expression_data.tsv", header = T))

# import original gene names mapping 
genenames <- read.table("net3_gene_ids.tsv", header = F)
genes <- genenames$V2; names(genes) <- genenames$V1

# import list of all transcription factors of Escherichia coli
tfs <- read.table("net3_transcription_factors.tsv", header = F)$V1

# create all possible pairs between tanscription factors and genes

predictions <- expand.grid(tfs,  rownames(exp), stringsAsFactors = FALSE)
predictions <- tibble::as_tibble(all_pairs[predictions$Var1 != predictions$Var2,])

# add original gene names

predictions$tfname <- genes[predictions$Var1]
predictions$genename <- genes[predictions$Var2]

# create feature table
inpreddata <- cbind(exp[predictions$Var1,], exp[predictions$Var2,])
inpreddata$pcc  <- sapply(seq.int(dim(predictions)[1]), 
                          function(i) cor(exp[predictions$Var1[i],], 
                                          exp[predictions$Var2[i],])))

# predict regulatory pairs
predictions$probability <- (model %>% predict(inpreddata))[,1]
predictions <- predictions[predictions$probability>0.5,]
predictions <- predictions[rev(order(predictions$probability)),]
predictions 
```


## Controlling DNN model training by Callback functions

EarlyStopping Callback: This callback monitors the
validation loss and stops the training if the validation
loss does not improve after a certain number of epochs
(defined by the "patience" parameter).

```{r, eval=FALSE}

early_stopping <- callback_early_stopping(monitor = "val_loss", patience = 3)


```

ModelCheckpoint Callback: This callback saves the model
weights to a file ("best_model.h5") whenever the validation
accuracy improves. By setting "save_best_only" to True, it
only saves the best model based on the monitored metric..

```{r, eval=FALSE}

model_checkpoint <- callback_model_checkpoint(filepath = "best_model.h5", 
                                              monitor = "val_accuracy", 
                                              save_best_only = TRUE)


```

ReduceLROnPlateau Callback: This callback reduces the
learning rate when the validation loss plateaus. The
learning rate is multiplied by the "factor" parameter, and
"patience" determines the number of epochs to wait before
reducing the learning rate.

```{r, eval=FALSE}

reduce_lr <- callback_reduce_lr_on_plateau(monitor = "val_loss", 
                                           factor = 0.1, patience = 2)


```

TensorBoard: This callback enables logging for TensorBoard,
which is a visualization tool for monitoring the training
process. It creates logs that can be visualized using
TensorBoard.

```{r, eval=FALSE}

tensorboard <- callback_tensorboard(log_dir = "logs")


```

Once callback functions are defined they can be incorporated during training as shown below for early_stopping and reduce_lr callback functions, but you can have as many functions as you want.

```{r, eval=FALSE}

history <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class,
                           epochs=40, batch_size = 50, validation_split=0.2,
                           callbacks = list(early_stopping, reduce_lr))


```
