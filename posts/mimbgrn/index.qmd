---
title: "Deep Learning for Predicting Gene Regulatory Networks: A Step-by-Step Protocol in R"
subtitle: Transcriptional regulatory network predictions
author:
  - name: Vijaykumar Yogesh Muley
categories: [Deep learning, GRN] # self-defined categories
citation:
  type: article-journal
  container-title: Methods in Molecular Biology
  volume: 2719
#  issue: 2
  issued: 2023-08
#  issn: 1539-9087
  url: https://doi.org/10.1007/978-1-0716-3461-5_15 
#image: preview_image.jpg
draft: false 
---

# Introduction

In the past two decades, machine learning has become a
powerful tool for predicting gene regulatory networks (GRN)
or transcriptional regulatory networks (TRN), which involve
transcription factors (TFs) and their target genes
[@Marbach2012; @Li2023; @Muley2022]. TFs
are a diverse family of DNA-binding proteins that regulate
gene expression by binding to specific DNA sequences in the
target gene's upstream regions [@Dynan1983;
@Vaquerizas2009]. Understanding GRNs is crucial for studying
transcriptional regulation in various cellular processes,
such as development, diseases, and response to
environmental stimuli [@Muley2020; @Levine2003; @Salah2016].

Machine learning, a sub-field of artificial intelligence,
has revolutionized complex problem-solving across various
domains, including finance, marketing, engineering,
medicine, and life sciences [@Greener2022; @Lecun2015]. It
enables machines to learn and improve from experience
without explicit programming. Machine learning has proven to
be a successful approach in predicting GRNs by utilizing co-expression
patterns among TFs and their target genes [@Marbach2012]. This is achieved by harnessing the vast amount of
publickly available microarray and next-generation
sequencing RNA-Seq data [@geo]. These
datasets, known as expression profiles, are represented
as a matrix with dimensions $m * n$. Where, $m$ represents the
total number of genes expressed by an organism, and $n$
corresponds to the number of physiological conditions or
experimental contexts in which gene expression was measured.

In simple terms, gene expression varies across different physiological conditions, resulting in distinct features or attributes. Genes and TFs themselves can be used as training examples for machine learning. Unsupervised machine learning algorithms can classify genes into groups based on similar features, particularly correlated expression. Genes that belong to the same group are often co-regulated and have functional relationships, offering insights into potential TFs that drive their correlated expression when they are part of the same group [@Barzel2013]. Clustering algorithms are examples of unsupervised machine learning methods that can infer TF-gene relationships without prior knowledge.

In contrast, supervised machine learning algorithms work with pre-defined groups. For instance, pairs of TFs and genes can be divided into two pre-defined groups: regulatory and non-regulatory pairs. In machine learning, these pre-defined groups are referred to as labels. Supervised algorithms learn a mathematical function, known as a classifier or model, to distinguish between labels by identifying label-specific patterns/rules from the input features [@Marbach2012]. The resulting classifier can then be used to assign the most appropriate labels to unclassified TF-gene pairs. The labeled data used to train the model is known as the gold standard or ground truth. Both types of machine learning utilize the inherent structure of the input data to uncover complex and hidden trends for classifying unlabeled examples, regardless of whether prior knowledge is available.

In traditional machine learning, researchers typically manually engineer features to train algorithms. However, deep learning, a subset of machine learning that utilizes neural networks, has the ability to automatically learn features from input data [@SCHMIDHUBER201585; @Lecun2015]. Deep learning models are structured into layers, with each layer responsible for transforming input data, extracting meaningful features that differentiate between labels, and generating output - the predicted labels. The depth of the model is determined by the number of layers it has. Within each layer, nodes or neurons perform calculations using activation functions, gradually transforming the data into more abstract representations [@Glorot2011; @Cybenko1989]. This process continues through the layers until the final output is produced. Through backpropagation, the model adjusts its parameters or weights to correct errors when the output doesn't match the expected gold standard labels, thereby improving accuracy. The resulting classifier can then be used to classify unclassified TF-gene pairs into either regulatory or non-regulatory groups based on the associated features. Deep learning models excel in analyzing and combining multiple features, leading to higher accuracy and greater capability compared to traditional machine learning models.


This chapter focuses on implementing a deep learning classifier using TensorFlow, an open-source software framework developed by Google. The classifier is built using the Keras API within the RStudio environment. The main goal of this chapter is to demonstrate a step-by-step protocol for predicting regulatory interactions between TFs and genes, using their expression profiles as the key features.

The protocol utilizes publicly available gene expression data and gold standard information for *Escherichia coli*, obtained from the DREAM5 project publication [@Marbach2012]. It guides readers through essential steps including data preprocessing, designing the neural network architecture, model training and validation, and making predictions for novel regulatory interactions. Additionally, the chapter provides valuable insights into parameter tuning for deep learning models.

By the end of this chapter, readers will have a comprehensive understanding of how to apply deep learning techniques to predict regulatory interactions between TFs and genes. This knowledge can be directly applied to their own research projects, enabling them to harness the power of deep learning in their investigations.

# Material

## Computational and software requirements

1.  This protocol has been tested on a desktop
    computer/laptop with 8 GB RAM running Windows and Mac
    OS. The computer's configuration required to fulfill
    learning and prediction tasks depend on the problem's
    complexity and the size of the training and unlabeled
    data. It is recommended to avoid running other
    applications simultaneously.

2.  While this protocol contains sufficient details to be
    followed by scientists without programming experience,
    basic to intermediate R programming skills are expected.

3.  Download and install R and its graphical user interface,
    RStudio, from <https://www.r-project.org> and
    <https://www.rstudio.com>, respectively.

4.  To efficiently execute R commands and document the
    entire protocol, create a file in RStudio by navigating
    to the File menu, selecting New File, and then choosing
    R script. By default, the file will be named
    "untitled.R". It is recommended to save this file with a
    meaningful name in the folder where you will execute
    this protocol.

5.  Next, go to the Session menu in RStudio, select Set
    Working Directory, and then click on To Source File
    Location, i.e., the directory where the untitled.R file
    was saved. This ensures that all the commands will be
    executed in the same directory, and the location will be
    used as a reference to import or export the data saved
    and generated for this protocol.

6.  Going forward, the "untitled.R" file will be used to
    input and save all R commands, allowing you to easily
    reproduce the results from scratch. To execute the
    commands, simply select them using the mouse or
    keyboard, and click on the "Run" option located in the
    top right corner of the file in RStudio. The output of
    the executed commands will be displayed in the console
    panel located below the file panel.

7.  This protocol relies on Keras and TensorFlow R packages,
    i.e., external libraries for deep learning-specific
    functions [@keras; @tensorflow]. To install these
    packages, copy the commands described below to your
    "untitled.R" file, select them, and execute them using
    the Run option. For more details about the following
    commands, please refer to their source website:
    <https://tensorflow.rstudio.com/install/>, and also
    <https://www.tensorflow.org/install>.

8.  To install the tensorflow R package, run the following
    command:

```{r, eval=FALSE}
install.packages("tensorflow")
```

9.  Before using the tensorflow package, we need to load the
    reticulate library and configure R to use a Python
    installation. If you already have Python installed, you
    can skip the install_python() command and provide the
    path to the Python executable. Here's an example of how
    to do it on macOS or Unix/Linux operating systems:

```{r, eval=FALSE}
# assuming python is not already installed

library(reticulate)
path_to_python <- install_python()
virtualenv_create("r-reticulate", python = path_to_python)

# assuming that the python is already installed and 
# executable is available in /usr/local/bin/ folder (check with /usr/bin/ also)

library(reticulate)
path_to_python <- "/usr/local/bin/python3" 
virtualenv_create("r-reticulate", python = path_to_python)

```

10. Next, use the install_tensorflow() function from the
    tensorflow package to install TensorFlow.

```{r, eval=FALSE}
library(tensorflow)
install_tensorflow(envname = "r-reticulate")
```

11. Alternatively, you can use the install_keras() function
    from the keras package to install TensorFlow along with
    some commonly used packages such as "scipy" and
    "tensorflow-datasets" as shown below.

```{r, eval=FALSE}
install.packages("keras")
library(keras)
install_keras(envname = "r-reticulate")
```

12. To confirm that the installation was successful, run the
    following command, which should return
    *tf.Tensor(b'Hello Tensorflow!', shape=(),
    dtype=string)*. If not then try "install_tensorflow()" command again. 

```{r, eval=FALSE}
library(tensorflow)
tf$constant("Hello Tensorflow!")
```

13. Install following packages to facilitate data handling
    and analysis.

```{r, eval=FALSE}
install.packages(c("readr", "tibble", "caret", "verification"))
```

## Data requirements

1. To begin, download Supplementary Data 1 from the DREAM5 network inference challenge publication (https://doi.org/10.1038/nmeth.2016) and unzip it in your current working directory. This action will create a directory named "DREAM5_network_inference_challenge." For detailed information about this data, please refer to the original publication by Marbach et al. [@Marbach2012]. If you have your own data, it should have the format similar to that of first three files described below. 

2. The required data for Escherichia coli is present in the "Network3" folder and its sub-folders. Copy four files listed below from these sub-folders into your current working directory to proceed with the analysis..

3. *net3_expression_data.tsv*: This file contains the expression profile data.

4. *net3_transcription_factors.tsv*: This file contains the list of transcription factors.

5. *DREAM5_NetworkInference_GoldStandard_Network3.tsv*: This file contains the gold standard data, which includes both regulatory (positive examples) and non-regulatory (negative examples) TF-gene pairs.

6. *net3_gene_ids.tsv*: This file serves as a mapping between the anonymous gene codes used in other files and the original gene names.


# Methods

## Loading R libraries and preparing workspace

1.  Load the Keras and
    TensorFlow libraries for use in the current RStudio
    session as shown in below along with magrittr. The magrittr library provides a useful function
    called *pipe* (i.e., %>%), which allows us to pass the
    output of one operation to another.

```{r, eval=FALSE}
library(magrittr)
library(keras)
library(tensorflow)
```

2.  Set a random odd number as a seed in the set.seed
    function to reproduce the results of this workflow (see
    **Note 1**). If you use seed number other than 1979, your results will be slightly different than the discusseed in the following text.

```{r, eval=FALSE}
set.seed(1979)
```

## Data preparation and exploration

1.  Import the gene expression data and store in a exp
    object and do a routine check up as shown below. **Note
    2** enlists popular public resources to obtain gene
    expression data for species of interests.

```{r, eval=FALSE}
exp <- read.table("net3_expression_data.tsv", header = T)
exp <- t(exp)
dim(exp)
exp[1:5,1:5]
```

The exp object is a data.frame containing the genes of
*Escherichia coli* in the rows and their expression levels
in diverse physiological conditions and time-points in the
columns. Note that the genes are anonymized for the DREAM5
challenge [@Marbach2012], and we will map them back to their original gene
names later.

2.  Normalization is a crucial step in data analysis, and
    gene expression data should be normalized if required
    (see **Note 3**). In this case, the data was already
    normalized, as indicated by the distribution plot
    generated using the following command and exported to PDF file i.e. *DeepLearning_Figure1.pdf*. The plot shows
    that the data is normally distributed and ready for
    further analysis (@fig-figure1).

```{r, eval=FALSE}
# plot
hist(exp, 100, col = "darkgrey", border = "white", 
     xlab = 'Expression intensity', main = "Gene expression")

# export plot to pdf file in the current directory
pdf(file = 'DeepLearning_Figure1.pdf', 
    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)
hist(exp, 100, col = "darkgrey", border = "white", 
     xlab = 'Expression intensity', main = "Gene expression")
dev.off()

```

![**Expression distribution of Escherichia coli genes**. The shape of the histogram reveals a normal distribution of expression data. This suggests that the expression data may not require additional processing.](DeepLearning_Figure1.pdf){#fig-figure1 fig.pos='b' height=100% }


3.  Load the gold standard data using the read_table
    function from the readr package and display its
    contents, as shown below. **Note 4** enlists popular
    public resources that can be used to create gold
    standard data for species of interests.

```{r, eval=FALSE}
gold <- readr::read_table("DREAM5_NetworkInference_GoldStandard_Network3.tsv",
                          col_names = F)
gold

```

In the gold object, the first two columns respectively list
the TFs and genes, while the third column
contains binary labels. A value of 1 in the label column
indicates that experimental evidence exists for the
regulation of the gene in column 2 by the transcription
factor in column 1, while a value of 0 indicates that there
is no known regulatory interaction between them.

3.  With the following command, TFs and genes were retained
    in the gold standard data if they were also present in
    the expression data. Next, we use the table function
    from base R to count the number of regulatory and
    non-regulatory pairs.

```{r, eval=FALSE}
gold <- gold[gold$X1 %in% rownames(exp) & gold$X2 %in% rownames(exp), ]
table(gold$X3)

```

4.  The gold standard data is imbalanced, with 2066
    regulatory pairs and 150,214 non-regulatory pairs. This
    is usually a common issue in biology, and training deep
    learning model on imbalanced gold standard can be tricky
    (see **Note 5**), which is explained in detail later.
    For time being, we will select all regulatory pairs and
    sample two times the number of non-regulatory pairs
    randomly as shown below. This is called down-sampling of majority class examples.

```{r, eval=FALSE}
keep_indices <- c(which(gold$X3==1), 
                  sample(which(gold$X3==0), size = sum(gold$X3)*2))
gold <- gold[keep_indices,]
table(gold$X3)

```

5. The data preparation process involves combining expression values for each TF-gene pair, allowing the raw data to be efficiently processed by a deep learning model for feature extraction. This step creates a data.frame that includes TF-gene pairs (optional) and their corresponding expression values. Next, the Pearson Correlation Coefficient (PCC) is calculated between the expression values of the TF and the gene in each pair. This correlation measure is a conventional approach for engineering features from raw data. The resulting correlation coefficients are added as a new column named "pcc" within the data.frame. Subsequently, labels are appended as the last column of the data.frame. The first two columns of the data.frame represent the names of the TF and gene, respectively. The remaining columns contain the expression values for the TF and gene, followed by the calculated PCC values between their expression values, and finally the gold standard label.

To facilitate further analysis, the column names that correspond to the expression values and PCC are stored in the "featurenames" object, as they will serve as features for subsequent processing.

The code snippet below demonstrates these steps:

```{r, eval=FALSE}

inputdata <- data.frame(tf = gold$X1, gene = gold$X2, 
                        exp[gold$X1,], exp[gold$X2,])
inputdata$pcc <- sapply(seq.int(dim(gold)[1]), function(i) 
                        cor(exp[gold$X1[i],], exp[gold$X2[i],]))
inputdata$class <- gold$X3

inputdata <- tibble::as_tibble(inputdata)

featurenames <- colnames(inputdata) %>% setdiff("class")  %>% 
  setdiff("tf") %>% setdiff("gene")

```

6. Machine learning models are built upon certain assumptions, that the input data inherently contains relevant structure for the classification task at hand. In our specific case, we assume that the expression of TFs is directly proportional to the expression of their target genes, as TFs regulate the activity of these genes. However, it's important to acknowledge that this assumption may not always hold true due to various factors, such as post-translational modifications [@Deribe2010]. This introduces non-linearity into the input data, making machine learning in non-linear high-dimensional spaces challenging and potentially leading to the creation of poor models.

Therefore, it is crucial to exercise caution when selecting data for modeling purposes. @fig-figure2 visually illustrates a clear distinction between expression values of TF-gene regulatory pairs and randomly selected pairs, thus indicating the suitability of using supervised learning techniques. The provided code snippet generates @fig-figure2.

```{r, eval=FALSE}
h1 <- hist(as.matrix(inputdata[inputdata$class=="1",featurenames]), 
           breaks = 100, plot = FALSE)
h2 <- hist(as.matrix(inputdata[inputdata$class=="0",featurenames]), 
           breaks = 100, plot = FALSE)

pdf(file = 'DeepLearning_Figure2.pdf', 
    width = 5, height = 5, pointsize = 10, useDingbats = FALSE)
plot(h2, col = rgb(0,0,1,0.4), freq = FALSE, 
     xlab = 'Expression intensity', 
     main = "Distribution of feature values for training data")
plot(h1, xaxt = 'n', yaxt = 'n', col = rgb(1,0,0,0.4), 
     freq = FALSE, add = TRUE)
dev.off()


```

![**Expression distribution of Escherichia coli TF-gene pairs**. The expression value distribution of non-regulatory TF-gene pairs shows unimodal normal distribution (purple color), while regulatory pairs show bimodal distribution, in which expression values surronding the first peak overlaps with the distribution of non-regulatory pairs, and the other peak ranges in a higher expression value zone, and distinct from the other two peaks. One of the explaination for this trend is that TF and genes of the regulatory pairs are dependant and express at higher levels than those form non-regulatory pairs. It indicate that expression data without processing contains regulatory signals and can be used for deep learning. ](DeepLearning_Figure2){#fig-figure2 fig.pos='b' height=100% }


7.  In general, gold standard data is divided into training, validation,
    and test sets, which is for training the model, validating the performance while model is being produced, and to estimate the accuracy final model in real world data. The division is often based on availability of the data. Typically, 60-80% of the input data retains for training, while
    40-20% is equally divided between validation and test datasets. The following code implements this process and create train,validation and test datasets in 80:10:10 proportions.

```{r, eval=FALSE}

indices <- nrow(inputdata) %>% sample.int(., ceiling( . * 0.8))
traindata <- inputdata[indices, ]
testdata <- inputdata[-indices, ]
indices <- nrow(testdata) %>% sample.int(., ceiling( . * 0.5))
valdata <- testdata[-indices, ]
testdata <- testdata[indices, ]
table(traindata$class)
table(valdata$class)
table(testdata$class)

```

## Defining deep learning model architecture

1.  To define the architecture of our deep learning model, we first
    create a sequential deep neural network (DNN) using the
    keras_model_sequential() function from the Keras
    library. We then add two dense layers using the
    layer_dense() function. The input layer is defined with
    the input_shape parameter, which needs to be equal to
    the total number of features in the input data or represents the shape of the features. Next, the
    single hidden layer contains 806 neurons and applies the
    "relu" activation function to introduce non-linearity in
    the layer's output @Glorot2011. The second layer_dense()
    function adds another fully connected layer with a
    single output unit. This output layer computes a
    weighted sum of the inputs from the previous layer feeds
    and applies the sigmoid activation function to produce a
    probability value between 0 and 1 @Cybenko1989,
    indicating the likelihood that the input corresponds to
    the positive class (regulatory pairs, in this case) based on input features. It
    is worth mentioning that successive layers are able to
    dynamically interpret the number of expected inputs
    based on the previous layer. Below is a typical code to
    define the DNN architecture.

```{r, eval=FALSE}
set_random_seed(1979)

model <- keras_model_sequential(
  input_shape = c(length(featurenames))) %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

```

2.  The code above includes a set_random_seed function for
    setting a random seed for TensorFlow operations to
    reproduce deep learning model (see **Note 6**). 

3.  The number of layers and units can be adjusted to
    improve the model's capacity to learn, but too much
    capacity can lead to overfitting, and too low underfit
    the data (see **Note 7**). The single hidden layer contains 806 neurons which is equivalent to the total number of physiological conditions in the expression data (columns) and the pcc column representing PCC for each TF-gene pair. 

4.  The summary function gives an overview of the model's
    architecture. The model has a simple architecture with
    input layer which feeds feature matrix and labels to fully connected layers hidden layer with "relu"
    activation, and a another fully connected output layer with "sigmoid"
    activation to produces a single probability value for
    each training example of being regulatory pair.

## Defining Model Compilation Parameters

1.  To configure the learning process, the model's
    compilation parameters are specified using the following
    code. During training, the DNN
    randomly assigns weights to all the neurons and their
    connections, and predicts the output. The DNN then
    assesses the accuracy of the output, and if necessary,
    automatically adjusts these weights to improve accuracy
    via the backpropagation process. The DNN requires an
    objective function to measure performance, which is
    referred to as the loss or objective function, as well
    as an optimizer function that directs learning towards
    the minimum loss by keeping track of previous best
    performances. The following code defines the compilation
    parameters for the DNN.

```{r, eval=FALSE}

model %>% compile(optimizer = optimizer_adam(),
 loss = "binary_crossentropy",
 metrics = c("accuracy"))

```

2.  The compile() function configures the model.

3.  The optimizer_adam() function is a widely used
    optimization algorithm that computes adaptive learning
    rates for each parameter based on the moment estimates
    of the gradient and past gradients and helps neural
    network to converge faster @kingma2017adam. See **Note
    8** for more details on learning and optimizer.

4.  The loss function measures the difference between
    predicted and actual values. The "binary_crossentropy"
    loss function is commonly used for binary classification
    tasks.

5.  In the above example, the "accuracy" metric is used to
    calculate the percentage of correctly predicted labels
    by the model. Accuracy metric is fine for prototyping
    DNN, but use more reliable metrics for practical
    application (see **Note 9**).

## Training deep learning model

1.  The following code chunk fits the DNN model to the
    training data and validates its performance using a
    validation dataset by using the fit() function:

```{r, eval=FALSE}
history <- model %>% fit(as.matrix(traindata[,featurenames]), 
                         traindata$class,
                         epochs=100, batch_size = 64,
                         validation_data = list(as.matrix(valdata[,featurenames]), 
                                                valdata$class))

```

2.  The fit() function takes the training data in the form
    of a matrix of predictor variables, and the
    corresponding binary class labels, epochs, batch size, and validation data.

3.  Epochs determine how many times the entire dataset is
    used for training the DNN. More epochs can
    improve performance, but there is a risk of overfitting
    to the training data (see **Note 10**).

4.  Batch size refers to the number of examples processed
    together in each training iteration. Smaller batch sizes
    can improve generalization, but convergence may be
    slower. Larger batch sizes speed up training but may
    result in poorer generalization. The optimal batch size
    depends on the dataset size and available computational resources, especially memory
    (see **Note 11**).

5.  The code uses validation data (optional) to assess the
    model's performance during training. Alternatively, use
    the validation_split parameter to specify the percentage
    of training data for validation (see **Note 12**). These
    parameters are optional and can be adjusted to suit the
    model and data requirements.

## Estimating the accuracy of deep learning model

1.  The history object records the loss and accuracy during the training over specified number of epochs. This information can be used to quickly check the performance of the model during training. The print(history) command shows that at the final epoch, the loss was 0.4628 on training data which is higher than the validation data i.e. 0.4296. Lower loss is better fit. Then, the accuracy was 0.7937 and 0.8174 respectively. We can also use evaluate function to compute accuracy and loss function over all epochs as shown below. The code plot(history)  generates a plot of the training and validation loss and  accuracy over the course of the training process. This  visualization can help identify trends or patterns in  the performance of the model.

```{r, eval=FALSE}
print(history)
evaluate(model, x = as.matrix(traindata[,featurenames]), y = traindata$class)
evaluate(model, x = as.matrix(valdata[,featurenames]), y = valdata$class)
plot(history)


```

2.  However, I prefer to access the 'loss' and 'accuracy'
    attributes from the history object, as this allows me
    the flexibility to visualize the data in a customized
    manner. For instance, @fig-figure3 was generated using the
    customized code provided below.

```{r, eval=FALSE}

pdf(file = 'DeepLearning_Figure3.pdf',
    width = 7, height = 4, pointsize = 10, useDingbats = FALSE)

par(mfrow = c(1,2), cex = 0.8)

maxLoss <- max(c(history$metrics$loss, history$metrics$val_loss))

plot(history$metrics$loss, main="Model Loss", xlab = "Epoch", ylab="Loss", 
     xlim = c(0, length(history$metrics$loss)), ylim = c(0, maxLoss),
     col="red2", type="b",lwd=1)
lines(history$metrics$val_loss, col="steelblue1", type="b",lwd=1)
legend("topright", c("Training","Validation"), col=c("red2", "steelblue1"), 
       lty=c(1,1), bty = "n")

plot(history$metrics$accuracy, col="red2", type="b",lwd=1, 
     main="Model Accuracy", xlab = "Epoch", ylab="Accuracy", 
     xlim = c(0, length(history$metrics$accuracy)), ylim = c(0, 1))
lines(history$metrics$val_accuracy, col="steelblue1", type="b",lwd=1)
abline(h = 0.5, col = "grey", lty = 3)
dev.off()


```

![**Expression distribution of Escherichia coli TF-gene pairs**. The distribution of expression values for non-regulatory TF-gene pairs follows a unimodal normal distribution (purple color). However, regulatory pairs exhibit a bimodal distribution (showed in reddish-orange colors). The first peak of the regulatory pairs' distribution overlaps with the distribution of non-regulatory pairs, while the second peak corresponds to higher expression values (orange color) and is distinct from the other two peaks. One possible explanation for this trend is that the TF and genes in regulatory pairs are dependent on each other and express at higher levels compared to non-regulatory pairs.](DeepLearning_Figure3){#fig-figure3 fig.pos='b' height=100% }

Upon observing the plot, we can identify certain inconsistencies (@fig-figure3). In a typical training process, the loss and accuracy gradually improve over multiple epochs, as evident in the plot. Initially, the loss experiences a steep decline up to the fifth epoch, after which it stabilizes from the tenth epoch onwards. The overall accuracy demonstrates a similar trend. These step-wise improvements indicate the model's effectiveness as it undergoes more training. The accuracy and loss after 30 epochs do not change much and can be set as the optimum number for epochs.  

Ideally, the accuracy and loss on the validation data should not exceed those on the training data since the model is trained on the latter. However, our results violate this assumption because validation data has low loss and high accuracy than the training data. If such discrepancies occur, it could indicate issues with the training process, model architecture, or the training and validation data itself and often referred to as over- or under-fitting of the input data.

In our specific case, multiple issues may coexist due to arbitrary parameter settings, incomplete gold standard data, an uneven distribution of positive and negative examples in the training and validation data, and the accuracy metric may not be good for our imbalanced data. However, these problems can be addressed during the model parameter tuning process, and explained in the next section. It is important to note that despite the model's performance not being ideal, the features do support the assumed classification task. The improvements in the loss and accuracy functions during training are smooth and indicative of good performance. While there is an issue with training and validation accuracy, it is marginal and may be a result of the imbalance in the gold standard data. Hence, it is crucial to evaluate the model's performance on the test data to estimate its practical accuracy. 

2.  The code provided below utilizes our model to predict the probabilities of outcomes for the test dataset, which is unlabelled and completely unknown to the model. This prediction is achieved by using the predict() function, and the resulting probabilities are stored in the variable predprob for further utilization.

```{r, eval=FALSE}

predprob <- model %>% predict(as.matrix(testdata[,featurenames]))

```

6.  To evaluate the performance of the DNN model on the test data and compare it with the training and validation data, the confusionMatrix() function from the caret package can be used. This function takes the predicted class probabilities in a binary format, specifically using a threshold of 0.5 or above (i.e., as.numeric(predprob > 0.5)), along with the true class labels. It then generates a table containing various important performance measures as shown below. 

```{r, eval=FALSE}

f1 <- function(indata, truelabels){
  res <-  model %>% predict(as.matrix(indata[,featurenames]))
  res <- as.numeric(res>0.5)
  res <- factor(res, c(1,0))
  caret::confusionMatrix(res, factor(truelabels, c(1,0)))
}

trainAccuracy <- f1(traindata, traindata$class)
validationAccuracy <- f1(valdata, valdata$class)
testAccuracy <-f1(testdata, testdata$class)

performance <- round(t(as.data.frame(rbind(t(trainAccuracy$byClass), 
                                           t(validationAccuracy$byClass), 
                                           t(testAccuracy$byClass)))),3)
colnames(performance) <- c("Traning", "Validation", "Test")

performance

```

The DNN model has an accuracy of 0.8323 on test data, which is
again better than the 0.8072 of training data, with a sensitivity of
0.5638 and specificity of 0.9491. In other words, the features
used for training are likely to distinguish 55.96% of real
regulatory interactions and 94.91% of non-regulatory pairs in the unlabelled data.  

7.  Biologists may prefer to use a higher probability cutoff
    to avoid false positive results, even if it means
    predicting fewer regulatory interactions. The ROC curve
    is a perfect instrument which shows how the classifier's
    performance changes at different threshold settings for
    guidance. In the following code, roc.plot function from
    verification package is used to plot ROC curve

```{r, eval=FALSE}

pdf(file = 'DeepLearning_Figure4.pdf',
    width = 5, height = 5, pointsize = 12, useDingbats = FALSE)

verification::roc.plot(testdata$class, predprob, 
                       ylab = "True Positive Rate", 
                       xlab = "False Positive Rate")
abline(v = 0.5596, col = "red2", lty = 3)
abline(h = 0.0509, col = "steelblue1", lty = 3)

dev.off()

```

![**The Receiver Operating Characteristic (ROC) curve for the deep learning model's performance on the test data**. The ROC curve shows that the model's discriminatory power and performance is very good at the standard probability value cutoff of 0.5, at which the proportion of actual negative cases incorrectly classified as positive i.e. false positive rate by the model are very low, and over 55% actual positive cases correctly classified as positive by the model, indicating that the model may perform well in practical applications.](DeepLearning_Figure4){#fig-figure4 fig.pos='b' height=100% }


The commands provided above generate @fig-figure4, which illustrates the trade-off between the true positive rate (TPR or sensitivity) and the false positive rate (FPR or 1-specificity) at various probability cutoffs. This plot assists in determining the optimal probability cutoff value to achieve the best results. The 0.5 probability cutoff, which we used to estimate accuracy, is indicated on the plot. It shows that at this cutoff, only 5.09% of the predicted regulatory pairs are expected to be incorrect, while 55.96% of the predicted regulatory pairs will be correct. 

8. In summary, all of the aforementioned results indicate that our model shows promising potential in practical applications. The following code shows that how to save the model for future use, and we also save performance table, expression and gold standard data which we all need to reproduce all the results or to make new better deep learning model. 

```{r, eval=FALSE}
save_model_hdf5(model, "EcoliRegModel.h5")
save(exp, performance, gold, history, file = "EcoliRegModel.RData")
```
 
The save_model_hdf5() function saves the model in the Hierarchical Data Format version 5 (HDF5) file format. HDF5 is a file format designed to store and organize large amounts of numerical data. It is a binary file format that can store the model architecture, weights, optimizer state, and any other information related to the model.
 
 
10. To prepare for the next section, it is assumed that all the commands mentioned above have been saved in a file named "Untitled.R" or with a similarly informative name. If necessary, restart RStudio to ensure a smooth transition. Restarting may not be essential but it can potentially expedite the subsequent analysis by clearing data dump from memory, particularly the prediction of the global gene regulatory network (GRN). Alternatively, you can proceed to the next section after executing the following commands.

```{r, eval=FALSE}
gc()
rm(history, testdata, traindata, valdata, predprob)
gc()

```


## Predicting genome-wide regulatory interactions

1. In this section, we will demonstrate how to evaluate the pairwise combinations of transcription factors (TFs) and genes in *Escherichia coli* using our model. We will assess the probability of these pairs being potential regulatory pairs. It's important to note that the total number of TF-gene pairs for *Escherichia coli* exceeds a million and a half, and in higher organisms, the number is even greater. Consequently, most computers may struggle to handle the feature data for all these pairs, potentially causing crashes. Therefore, the following code is specifically provided for users who do not possess high-end computational resources. For those with ample computational resources, I recommend referring to the code provided in **Note 13**. As these codes can be used across different sessions of RStudio, they assume that the computer or RStudio has been restarted and the same directory has been set (can be verified using the getwd() function). 


```{r, eval=FALSE}

library(magrittr)
library(keras)
library(tensorflow)

set.seed(1979)


model <- load_model_hdf5("EcoliRegModel.h5")

# import expression data file
exp <- t(read.table("net3_expression_data.tsv", header = T))

# import original gene names mapping 
genenames <- read.table("net3_gene_ids.tsv", header = F)
genes <- genenames$V2; names(genes) <- genenames$V1

# import list of all transcription factors of Escherichia coli
# tfs <- names(genes)[genes %in% c("gadX", "flhC", "flhD","dnaA")] # trail run
tfs <- read.table("net3_transcription_factors.tsv", header = F)$V1

length(tfs)*nrow(exp)


predictions <- NULL

for(i in tfs){
  
  tfdata <-data.frame(tf = i, gene = rownames(exp), 
                      tfname = genes[i],
                      genename = genes[rownames(exp)])
  tfdata <- tibble::as_tibble(tfdata[tfdata$tf != tfdata$gene,])
  
  inpreddata <- cbind(exp[tfdata$tf,], exp[tfdata$gene,])
  inpreddata <- cbind(inpreddata, 
                      sapply(seq.int(dim(tfdata)[1]), 
                             function(i) 
                               cor(exp[tfdata$tf[i],], 
                                   exp[tfdata$gene[i],])))
  
  tfdata$probability <- (model %>% predict(inpreddata))[,1]
  
  predictions <- rbind(predictions, tfdata[tfdata$probability>0.5,])
  
}

predictions <- predictions[rev(order(predictions$probability)),]
predictions

```


4. Our model predicted only 80,225 as regulatory out of 1,506,674 TF-gene pairs. There are number of sophisticated ways to evaluate the predicted GRN [@Muley2022]. Here, however, we will quickly select few TF-gene pairs with high probability of being regulatory, and will study them. Escherichi coli has many acid inducible systems which protect cells  against acid stress to pH 2 or below. Of them, the glutamate depending system is the most effect and relies on two glutamate decarboxylases (GadA and GadB) combined with a putative glutamate:gamma-aminobutyric acid antiporter (GadC). According to our predictions, it appears to be regluated by OsmE, which is osmolarity inducible factor, however, there is no information exists that it regulates glutamate-dependant acid resistance system. though functionally it would make sense. The GadX is a known regulator of the acid resistance system. Also, master transcription factors associated with flagella biosynthesis and chemotaxis were among the highly ranked TF-gene pairs. So, lets check target genes of GadX, FlhC, and FlhD transcription factors.  

```{r, eval=FALSE}
predictions[predictions$tfname=="gadX",]
predictions[predictions$tfname=="flhC",]
predictions[predictions$tfname=="flhD",]
```

Our results suggests that GadX may regulate 16 genes at the probability cutoff of 0.5 or above, and of 16, five genes i.e. gadA, gadE gadB, gadC and gadW belong to glutamate dependant acid protection system, confirming the accuracy of our prediction system. Likewise, FlhC and FlhD regulates a large number of genes involved in flagellar assembly and chemotaxis reassuring our prediction system. The overlap of their target genes is 652, which is highly expected. One of the problem of deep learning algorithm is that they can memorize the training dataset and many of the predictions could be part of training or validation dataset. Therefore, lets remove all predictions that were part of the training, and again cross-check the results using following command. As you can see, there are still many predictions which were novel, and make perfect sense at functional level.  

```{r, eval=FALSE}
predictions[!paste0(predictions$tf,predictions$gene) %in%  
              paste0(inputdata$tf[inputdata$class==1], 
                     inputdata$gene[inputdata$class==1]),] 
```


4. To export results from prediction object to a tabular, csv or excel file, use following code. The third command requires writexl R package which should be installed using install.packages function. Likewise, you can also export the performance table. 

```{r, eval=FALSE}
write.table(predictions, file = "grnPredictionsEcoli.txt", 
            col.names = T, row.names = F, quote = F, sep = "\t")
write.csv(predictions, file = "grnPredictionsEcoli.csv", row.names = F)
writexl::write_xlsx(list(Table1 = predictions), 
                    path = "grnPredictionsEcoli.xlsx", col_names = T, )

```


## Tuning Deep Learning for Improved Performance

1.  Developing an accurate DNN
    requires considering crucial aspects and parameters.
    Deep learning deals with high-dimensional spaces and
    vast data in a complex representations, making it challenging to find the optimal
    model configuration. Classification accuracy depends on
    problem complexity, data quality, and training accuracy.
    Fine-tuning hyperparameters that affect training process through practice and
    experimentation is necessary. Various approaches can
    enhance model performance.

2.  By experimenting with different activation functions,
    optimization algorithms, and loss functions, you can
    fine-tune your deep learning model to achieve better
    performance and more accurate predictions.

3. By changing the activation function,
you can influence how the model learns and represents
complex patterns. Experimenting with different activation
functions like sigmoid, ReLU, or tanh can help improve the
model's performance in capturing non-linear relationships
and making accurate predictions.

4. The choice of the optimization
algorithm affects how the model learns and converges to an
optimal solution. Trying different optimization algorithms
such as Adam, RMSprop, or stochastic gradient descent (SGD)
can impact the speed of convergence and the quality of the
final model. See **Note 8** for details and functions in Keras.

5. The loss function measures the model's
performance during training and guides the learning process.
Switching the loss function, such as using binary
cross-entropy instead of mean squared error, can improve the
model's ability to handle classification tasks and
optimize for the desired output.

3.  The design of the DNN involves carefully considering the number of layers and units, striking a balance between capturing complex relationships and avoiding overfitting for optimal configuration. Layers transform input data into meaningful representations, extracting relevant features. Deeper models with more layers learn intricate data representations, capture complex relationships, and discover abstract patterns. Most problems can be handled with one to three hidden layers, depending on linear and non-linear relationships in the features. The number of units within each layer impacts the model's capacity. Setting the number of units based on the total input feature shape, gradually decreasing, enables capturing finer details in the data. Adding more layers or units can enhance learning and generalization, but overfitting to training data becomes a risk. Finding the right balance involves iterative refinement and experimentation for optimal configuration that achieves desired performance without overfitting. See **Note 7** also.


4.  Dropout is a regularization technique commonly used in
    deep learning models to prevent overfitting [@dropout;
    @JMLRv15srivastava14a]. It involves randomly dropping a
    certain percentage of neurons or units in a layer during
    the training process. This dropout of neurons helps the
    network learn more robust and generalizable
    representations by reducing the reliance on specific
    input patterns. By randomly removing neurons,
    layer_dropout encourages the model to learn from
    different subsets of neurons, making it less sensitive
    to individual neurons and reducing the risk of
    overfitting. The dropout rate, which represents the
    proportion of neurons to be dropped, is an important
    hyperparameter that can be tuned to achieve optimal
    performance and prevent overfitting.

5.  The learning rate is a crucial parameter that determines the size of the steps taken by the model during training, influencing both the speed of convergence and the overall performance. If the learning rate is set too high, it can cause divergence, leading to a deterioration in the model's performance. On the other hand, a very low learning rate can result in slow convergence, prolonging the training process. It is common to start with default learning rates, which typically yield satisfactory results, and then fine-tune them as needed within the range of 0 to 1. The learning rate can be adjusted using the optimizer object, such as optimizer_adam(learning_rate = 0.001) in the compile function. 

6.  During each epoch, the model goes through one complete iteration of the training dataset, updating its parameters (weights and biases) based on observed patterns and the optimization algorithm. Performing multiple epochs is essential for the model to improve its predictions and overall performance. Choosing the appropriate number of epochs is a critical hyperparameter selection, ensuring a balance between learning meaningful patterns and preventing overfitting.

7.  In deep learning, training is commonly performed using
    mini-batches, which involve dividing the entire training
    dataset into smaller subsets. The batch size refers to
    the number of training examples processed together
    before the model updates its internal parameters
    (weights and biases) based on computed gradients. Larger
    batch sizes can lead to faster training as more samples
    are processed in parallel, but they also require more
    memory. Conversely, smaller batch sizes allow for more
    frequent model updates and may potentially improve
    generalization. Choosing an optimal batch size depends
    on factors such as computational resources, dataset
    size, and model complexity. It often involves striking a
    balance between training speed and memory usage. For
    example, if the training data consists of 10 examples
    and a batch size of 5 is used, one epoch would require
    two passes of the training examples (10/5). The weights
    are adjusted batch size times per epoch. Experimenting
    with different batch sizes is recommended to find the
    optimal balance for a specific deep learning task. See **Note 11** for more details.

8. Batch normalization has been shown to reduce the need for a large number of training steps and, in some cases, the use of dropout, resulting in stable and faster convergence, particularly with higher learning rates [@pmlrv37ioffe15]. However, it has also been observed that using batch normalization in conjunction with dropout can sometimes lead to a decrease in performance. It is worth noting that our model, when included batch normalization for training, achieves 100% accuracy on the training set. Nevertheless, it is essential to interpret such results with caution and not solely rely on exact accuracy as an indicator of model performance. Achieving perfect accuracy may be indicative of overfitting, where the model memorizes the training data without truly understanding the underlying patterns. Therefore, it is crucial to evaluate the model's ability to generalize to unseen examples by assessing its performance on separate validation or test data. Additionally, considering other evaluation metrics and techniques, such as cross-validation or precision and recall analysis, can provide a more comprehensive understanding of the model's capabilities. Adopting this mindset will enable the development of reliable and robust machine learning models. It can be implemented in code using layer_batch_normalization() function, right after each hidden layer.
    

9.  Imbalanced training data, where positive and negative examples are unevenly represented, can introduce bias in the learning process. To mitigate this issue, various techniques such as stratification through over- or down-sampling and assigning different weights to classes have been proposed and extensively discussed in the literature. For coding examples related to handling imbalanced data, please refer to **Note 5**. 

10. Data augmentation is a technique commonly used in deep learning to expand the training dataset artificially. It involves applying various transformations, such as rotation, scaling, flipping, cropping, or adding noise, to the existing data. By diversifying the training data, data augmentation helps prevent overfitting and enhances the model's ability to generalize to new, unseen examples. Please refer to **Note 5** also. 

11. When evaluating model performance, it is important to consider both data-driven and field-specific preferences. For imbalanced training data, precision and recall metrics provide more meaningful insights than overall accuracy. It is crucial to use performance metrics wisely to guide model training, ensuring that the model is not biased towards the majority class. For coding examples related to handling performance metrics, please refer to **Note 9**. 

12. Callbacks are functions that offer customization options for the training process of a deep learning model. They allow for monitoring the model's progress during training and performing specific actions based on certain conditions. Examples of actions that can be taken using callbacks include saving model weights, adjusting the learning rate, early stopping, and logging metrics. By leveraging callbacks, deep learning models can be tailored to achieve improved performance, efficiency, and customization. For coding examples related to callbacks, please refer to **Note 14**.

13. The R ecosystem provides extensive support and packages that can be utilized with the Keras R API for finding optimal hyperparameters in deep learning model development. One such package is kerasTuner, which is specifically designed for hyperparameter tuning with Keras models. It offers a user-friendly interface for defining a search space of hyperparameters and performing hyperparameter optimization using techniques like random search, hyperband, or Bayesian optimization. The seamless integration of kerasTuner with the Keras R API enables efficient identification of the best hyperparameters for deep learning models.

14. I recommend users to explore prototype examples of deep learning available at https://tensorflow.rstudio.com/examples/. These examples serve as valuable resources for understanding and implementing deep learning techniques.



# Notes

1.  Randomization is often involved in data analysis or
    machine learning tasks, such as shuffling datasets or
    initializing weights in neural networks. Setting the
    seed ensures that the same set of random numbers is
    generated each time the code is run. This is
    particularly useful when you need to reproduce and debug
    results or share code with others, ensuring consistent
    outcomes. The seed should be declared at the beginning
    of your script or function with any non-negative integer
    value, to ensure consistent results throughout your
    code.

2.  DNA microarray, RNA-Seq, and single-cell RNA-Seq data
    are accessible for several species and almost all model
    organisms through several public repositories. I
    recommend users to explore the NCBI Gene Expression
    Omnibus (GEO) [@geo], EBI ArrayExpress [@arrayexpress],
    PanglaoDB [@panglaodb], and recount[@recount] databases
    which offer processed gene expression and transcriptomic
    data that have undergone quality control, normalization,
    and sometimes additional analysis. For time series,
    cells, and tissues data, the FANTOM5 consortium
    [@fantom5] and The Genotype-Tissue Expression Project
    (GTEx) [@gtex] databases provide valuable resources.
    Additionally, for next-generation sequencing raw data,
    the NCBI Sequence Read Archive (SRA) is a valuable
    repository [@sra].

3.  Normalization of gene expression, RNA-seq, and scRNA-seq
    data is a statistical process that adjusts the
    expression values to remove systematic biases and
    variability, allowing for meaningful comparisons and
    accurate analysis of gene expression levels across
    samples or cells [@rnaseqnorm; @arraynorm;
    @scRNAseqanalysis]. R/Bioconductor has several excellent
    packages to perform normalization seamlessly, which
    users should explore on their own.

4.  Researchers assemble GRN gold standard usually from
    experimentally derived regulatory interactions reported
    in literature and databases [@GarciaAlonso01082019]. For instance,
    I have assembled human transcriptional regulatory
    network from 14 resources which can be used for benchmarking predictions
    in humans [@Muley2022], while TFLink database provide
    such information for few more model organisms [@tflink].
    JASPAR database is worth to explore to create a GRN
    based on information available in other species
    [@jaspar].

5.  Imbalanced data is a common challenge in biology, making
    training deep learning models tricky
    [@golddeeplearning]. R packages such as caret, ROSE, and
    smote [@caret; @rose; @smote] are popular resources for
    addressing imbalanced data. While the smotefamily package
    utilizes the SMOTE algorithm to generate synthetic
    samples of the minority class. Another approach is to down-size the majority class examples to make them compatible with minority class. I have used this approach for our problem. Also, I often prefer the class
    weight approach. In the provided code snippet, the
    "class_weight" parameter is used during model training
    for binary classification. By calculating class weights
    based on the class distribution in the training data,
    and passing them as the "class_weight" argument in the
    fit function, you can assign higher weights to the
    minority class and lower weights to the majority class.
    This enables the model to effectively handle class
    imbalance during optimization, prioritizing the minority
    class samples for better learning outcomes. 

```{r, eval=FALSE, echo=FALSE}

weights <- as.list((1/table(traindata$class)))

history <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class, 
                         epochs = 10, batch_size = 16, 
                         validation_data = list(as.matrix(valdata[,featurenames]), valdata$class),
                         class_weight = weights)

```

6.  The purpose of set_random_seed() is similar to that of
    set.seed(), as mentioned in **Note 1**. The key
    distinction is that set_random_seed() specifically sets
    a seed for TensorFlow operations, ensuring
    reproducibility of the training process. However, in practice users should confirmed robustness of the obtained results with many seed number. Its rare but sometimes set_random_seed function does not work, and throw error about version of TensorFlow, in that case, it should work after installing tensorflow again using "install_tensorflow()" command.

7.  The number of layers and neurons (units) in a neural
    network can be adjusted to enhance its learning capacity
    [@layersneurons]. However, there is no definitive rule
    on how many layers and neurons a network should have for
    a given dataset. If the data is linearly separable,
    multiple hidden layers may not be necessary. It is
    generally recommended to keep the number of hidden
    layers as minimal as possible. The number of neurons in
    a layer can be determined
    based on the shape of your training data. Typically, the
    number of neurons in a layer is equal to the number of
    features (columns) in your data. Some network
    configurations add an additional node for a bias term.
    It is crucial to maintain a low number of nodes to
    ensure the network's ability to generalize well. Having
    an excessive number of nodes may cause the network to
    perfectly recall the training set but perform poorly on
    new, unseen samples. The following code shows how to include more layer but there are numbe of different ways one can add layers and decide on number of neurons. Usually in each successive layers, the number of neurons are decreased.
    
```{r, eval=FALSE, echo=FALSE}

model <- keras_model_sequential(input_shape = c(length(featurenames))) %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dense(units = 806, activation = "relu") %>%
  layer_dropout(0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 56, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(0.1) %>%
  layer_dense(units = 1, activation = "sigmoid")

```


8.  Optimization functions in Keras R library are algorithms
    that optimize the weights and biases of a neural network
    during the training process. These functions help in
    finding the optimal set of parameters that minimize the
    loss function and improve the model's performance. Keras R API provides several
    commonly used optimization functions such as optimizer_adam(),
    optimizer_sgd() for Stochastic Gradient Descent and
    optimizer_rmsprop() for RMSprop. Learning rate is an important hyperparameter that defines the speed of learning and can be set with learning_rate parameter. For example, optimizer_adam(learning_rate = 0.00001).

9.  Accuracy can be misleading when dealing with imbalanced
    datasets as it does not account for class imbalance.
    Recall and precision metrics are often preferred over
    accuracy in most scenarios, especially when the cost of false
    positives and false negatives differs significantly.
    Recall measures the proportion of true positive cases
    correctly identified, while precision measures the
    proportion of correctly identified positive cases. These
    metrics provide a more comprehensive understanding of
    the model's performance, especially when the focus is on
    correctly identifying specific classes or minimizing
    false positives/negatives. These metrics could be set in
    with the complie() function as shown below. Remove
    comment (#) character to include the metric for
    training.

```{r, eval=FALSE, echo=FALSE}

metrics <- list(
  "fn" = metric_false_negatives(),
  "fp" = metric_false_positives(),
  "tn" = metric_true_negatives(),
  "tp" = metric_true_positives(),
  "accuracy" = metric_accuracy(),
  "precision" = metric_precision(),
  "recall" = metric_recall(),
  "auc" = metric_auc())

model %>% compile(optimizer = optimizer_adam(),
                  loss = "binary_crossentropy",
                  metrics = metrics)

```

10. To set the number of epochs for training, start with a small value like 10 or 20 to
    evaluate initial performance and identify any issues.
    Since the optimal number of epochs varies based on the
    dataset and problem, it's crucial to experiment and
    fine-tune this parameter for optimal results. Monitor
    the training and validation loss during training;
    ideally, the loss should decrease. However, if the
    training loss continues to decrease while the validation
    loss increases, it suggests overfitting. To prevent
    overfitting, implement early stopping (see callbacks
    function), which allows you to halt training if the
    validation loss stagnates or deteriorates after a
    certain number of epochs. Lastly, consider the
    computational resources and time constraints available,
    as training deep learning models can be computationally
    demanding. Striking a balance between model performance
    and practical limitations is key.

11. The batch size in deep learning determines the number of training examples processed in each iteration. It plays a crucial role in efficient model training. Choosing an appropriate batch size is important, as it impacts convergence speed and computational efficiency. Smaller batch sizes, such as 32 or 64, enable faster updates of model parameters but may introduce more noise and slower convergence. On the other hand, larger batch sizes enhance stability and convergence but require more memory and computational resources. It is recommended to experiment with different batch sizes based on the available resources, considering the trade-off between convergence speed and computational efficiency.


12. The code uses validation data (optional) to assess the
    model's performance during training. Alternatively, use
    the validation_split parameter to specify the percentage
    of training data for validation. For example, in the fit
    function, you can set validation_split = 0.15 parameter
    to use 15% of training data for validation. These
    parameters are optional and can be adjusted to suit the
    model and data requirements.

13. An alternative code for prediction of GRN on high-end computational resources.


```{r, eval=FALSE}

library(magrittr)
library(keras)
library(tensorflow)

set.seed(1979)


model <- load_model_hdf5("EcoliRegModel.h5")

# import expression data file
exp <- t(read.table("net3_expression_data.tsv", header = T))

# import original gene names mapping 
genenames <- read.table("net3_gene_ids.tsv", header = F)
genes <- genenames$V2; names(genes) <- genenames$V1

# import list of all transcription factors of Escherichia coli
tfs <- read.table("net3_transcription_factors.tsv", header = F)$V1

# create all possible pairs between tanscription factors and genes

predictions <- expand.grid(tfs,  rownames(exp), stringsAsFactors = FALSE)
predictions <- tibble::as_tibble(all_pairs[predictions$Var1 != predictions$Var2,])

# add original gene names

predictions$tfname <- genes[predictions$Var1]
predictions$genename <- genes[predictions$Var2]

# create feature table
inpreddata <- cbind(exp[predictions$Var1,], exp[predictions$Var2,])
inpreddata$pcc  <- sapply(seq.int(dim(predictions)[1]), 
                          function(i) cor(exp[predictions$Var1[i],], 
                                          exp[predictions$Var2[i],])))

# predict regulatory pairs
predictions$probability <- (model %>% predict(inpreddata))[,1]
predictions <- predictions[predictions$probability>0.5,]
predictions <- predictions[rev(order(predictions$probability)),]
predictions 
```

14. Callback functions are handy to control the training as
    per our desires. In the following code, I have provided
    examples of small functions that we will use to control
    the training process dynamically.

EarlyStopping Callback: This callback monitors the
validation loss and stops the training if the validation
loss does not improve after a certain number of epochs
(defined by the "patience" parameter).

```{r, eval=FALSE}

early_stopping <- callback_early_stopping(monitor = "val_loss", patience = 3)


```

ModelCheckpoint Callback: This callback saves the model
weights to a file ("best_model.h5") whenever the validation
accuracy improves. By setting "save_best_only" to True, it
only saves the best model based on the monitored metric..

```{r, eval=FALSE}

model_checkpoint <- callback_model_checkpoint(filepath = "best_model.h5", 
                                              monitor = "val_accuracy", 
                                              save_best_only = TRUE)


```

ReduceLROnPlateau Callback: This callback reduces the
learning rate when the validation loss plateaus. The
learning rate is multiplied by the "factor" parameter, and
"patience" determines the number of epochs to wait before
reducing the learning rate.

```{r, eval=FALSE}

reduce_lr <- callback_reduce_lr_on_plateau(monitor = "val_loss", 
                                           factor = 0.1, patience = 2)


```

TensorBoard: This callback enables logging for TensorBoard,
which is a visualization tool for monitoring the training
process. It creates logs that can be visualized using
TensorBoard.

```{r, eval=FALSE}

tensorboard <- callback_tensorboard(log_dir = "logs")


```

Once callback functions are defined they can be incorporated during training as shown below for early_stopping and reduce_lr callback functions, but you can have as many functions as you want.

```{r, eval=FALSE}

history <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class,
                           epochs=40, batch_size = 50, validation_split=0.2,
                           callbacks = list(early_stopping, reduce_lr))


```




```{r, eval=FALSE, echo=FALSE}

genenames <- read.table("DREAM5_network_inference_challenge/Network3/anonymization/net3_gene_ids.tsv", header = F)
v <-  data.frame(tf = rep("G259", nrow(exp)), gene = sapply(rownames(exp), function(x) genenames[genenames$V1==x,"V2"]))
unclassified <- as.matrix(data.frame(exp[v$tf,], exp))

for(mynum in 1:10000){
  
  sequence <- 2^(2:3)
  print(sequence)
  
  seed <- sample(1:100000, 1)
  lr <- runif(1, min = 10e-10, max = 0.05)
  batch <- sample(sample(2^(7:12),1), 1)
  epoch <- sample(seq(5, 50, 5), 1)
  layer1 <- sample(2^(8:10),1)
  layer2 <- sample(2^(5:7),1)
  layer3 <- sample(2^(2:4),1)
  dropo <- sample(seq(0.1, 0.5, 0.1), 1)

  mypara <- c(mynum, layer1, layer2, layer3, epoch, batch, lr, dropo, seed)
  cat("-----------------------\n")
  print(mypara)
  cat("-----------------------\n")

  set_random_seed(seed)
  
  model <- keras_model_sequential(input_shape = c(length(featurenames))) %>%
    layer_dense(units = layer1, activation = "relu") %>%
    layer_dropout(dropo) %>%
    layer_dense(units = layer2, activation = "relu") %>%
    layer_dropout(dropo) %>%
    layer_dense(units = layer3, activation = "relu") %>%
    layer_dropout(dropo) %>%
    layer_dense(1, activation = "sigmoid")
  
  
  metrics <- list(
#    metric_auc(name = "auc"),
    metric_precision(name = "precision"),
    metric_recall(name = "recall"))
  
  model %>% compile(
    optimizer = optimizer_adam(lr),
    loss = "binary_crossentropy",
    metrics = metrics)
  
  class_weights <- as.list(1/table(traindata$class))
  
  history <- model %>% fit(as.matrix(traindata[,featurenames]), traindata$class, 
                           epochs = epoch, batch_size = batch, 
                           validation_data = list(as.matrix(valdata[,featurenames]), valdata$class),
                           class_weight = class_weights)
  
  predprob <- model %>% predict(unclassified)
  probability <- data.frame(gene = v$gene, probability = predprob)
  probability <- probability[rev(order(probability$probability)),][1:20,]
  
  res <- list(p = mypara, pred = probability)
  save(res, file = paste0("vijayPred.", mynum, ".RData"))
  
}
```

# References
